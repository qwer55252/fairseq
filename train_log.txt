DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME
DEBUG:hydra.core.utils:Setting JobRuntime:name=hydra_train
[2024-09-27 16:33:41,020][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 3200000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3200000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 80000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'wer', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec_ctc', 'w2v_path': '/home/kobie/workspace/fairseq/weights/wav2vec_small.pt', 'no_pretrained_weights': False, 'dropout_input': 0.0, 'final_dropout': 0.0, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'apply_mask': True, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 64, 'mask_channel_prob': 0.5, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'freeze_finetune_updates': 0, 'feature_grad_mult': 0.0, 'layerdrop': 0.1, 'drop_path': 0.0, 'mask_channel_min_space': 1, 'mask_channel_before': False, 'normalize': False, 'update_alibi': True, 'data': '/home/kobie/workspace/data/LibriSpeech', 'w2v_args': None, 'offload_activations': False, 'min_params_to_wrap': 100000000, 'checkpoint_activations': False, 'ddp_backend': 'pytorch_ddp', 'zero_mask': False, 'load_ema': False, 'layer_decay': 1.0, 'layer_type': transformer, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all', 'freeze_regex': None, 'blank_weight': 0.0, 'blank_mode': 'add'}, 'task': {'_name': 'audio_finetuning', 'data': '/home/kobie/workspace/data/LibriSpeech', 'labels': 'ltr', 'multi_corpus_keys': None, 'multi_corpus_sampling_weights': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_sample_size': None, 'min_sample_size': None, 'num_batch_buckets': 0, 'tpu': False, 'text_compression_level': none, 'rebuild_batches': True, 'precompute_mask_config': None, 'post_save_script': None, 'subsample': 1.0, 'seed': 1, 'eval_wer': False, 'eval_wer_config': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_wer_tokenizer': None, 'eval_wer_post_process': 'letter', 'eval_bleu': False, 'eval_bleu_detok': None, 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_args': '{}', 'eval_bleu_print_samples': False, 'autoregressive': False, 'target_dictionary': None}, 'criterion': {'_name': 'ctc', 'zero_infinity': True, 'sentence_avg': True, 'post_process': 'letter', 'wer_kenlm_model': None, 'wer_lexicon': None, 'wer_lm_weight': 2.0, 'wer_word_score': -1.0, 'wer_sil_weight': 0.0, 'wer_args': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'tri_stage', 'warmup_steps': 0, 'hold_steps': 0, 'decay_steps': 0, 'phase_ratio': [0.1, 0.4, 0.5], 'init_lr_scale': 0.01, 'final_lr_scale': 0.05, 'max_update': 80000.0, 'lr': [3e-05]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2024-09-27 16:33:41,028][fairseq.tasks.audio_finetuning][INFO] - Using dict_path : /home/kobie/workspace/data/LibriSpeech/dict.ltr.txt
/home/kobie/workspace/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
[2024-09-27 16:33:41,828][fairseq.models.wav2vec.wav2vec2_asr][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '', 'wandb_project': None, 'azureml_logging': False, 'seed': 5, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 64, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair2106:55498', 'distributed_port': 55498, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 1400000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 5, 'validate_interval_updates': 10000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1400000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 25.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint/abaevski/asr/speechbert_raw_250k_q5/best_ld_400k.nep.qtz.nnf0.ng512.pq.lv320.lvb2.lr0.0005.mask10.mprob0.65.mstd0.mstd0.05.drp_i0.1.drp_f0.1.fgm0.1.qini1.fpen.pen0_0_0.1_10.cpl1.neg100.mxsz250000.s5.ngpu64', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 64}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.1, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': True, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 0.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 64, 'mask_channel_prob': 0.5, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 8000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'audio_pretraining', 'data': '/private/home/abaevski/data/librispeech/full', 'labels': None, 'multi_corpus_keys': None, 'multi_corpus_sampling_weights': None, 'binarized_dataset': False, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_sample_size': 250000, 'min_sample_size': 32000, 'num_batch_buckets': 0, 'tpu': True, 'text_compression_level': 'none', 'rebuild_batches': True, 'precompute_mask_config': None, 'post_save_script': None, 'subsample': 1.0, 'seed': 5}, 'criterion': None, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0005]}, 'lr_scheduler': None, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
/home/kobie/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
Wav2Vec2Model(
  (feature_extractor): ConvFeatureExtractionModel(
    (conv_layers): ModuleList(
      (0): Sequential(
        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
        (3): GELU(approximate='none')
      )
      (1-4): 4 x Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU(approximate='none')
      )
      (5-6): 2 x Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
        (1): Dropout(p=0.0, inplace=False)
        (2): GELU(approximate='none')
      )
    )
  )
  (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.1, inplace=False)
  (quantizer): None
  (project_q): None
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
      (1): SamePad()
      (2): GELU(approximate='none')
    )
    (layers): ModuleList(
      (0-11): 12 x TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (final_proj): None
)
[2024-09-27 16:33:42,477][fairseq_cli.train][INFO] - Wav2VecCtc(
  (w2v_encoder): Wav2VecEncoder(
    (w2v_model): Wav2Vec2Model(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (quantizer): None
      (project_q): None
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (final_dropout): Dropout(p=0.0, inplace=False)
    (proj): Linear(in_features=768, out_features=32, bias=True)
  )
)
[2024-09-27 16:33:42,478][fairseq_cli.train][INFO] - task: AudioFinetuningTask
[2024-09-27 16:33:42,478][fairseq_cli.train][INFO] - model: Wav2VecCtc
[2024-09-27 16:33:42,478][fairseq_cli.train][INFO] - criterion: CtcCriterion
[2024-09-27 16:33:42,479][fairseq_cli.train][INFO] - num. shared model params: 94,396,320 (num. trained: 94,396,320)
[2024-09-27 16:33:42,479][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2024-09-27 16:33:42,480][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 296, skipped 0 samples
[2024-09-27 16:33:42,598][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
[2024-09-27 16:33:42,598][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
[2024-09-27 16:33:42,598][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
[2024-09-27 16:33:42,598][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
[2024-09-27 16:33:42,598][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
[2024-09-27 16:33:42,598][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
[2024-09-27 16:33:42,598][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2024-09-27 16:33:42,598][fairseq.utils][INFO] - rank   0: capabilities =  8.9  ; total memory = 23.543 GB ; name = NVIDIA GeForce RTX 4090                 
[2024-09-27 16:33:42,598][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2024-09-27 16:33:42,598][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2024-09-27 16:33:42,599][fairseq_cli.train][INFO] - max tokens per device = 3200000 and max sentences per device = None
[2024-09-27 16:33:42,599][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2024-09-27 16:33:42,599][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2024-09-27 16:33:42,599][fairseq.trainer][INFO] - loading train data for epoch 1
[2024-09-27 16:33:42,610][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 28243, skipped 0 samples
[2024-09-27 16:33:42,622][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:33:42,623][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True
[2024-09-27 16:33:42,623][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True
[2024-09-27 16:33:42,623][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch
[2024-09-27 16:33:42,623][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 1
[2024-09-27 16:33:42,945][fairseq_cli.train][INFO] - begin dry-run validation on "valid" subset
[2024-09-27 16:33:42,945][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:33:42,945][fairseq.tasks.fairseq_task][INFO] - reuse_dataloader = True
[2024-09-27 16:33:42,945][fairseq.tasks.fairseq_task][INFO] - rebuild_batches = True
[2024-09-27 16:33:42,945][fairseq.tasks.fairseq_task][INFO] - batches will be rebuilt for each epoch
[2024-09-27 16:33:42,945][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 1
[2024-09-27 16:33:43,387][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 16:33:43,389][fairseq.trainer][INFO] - begin training epoch 1
[2024-09-27 16:33:43,390][fairseq_cli.train][INFO] - Start iterating over samples
/home/kobie/workspace/fairseq/fairseq/tasks/fairseq_task.py:531: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
[2024-09-27 16:33:44,162][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
[2024-09-27 16:33:44,498][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
[2024-09-27 16:33:44,813][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2024-09-27 16:33:45,084][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-09-27 16:33:45,746][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-09-27 16:34:23,857][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
[2024-09-27 16:34:26,352][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
[2024-09-27 16:34:45,967][train_inner][INFO] - {"epoch": 1, "update": 0.29, "loss": "2026.29", "ntokens": "7407.73", "nsentences": "40.36", "nll_loss": "11.04", "wps": "24370.3", "ups": "3.29", "wpb": "7407.7", "bsz": "40.4", "num_updates": "200", "lr": "1.0425e-06", "gnorm": "2612.14", "loss_scale": "1", "train_wall": "62", "gb_free": "19.2", "wall": "63"}
[2024-09-27 16:35:45,769][train_inner][INFO] - {"epoch": 1, "update": 0.571, "loss": "1112.07", "ntokens": "7394.86", "nsentences": "39", "nll_loss": "5.865", "wps": "24731.3", "ups": "3.34", "wpb": "7394.9", "bsz": "39", "num_updates": "400", "lr": "1.785e-06", "gnorm": "1364.1", "loss_scale": "1", "train_wall": "59", "gb_free": "18.8", "wall": "123"}
[2024-09-27 16:36:44,769][train_inner][INFO] - {"epoch": 1, "update": 0.851, "loss": "816.14", "ntokens": "7284.12", "nsentences": "39.48", "nll_loss": "4.423", "wps": "24692.1", "ups": "3.39", "wpb": "7284.1", "bsz": "39.5", "num_updates": "600", "lr": "2.5275e-06", "gnorm": "454.12", "loss_scale": "1", "train_wall": "59", "gb_free": "19.4", "wall": "182"}
[2024-09-27 16:37:16,028][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 16:37:16,028][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:37:16,064][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 2
[2024-09-27 16:37:18,686][valid][INFO] - {"epoch": 1, "valid_loss": "796.079", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "4.162", "valid_uer": "100", "valid_wer": "100", "valid_raw_wer": "100", "valid_wps": "22266.2", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "706"}
[2024-09-27 16:37:18,686][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 706 updates
[2024-09-27 16:37:18,687][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:37:19,491][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:37:19,835][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 706 updates, score 100.0) (writing took 1.1484848130494356 seconds)
[2024-09-27 16:37:19,835][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2024-09-27 16:37:19,835][train][INFO] - {"epoch": 1, "train_loss": "1239.2", "train_ntokens": "7355.72", "train_nsentences": "39.6827", "train_nll_loss": "6.685", "train_wps": "24192", "train_ups": "3.29", "train_wpb": "7355.7", "train_bsz": "39.7", "train_num_updates": "706", "train_lr": "2.92103e-06", "train_gnorm": "1292.42", "train_loss_scale": "1", "train_train_wall": "211", "train_gb_free": "18", "train_wall": "217"}
[2024-09-27 16:37:19,836][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:37:19,877][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 2
[2024-09-27 16:37:19,999][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 16:37:20,001][fairseq.trainer][INFO] - begin training epoch 2
[2024-09-27 16:37:20,001][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 16:37:47,585][train_inner][INFO] - {"epoch": 2, "update": 1.132, "loss": "777.28", "ntokens": "7304.24", "nsentences": "39.2", "nll_loss": "4.171", "wps": "23255.9", "ups": "3.18", "wpb": "7304.2", "bsz": "39.2", "num_updates": "800", "lr": "3.27e-06", "gnorm": "250.338", "loss_scale": "1", "train_wall": "58", "gb_free": "18", "wall": "245"}
[2024-09-27 16:38:46,729][train_inner][INFO] - {"epoch": 2, "update": 1.412, "loss": "761.033", "ntokens": "7336.71", "nsentences": "39.96", "nll_loss": "4.145", "wps": "24809.6", "ups": "3.38", "wpb": "7336.7", "bsz": "40", "num_updates": "1000", "lr": "4.0125e-06", "gnorm": "213.685", "loss_scale": "1", "train_wall": "59", "gb_free": "19.5", "wall": "304"}
[2024-09-27 16:39:46,410][train_inner][INFO] - {"epoch": 2, "update": 1.693, "loss": "754.166", "ntokens": "7399.25", "nsentences": "40.44", "nll_loss": "4.122", "wps": "24796.1", "ups": "3.35", "wpb": "7399.2", "bsz": "40.4", "num_updates": "1200", "lr": "4.755e-06", "gnorm": "181.366", "loss_scale": "1", "train_wall": "59", "gb_free": "18.3", "wall": "364"}
[2024-09-27 16:40:45,466][train_inner][INFO] - {"epoch": 2, "update": 1.973, "loss": "772.298", "ntokens": "7357.76", "nsentences": "39.12", "nll_loss": "4.106", "wps": "24918", "ups": "3.39", "wpb": "7357.8", "bsz": "39.1", "num_updates": "1400", "lr": "5.4975e-06", "gnorm": "142.394", "loss_scale": "1", "train_wall": "59", "gb_free": "19.1", "wall": "423"}
[2024-09-27 16:40:50,987][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 16:40:50,988][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:40:51,022][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 3
[2024-09-27 16:40:53,635][valid][INFO] - {"epoch": 2, "valid_loss": "781.404", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "4.086", "valid_uer": "100", "valid_wer": "100", "valid_raw_wer": "100", "valid_wps": "22345.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "1419", "valid_best_wer": "100"}
[2024-09-27 16:40:53,636][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 1419 updates
[2024-09-27 16:40:53,637][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:40:54,764][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:40:55,357][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 1419 updates, score 100.0) (writing took 1.7208162068855017 seconds)
[2024-09-27 16:40:55,357][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2024-09-27 16:40:55,357][train][INFO] - {"epoch": 2, "train_loss": "766.319", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "4.129", "train_wps": "24321", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "1419", "train_lr": "5.56804e-06", "train_gnorm": "188.219", "train_loss_scale": "1", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "433"}
[2024-09-27 16:40:55,358][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:40:55,400][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 3
[2024-09-27 16:40:55,519][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 16:40:55,521][fairseq.trainer][INFO] - begin training epoch 3
[2024-09-27 16:40:55,521][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 16:41:49,210][train_inner][INFO] - {"epoch": 3, "update": 2.254, "loss": "722.395", "ntokens": "7380.59", "nsentences": "41.08", "nll_loss": "4.021", "wps": "23157", "ups": "3.14", "wpb": "7380.6", "bsz": "41.1", "num_updates": "1600", "lr": "6.24e-06", "gnorm": "185.355", "loss_scale": "1", "train_wall": "59", "gb_free": "19.3", "wall": "487"}
[2024-09-27 16:42:48,346][train_inner][INFO] - {"epoch": 3, "update": 2.534, "loss": "636.641", "ntokens": "7373.39", "nsentences": "39.24", "nll_loss": "3.388", "wps": "24937.4", "ups": "3.38", "wpb": "7373.4", "bsz": "39.2", "num_updates": "1800", "lr": "6.9825e-06", "gnorm": "259.521", "loss_scale": "1", "train_wall": "59", "gb_free": "19", "wall": "546"}
[2024-09-27 16:43:47,184][train_inner][INFO] - {"epoch": 3, "update": 2.815, "loss": "454.815", "ntokens": "7278.28", "nsentences": "38.6", "nll_loss": "2.412", "wps": "24740", "ups": "3.4", "wpb": "7278.3", "bsz": "38.6", "num_updates": "2000", "lr": "7.725e-06", "gnorm": "256.506", "loss_scale": "1", "train_wall": "58", "gb_free": "19.1", "wall": "605"}
[2024-09-27 16:44:26,423][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 16:44:26,423][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:44:26,462][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 4
[2024-09-27 16:44:29,128][valid][INFO] - {"epoch": 3, "valid_loss": "186.065", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.973", "valid_uer": "20.379", "valid_wer": "58.958", "valid_raw_wer": "58.958", "valid_wps": "22060.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "2132", "valid_best_wer": "58.958"}
[2024-09-27 16:44:29,128][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 2132 updates
[2024-09-27 16:44:29,129][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:44:30,148][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:44:30,731][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 2132 updates, score 58.958) (writing took 1.6020722810644656 seconds)
[2024-09-27 16:44:30,731][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2024-09-27 16:44:30,731][train][INFO] - {"epoch": 3, "train_loss": "556.4", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "2.998", "train_wps": "24337.8", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "2132", "train_lr": "8.21505e-06", "train_gnorm": "238.73", "train_loss_scale": "1", "train_train_wall": "210", "train_gb_free": "18.3", "train_wall": "648"}
[2024-09-27 16:44:30,732][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:44:30,774][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 4
[2024-09-27 16:44:30,893][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 16:44:30,895][fairseq.trainer][INFO] - begin training epoch 4
[2024-09-27 16:44:30,896][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 16:44:51,128][train_inner][INFO] - {"epoch": 4, "update": 3.095, "loss": "346.52", "ntokens": "7380.12", "nsentences": "39.48", "nll_loss": "1.854", "wps": "23083", "ups": "3.13", "wpb": "7380.1", "bsz": "39.5", "num_updates": "2200", "lr": "8.4675e-06", "gnorm": "243.811", "loss_scale": "1", "train_wall": "59", "gb_free": "19.3", "wall": "669"}
[2024-09-27 16:45:50,057][train_inner][INFO] - {"epoch": 4, "update": 3.376, "loss": "298.837", "ntokens": "7289.55", "nsentences": "39", "nll_loss": "1.599", "wps": "24740.4", "ups": "3.39", "wpb": "7289.6", "bsz": "39", "num_updates": "2400", "lr": "9.21e-06", "gnorm": "241.185", "loss_scale": "1", "train_wall": "59", "gb_free": "19.3", "wall": "727"}
[2024-09-27 16:46:49,461][train_inner][INFO] - {"epoch": 4, "update": 3.656, "loss": "257.633", "ntokens": "7373.64", "nsentences": "40.12", "nll_loss": "1.402", "wps": "24825.4", "ups": "3.37", "wpb": "7373.6", "bsz": "40.1", "num_updates": "2600", "lr": "9.9525e-06", "gnorm": "237.593", "loss_scale": "1", "train_wall": "59", "gb_free": "18.4", "wall": "787"}
[2024-09-27 16:47:48,409][train_inner][INFO] - {"epoch": 4, "update": 3.937, "loss": "233.797", "ntokens": "7360.68", "nsentences": "39.8", "nll_loss": "1.264", "wps": "24973.5", "ups": "3.39", "wpb": "7360.7", "bsz": "39.8", "num_updates": "2800", "lr": "1.0695e-05", "gnorm": "235.944", "loss_scale": "1", "train_wall": "59", "gb_free": "19.3", "wall": "846"}
[2024-09-27 16:48:01,717][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 16:48:01,717][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:48:01,752][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 5
[2024-09-27 16:48:04,391][valid][INFO] - {"epoch": 4, "valid_loss": "96.2", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.503", "valid_uer": "10.319", "valid_wer": "36.838", "valid_raw_wer": "36.838", "valid_wps": "22020.6", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "2845", "valid_best_wer": "36.838"}
[2024-09-27 16:48:04,391][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 2845 updates
[2024-09-27 16:48:04,392][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:48:05,451][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:48:06,045][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 2845 updates, score 36.838) (writing took 1.6541436889674515 seconds)
[2024-09-27 16:48:06,046][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2024-09-27 16:48:06,046][train][INFO] - {"epoch": 4, "train_loss": "268.356", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "1.446", "train_wps": "24344.4", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "2845", "train_lr": "1.08621e-05", "train_gnorm": "237.284", "train_loss_scale": "1", "train_train_wall": "209", "train_gb_free": "19.6", "train_wall": "863"}
[2024-09-27 16:48:06,047][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:48:06,087][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 5
[2024-09-27 16:48:06,215][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 16:48:06,217][fairseq.trainer][INFO] - begin training epoch 5
[2024-09-27 16:48:06,217][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 16:48:52,001][train_inner][INFO] - {"epoch": 5, "update": 4.217, "loss": "222.833", "ntokens": "7372.26", "nsentences": "39.28", "nll_loss": "1.187", "wps": "23186.4", "ups": "3.15", "wpb": "7372.3", "bsz": "39.3", "num_updates": "3000", "lr": "1.14375e-05", "gnorm": "232.395", "loss_scale": "1", "train_wall": "59", "gb_free": "19.2", "wall": "909"}
[2024-09-27 16:49:51,655][train_inner][INFO] - {"epoch": 5, "update": 4.498, "loss": "202.167", "ntokens": "7404.53", "nsentences": "39.6", "nll_loss": "1.081", "wps": "24824.9", "ups": "3.35", "wpb": "7404.5", "bsz": "39.6", "num_updates": "3200", "lr": "1.218e-05", "gnorm": "230.911", "loss_scale": "1", "train_wall": "59", "gb_free": "19", "wall": "969"}
[2024-09-27 16:50:50,763][train_inner][INFO] - {"epoch": 5, "update": 4.778, "loss": "185.209", "ntokens": "7325.52", "nsentences": "40.2", "nll_loss": "1.016", "wps": "24787", "ups": "3.38", "wpb": "7325.5", "bsz": "40.2", "num_updates": "3400", "lr": "1.29225e-05", "gnorm": "221.102", "loss_scale": "1", "train_wall": "59", "gb_free": "17.6", "wall": "1028"}
[2024-09-27 16:51:37,551][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 16:51:37,551][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:51:37,589][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 6
[2024-09-27 16:51:40,220][valid][INFO] - {"epoch": 5, "valid_loss": "64.259", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.336", "valid_uer": "6.894", "valid_wer": "25.562", "valid_raw_wer": "25.562", "valid_wps": "22042", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "3558", "valid_best_wer": "25.562"}
[2024-09-27 16:51:40,221][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 3558 updates
[2024-09-27 16:51:40,221][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:51:41,295][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:51:41,872][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 3558 updates, score 25.562) (writing took 1.6511493627913296 seconds)
[2024-09-27 16:51:41,872][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2024-09-27 16:51:41,872][train][INFO] - {"epoch": 5, "train_loss": "196.678", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "1.06", "train_wps": "24286.7", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "3558", "train_lr": "1.35091e-05", "train_gnorm": "225.649", "train_loss_scale": "1", "train_train_wall": "210", "train_gb_free": "17.9", "train_wall": "1079"}
[2024-09-27 16:51:41,873][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:51:41,914][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 6
[2024-09-27 16:51:42,032][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 16:51:42,033][fairseq.trainer][INFO] - begin training epoch 6
[2024-09-27 16:51:42,034][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 16:51:54,476][train_inner][INFO] - {"epoch": 6, "update": 5.059, "loss": "179.297", "ntokens": "7308.72", "nsentences": "39.24", "nll_loss": "0.963", "wps": "22942.5", "ups": "3.14", "wpb": "7308.7", "bsz": "39.2", "num_updates": "3600", "lr": "1.3665e-05", "gnorm": "217.435", "loss_scale": "1", "train_wall": "59", "gb_free": "19.1", "wall": "1092"}
[2024-09-27 16:52:53,576][train_inner][INFO] - {"epoch": 6, "update": 5.339, "loss": "176.006", "ntokens": "7338.8", "nsentences": "39.52", "nll_loss": "0.948", "wps": "24835.3", "ups": "3.38", "wpb": "7338.8", "bsz": "39.5", "num_updates": "3800", "lr": "1.44075e-05", "gnorm": "217.629", "loss_scale": "1", "train_wall": "59", "gb_free": "19.2", "wall": "1151"}
[2024-09-27 16:53:52,558][train_inner][INFO] - {"epoch": 6, "update": 5.62, "loss": "167.235", "ntokens": "7342.16", "nsentences": "39.12", "nll_loss": "0.891", "wps": "24896.7", "ups": "3.39", "wpb": "7342.2", "bsz": "39.1", "num_updates": "4000", "lr": "1.515e-05", "gnorm": "214.087", "loss_scale": "1", "train_wall": "59", "gb_free": "18.2", "wall": "1210"}
[2024-09-27 16:54:51,774][train_inner][INFO] - {"epoch": 6, "update": 5.9, "loss": "157.983", "ntokens": "7343.6", "nsentences": "39.76", "nll_loss": "0.855", "wps": "24802.7", "ups": "3.38", "wpb": "7343.6", "bsz": "39.8", "num_updates": "4200", "lr": "1.58925e-05", "gnorm": "210.235", "loss_scale": "1", "train_wall": "59", "gb_free": "17.5", "wall": "1269"}
[2024-09-27 16:55:13,077][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 16:55:13,077][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:55:13,111][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 7
[2024-09-27 16:55:15,762][valid][INFO] - {"epoch": 6, "valid_loss": "48.52", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.254", "valid_uer": "5.041", "valid_wer": "18.499", "valid_raw_wer": "18.499", "valid_wps": "22125.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "4271", "valid_best_wer": "18.499"}
[2024-09-27 16:55:15,763][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 4271 updates
[2024-09-27 16:55:15,763][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:55:16,928][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:55:17,506][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 4271 updates, score 18.499) (writing took 1.7436533051077276 seconds)
[2024-09-27 16:55:17,506][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2024-09-27 16:55:17,507][train][INFO] - {"epoch": 6, "train_loss": "165.311", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.891", "train_wps": "24308.4", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "4271", "train_lr": "1.61561e-05", "train_gnorm": "213.552", "train_loss_scale": "2", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "1295"}
[2024-09-27 16:55:17,507][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:55:17,548][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 7
[2024-09-27 16:55:17,671][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 16:55:17,673][fairseq.trainer][INFO] - begin training epoch 7
[2024-09-27 16:55:17,673][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 16:55:55,633][train_inner][INFO] - {"epoch": 7, "update": 6.181, "loss": "157.063", "ntokens": "7351.95", "nsentences": "39.16", "nll_loss": "0.837", "wps": "23025.5", "ups": "3.13", "wpb": "7351.9", "bsz": "39.2", "num_updates": "4400", "lr": "1.6635e-05", "gnorm": "216.916", "loss_scale": "2", "train_wall": "59", "gb_free": "19.4", "wall": "1333"}
[2024-09-27 16:56:54,885][train_inner][INFO] - {"epoch": 7, "update": 6.461, "loss": "144.126", "ntokens": "7354.02", "nsentences": "39.96", "nll_loss": "0.783", "wps": "24823.1", "ups": "3.38", "wpb": "7354", "bsz": "40", "num_updates": "4600", "lr": "1.73775e-05", "gnorm": "210.31", "loss_scale": "2", "train_wall": "59", "gb_free": "19.3", "wall": "1392"}
[2024-09-27 16:57:54,304][train_inner][INFO] - {"epoch": 7, "update": 6.742, "loss": "145.694", "ntokens": "7377.28", "nsentences": "39.4", "nll_loss": "0.778", "wps": "24831.4", "ups": "3.37", "wpb": "7377.3", "bsz": "39.4", "num_updates": "4800", "lr": "1.812e-05", "gnorm": "213.959", "loss_scale": "2", "train_wall": "59", "gb_free": "17.4", "wall": "1452"}
[2024-09-27 16:58:48,739][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 16:58:48,739][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:58:48,776][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 8
[2024-09-27 16:58:51,439][valid][INFO] - {"epoch": 7, "valid_loss": "39.364", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.206", "valid_uer": "3.911", "valid_wer": "14.229", "valid_raw_wer": "14.229", "valid_wps": "22071.5", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "4984", "valid_best_wer": "14.229"}
[2024-09-27 16:58:51,439][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 4984 updates
[2024-09-27 16:58:51,440][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:58:52,530][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 16:58:53,113][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 4984 updates, score 14.229) (writing took 1.6739602407906204 seconds)
[2024-09-27 16:58:53,114][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2024-09-27 16:58:53,114][train][INFO] - {"epoch": 7, "train_loss": "145.563", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.784", "train_wps": "24311.4", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "4984", "train_lr": "1.88031e-05", "train_gnorm": "212.659", "train_loss_scale": "2", "train_train_wall": "210", "train_gb_free": "19.1", "train_wall": "1511"}
[2024-09-27 16:58:53,115][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 16:58:53,156][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 8
[2024-09-27 16:58:53,276][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 16:58:53,278][fairseq.trainer][INFO] - begin training epoch 8
[2024-09-27 16:58:53,278][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 16:58:58,257][train_inner][INFO] - {"epoch": 8, "update": 7.022, "loss": "135.347", "ntokens": "7373.63", "nsentences": "40.8", "nll_loss": "0.749", "wps": "23059.7", "ups": "3.13", "wpb": "7373.6", "bsz": "40.8", "num_updates": "5000", "lr": "1.88625e-05", "gnorm": "206.187", "loss_scale": "2", "train_wall": "59", "gb_free": "19.3", "wall": "1516"}
[2024-09-27 16:59:57,427][train_inner][INFO] - {"epoch": 8, "update": 7.303, "loss": "141.892", "ntokens": "7361.17", "nsentences": "38.92", "nll_loss": "0.75", "wps": "24881.6", "ups": "3.38", "wpb": "7361.2", "bsz": "38.9", "num_updates": "5200", "lr": "1.9605e-05", "gnorm": "214.861", "loss_scale": "2", "train_wall": "59", "gb_free": "18.3", "wall": "1575"}
[2024-09-27 17:00:56,102][train_inner][INFO] - {"epoch": 8, "update": 7.583, "loss": "136.569", "ntokens": "7278.48", "nsentences": "38.56", "nll_loss": "0.724", "wps": "24809.5", "ups": "3.41", "wpb": "7278.5", "bsz": "38.6", "num_updates": "5400", "lr": "2.03475e-05", "gnorm": "211.919", "loss_scale": "2", "train_wall": "58", "gb_free": "17.7", "wall": "1634"}
[2024-09-27 17:01:55,549][train_inner][INFO] - {"epoch": 8, "update": 7.864, "loss": "131.133", "ntokens": "7380.23", "nsentences": "40.12", "nll_loss": "0.713", "wps": "24829.7", "ups": "3.36", "wpb": "7380.2", "bsz": "40.1", "num_updates": "5600", "lr": "2.109e-05", "gnorm": "207.535", "loss_scale": "2", "train_wall": "59", "gb_free": "18.5", "wall": "1693"}
[2024-09-27 17:02:24,237][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:02:24,237][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:02:24,271][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 9
[2024-09-27 17:02:26,903][valid][INFO] - {"epoch": 8, "valid_loss": "33.694", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.176", "valid_uer": "3.31", "valid_wer": "11.812", "valid_raw_wer": "11.812", "valid_wps": "22098.8", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "5697", "valid_best_wer": "11.812"}
[2024-09-27 17:02:26,903][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 5697 updates
[2024-09-27 17:02:26,904][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:02:28,018][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:02:28,606][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 5697 updates, score 11.812) (writing took 1.7026086458936334 seconds)
[2024-09-27 17:02:28,606][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2024-09-27 17:02:28,607][train][INFO] - {"epoch": 8, "train_loss": "134.179", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.723", "train_wps": "24324.4", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "5697", "train_lr": "2.14501e-05", "train_gnorm": "209.862", "train_loss_scale": "2", "train_train_wall": "210", "train_gb_free": "18.9", "train_wall": "1726"}
[2024-09-27 17:02:28,608][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:02:28,649][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 9
[2024-09-27 17:02:28,769][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:02:28,771][fairseq.trainer][INFO] - begin training epoch 9
[2024-09-27 17:02:28,772][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:02:59,508][train_inner][INFO] - {"epoch": 9, "update": 8.144, "loss": "126.451", "ntokens": "7382.46", "nsentences": "40.24", "nll_loss": "0.689", "wps": "23084.8", "ups": "3.13", "wpb": "7382.5", "bsz": "40.2", "num_updates": "5800", "lr": "2.18325e-05", "gnorm": "206.589", "loss_scale": "2", "train_wall": "59", "gb_free": "18.4", "wall": "1757"}
[2024-09-27 17:03:58,444][train_inner][INFO] - {"epoch": 9, "update": 8.425, "loss": "132.429", "ntokens": "7352.35", "nsentences": "38.44", "nll_loss": "0.692", "wps": "24950.3", "ups": "3.39", "wpb": "7352.4", "bsz": "38.4", "num_updates": "6000", "lr": "2.2575e-05", "gnorm": "211.551", "loss_scale": "2", "train_wall": "59", "gb_free": "18.5", "wall": "1816"}
[2024-09-27 17:04:57,751][train_inner][INFO] - {"epoch": 9, "update": 8.705, "loss": "121.793", "ntokens": "7354.34", "nsentences": "40.4", "nll_loss": "0.669", "wps": "24801.1", "ups": "3.37", "wpb": "7354.3", "bsz": "40.4", "num_updates": "6200", "lr": "2.33175e-05", "gnorm": "206.407", "loss_scale": "2", "train_wall": "59", "gb_free": "18.1", "wall": "1875"}
[2024-09-27 17:05:56,914][train_inner][INFO] - {"epoch": 9, "update": 8.986, "loss": "120.022", "ntokens": "7346.52", "nsentences": "40.16", "nll_loss": "0.656", "wps": "24834.9", "ups": "3.38", "wpb": "7346.5", "bsz": "40.2", "num_updates": "6400", "lr": "2.406e-05", "gnorm": "207.037", "loss_scale": "2", "train_wall": "59", "gb_free": "18.1", "wall": "1934"}
[2024-09-27 17:05:59,805][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:05:59,806][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:05:59,840][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 10
[2024-09-27 17:06:02,472][valid][INFO] - {"epoch": 9, "valid_loss": "29.792", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.156", "valid_uer": "2.833", "valid_wer": "10.072", "valid_raw_wer": "10.072", "valid_wps": "22085.7", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "6410", "valid_best_wer": "10.072"}
[2024-09-27 17:06:02,473][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 6410 updates
[2024-09-27 17:06:02,474][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:06:03,634][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:06:04,221][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 9 @ 6410 updates, score 10.072) (writing took 1.74828058690764 seconds)
[2024-09-27 17:06:04,222][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2024-09-27 17:06:04,222][train][INFO] - {"epoch": 9, "train_loss": "125.413", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.676", "train_wps": "24310.5", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "6410", "train_lr": "2.40971e-05", "train_gnorm": "208.619", "train_loss_scale": "2", "train_train_wall": "210", "train_gb_free": "19.6", "train_wall": "1942"}
[2024-09-27 17:06:04,223][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:06:04,264][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 10
[2024-09-27 17:06:04,380][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:06:04,382][fairseq.trainer][INFO] - begin training epoch 10
[2024-09-27 17:06:04,382][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:07:00,846][train_inner][INFO] - {"epoch": 10, "update": 9.266, "loss": "117.757", "ntokens": "7368.53", "nsentences": "39.96", "nll_loss": "0.639", "wps": "23051.1", "ups": "3.13", "wpb": "7368.5", "bsz": "40", "num_updates": "6600", "lr": "2.48025e-05", "gnorm": "216.326", "loss_scale": "2", "train_wall": "59", "gb_free": "19", "wall": "1998"}
[2024-09-27 17:07:59,830][train_inner][INFO] - {"epoch": 10, "update": 9.547, "loss": "120.153", "ntokens": "7303.27", "nsentences": "38.84", "nll_loss": "0.639", "wps": "24763.8", "ups": "3.39", "wpb": "7303.3", "bsz": "38.8", "num_updates": "6800", "lr": "2.5545e-05", "gnorm": "211.906", "loss_scale": "2", "train_wall": "59", "gb_free": "17.5", "wall": "2057"}
[2024-09-27 17:08:59,227][train_inner][INFO] - {"epoch": 10, "update": 9.827, "loss": "118.185", "ntokens": "7400.05", "nsentences": "39.8", "nll_loss": "0.636", "wps": "24917.2", "ups": "3.37", "wpb": "7400", "bsz": "39.8", "num_updates": "7000", "lr": "2.62875e-05", "gnorm": "204.211", "loss_scale": "2", "train_wall": "59", "gb_free": "19.3", "wall": "2117"}
[2024-09-27 17:09:35,706][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:09:35,706][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:09:35,740][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 11
[2024-09-27 17:09:38,385][valid][INFO] - {"epoch": 10, "valid_loss": "27.181", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.142", "valid_uer": "2.674", "valid_wer": "9.508", "valid_raw_wer": "9.508", "valid_wps": "22114.7", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "7123", "valid_best_wer": "9.508"}
[2024-09-27 17:09:38,386][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 7123 updates
[2024-09-27 17:09:38,386][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:09:39,530][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:09:40,111][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 10 @ 7123 updates, score 9.508) (writing took 1.7253869879059494 seconds)
[2024-09-27 17:09:40,111][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2024-09-27 17:09:40,112][train][INFO] - {"epoch": 10, "train_loss": "117.925", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.635", "train_wps": "24279.6", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "7123", "train_lr": "2.67441e-05", "train_gnorm": "210.682", "train_loss_scale": "2", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "2158"}
[2024-09-27 17:09:40,112][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:09:40,154][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 11
[2024-09-27 17:09:40,273][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:09:40,275][fairseq.trainer][INFO] - begin training epoch 11
[2024-09-27 17:09:40,275][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:10:03,085][train_inner][INFO] - {"epoch": 11, "update": 10.108, "loss": "114.496", "ntokens": "7349.3", "nsentences": "40.36", "nll_loss": "0.629", "wps": "23017.7", "ups": "3.13", "wpb": "7349.3", "bsz": "40.4", "num_updates": "7200", "lr": "2.703e-05", "gnorm": "207.35", "loss_scale": "2", "train_wall": "59", "gb_free": "19.1", "wall": "2180"}
[2024-09-27 17:11:02,478][train_inner][INFO] - {"epoch": 11, "update": 10.388, "loss": "110.828", "ntokens": "7371.29", "nsentences": "40.04", "nll_loss": "0.602", "wps": "24822.5", "ups": "3.37", "wpb": "7371.3", "bsz": "40", "num_updates": "7400", "lr": "2.77725e-05", "gnorm": "199.24", "loss_scale": "2", "train_wall": "59", "gb_free": "19.4", "wall": "2240"}
[2024-09-27 17:12:01,330][train_inner][INFO] - {"epoch": 11, "update": 10.669, "loss": "119.177", "ntokens": "7320.57", "nsentences": "38.28", "nll_loss": "0.623", "wps": "24877.8", "ups": "3.4", "wpb": "7320.6", "bsz": "38.3", "num_updates": "7600", "lr": "2.8515e-05", "gnorm": "216.182", "loss_scale": "2", "train_wall": "58", "gb_free": "18.5", "wall": "2299"}
[2024-09-27 17:13:00,716][train_inner][INFO] - {"epoch": 11, "update": 10.95, "loss": "110.872", "ntokens": "7350.03", "nsentences": "39.88", "nll_loss": "0.602", "wps": "24753.6", "ups": "3.37", "wpb": "7350", "bsz": "39.9", "num_updates": "7800", "lr": "2.92575e-05", "gnorm": "205.139", "loss_scale": "2", "train_wall": "59", "gb_free": "19.1", "wall": "2358"}
[2024-09-27 17:13:11,367][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:13:11,367][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:13:11,401][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 12
[2024-09-27 17:13:14,023][valid][INFO] - {"epoch": 11, "valid_loss": "25.308", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.132", "valid_uer": "2.443", "valid_wer": "8.784", "valid_raw_wer": "8.784", "valid_wps": "22127.9", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "7836", "valid_best_wer": "8.784"}
[2024-09-27 17:13:14,023][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 7836 updates
[2024-09-27 17:13:14,024][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:13:15,148][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:13:15,743][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 11 @ 7836 updates, score 8.784) (writing took 1.7194408290088177 seconds)
[2024-09-27 17:13:15,743][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2024-09-27 17:13:15,743][train][INFO] - {"epoch": 11, "train_loss": "113.27", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.61", "train_wps": "24308.6", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "7836", "train_lr": "2.93912e-05", "train_gnorm": "206.286", "train_loss_scale": "2", "train_train_wall": "210", "train_gb_free": "19.6", "train_wall": "2373"}
[2024-09-27 17:13:15,744][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:13:15,786][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 12
[2024-09-27 17:13:15,904][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:13:15,906][fairseq.trainer][INFO] - begin training epoch 12
[2024-09-27 17:13:15,906][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:14:04,406][train_inner][INFO] - {"epoch": 12, "update": 11.23, "loss": "108.495", "ntokens": "7358.22", "nsentences": "39.92", "nll_loss": "0.589", "wps": "23106.5", "ups": "3.14", "wpb": "7358.2", "bsz": "39.9", "num_updates": "8000", "lr": "3e-05", "gnorm": "201.303", "loss_scale": "2", "train_wall": "59", "gb_free": "18.2", "wall": "2422"}
[2024-09-27 17:15:03,618][train_inner][INFO] - {"epoch": 12, "update": 11.511, "loss": "110.689", "ntokens": "7324.4", "nsentences": "38.96", "nll_loss": "0.589", "wps": "24739.4", "ups": "3.38", "wpb": "7324.4", "bsz": "39", "num_updates": "8200", "lr": "3e-05", "gnorm": "202.819", "loss_scale": "2", "train_wall": "59", "gb_free": "19.4", "wall": "2481"}
[2024-09-27 17:16:02,735][train_inner][INFO] - {"epoch": 12, "update": 11.791, "loss": "110.924", "ntokens": "7339.27", "nsentences": "39.12", "nll_loss": "0.591", "wps": "24829.9", "ups": "3.38", "wpb": "7339.3", "bsz": "39.1", "num_updates": "8400", "lr": "3e-05", "gnorm": "203.187", "loss_scale": "4", "train_wall": "59", "gb_free": "19.3", "wall": "2540"}
[2024-09-27 17:16:47,157][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:16:47,158][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:16:47,192][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 13
[2024-09-27 17:16:49,838][valid][INFO] - {"epoch": 12, "valid_loss": "23.46", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.123", "valid_uer": "2.305", "valid_wer": "8.154", "valid_raw_wer": "8.154", "valid_wps": "22085.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "8549", "valid_best_wer": "8.154"}
[2024-09-27 17:16:49,839][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 8549 updates
[2024-09-27 17:16:49,839][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:16:51,006][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:16:51,596][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 12 @ 8549 updates, score 8.154) (writing took 1.75776164396666 seconds)
[2024-09-27 17:16:51,597][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2024-09-27 17:16:51,597][train][INFO] - {"epoch": 12, "train_loss": "108.975", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.587", "train_wps": "24283.7", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "8549", "train_lr": "3e-05", "train_gnorm": "201.056", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "2589"}
[2024-09-27 17:16:51,598][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:16:51,639][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 13
[2024-09-27 17:16:51,760][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:16:51,762][fairseq.trainer][INFO] - begin training epoch 13
[2024-09-27 17:16:51,762][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:17:07,092][train_inner][INFO] - {"epoch": 13, "update": 12.072, "loss": "105.64", "ntokens": "7384.43", "nsentences": "40.2", "nll_loss": "0.575", "wps": "22948.3", "ups": "3.11", "wpb": "7384.4", "bsz": "40.2", "num_updates": "8600", "lr": "3e-05", "gnorm": "199.03", "loss_scale": "4", "train_wall": "59", "gb_free": "19.3", "wall": "2604"}
[2024-09-27 17:18:06,464][train_inner][INFO] - {"epoch": 13, "update": 12.352, "loss": "105.362", "ntokens": "7401.41", "nsentences": "40.48", "nll_loss": "0.576", "wps": "24932.3", "ups": "3.37", "wpb": "7401.4", "bsz": "40.5", "num_updates": "8800", "lr": "3e-05", "gnorm": "198.556", "loss_scale": "4", "train_wall": "59", "gb_free": "17.9", "wall": "2664"}
[2024-09-27 17:19:05,915][train_inner][INFO] - {"epoch": 13, "update": 12.633, "loss": "105.397", "ntokens": "7375.61", "nsentences": "39.32", "nll_loss": "0.562", "wps": "24812.7", "ups": "3.36", "wpb": "7375.6", "bsz": "39.3", "num_updates": "9000", "lr": "3e-05", "gnorm": "200.225", "loss_scale": "4", "train_wall": "59", "gb_free": "17.6", "wall": "2723"}
[2024-09-27 17:20:05,021][train_inner][INFO] - {"epoch": 13, "update": 12.913, "loss": "105.19", "ntokens": "7324.74", "nsentences": "39.64", "nll_loss": "0.569", "wps": "24784.9", "ups": "3.38", "wpb": "7324.7", "bsz": "39.6", "num_updates": "9200", "lr": "3e-05", "gnorm": "200.864", "loss_scale": "4", "train_wall": "59", "gb_free": "19.4", "wall": "2782"}
[2024-09-27 17:20:23,004][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:20:23,004][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:20:23,039][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 14
[2024-09-27 17:20:25,663][valid][INFO] - {"epoch": 13, "valid_loss": "22.373", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.117", "valid_uer": "2.15", "valid_wer": "7.618", "valid_raw_wer": "7.618", "valid_wps": "22106.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "9262", "valid_best_wer": "7.618"}
[2024-09-27 17:20:25,663][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 9262 updates
[2024-09-27 17:20:25,664][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:20:26,786][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:20:27,382][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 13 @ 9262 updates, score 7.618) (writing took 1.7182791088707745 seconds)
[2024-09-27 17:20:27,382][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2024-09-27 17:20:27,382][train][INFO] - {"epoch": 13, "train_loss": "105.776", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.57", "train_wps": "24291.4", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "9262", "train_lr": "3e-05", "train_gnorm": "200.584", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "18.4", "train_wall": "2805"}
[2024-09-27 17:20:27,383][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:20:27,424][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 14
[2024-09-27 17:20:27,547][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:20:27,549][fairseq.trainer][INFO] - begin training epoch 14
[2024-09-27 17:20:27,549][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:21:08,199][train_inner][INFO] - {"epoch": 14, "update": 13.194, "loss": "107.043", "ntokens": "7271.31", "nsentences": "38.72", "nll_loss": "0.57", "wps": "23018.5", "ups": "3.17", "wpb": "7271.3", "bsz": "38.7", "num_updates": "9400", "lr": "3e-05", "gnorm": "202.404", "loss_scale": "4", "train_wall": "58", "gb_free": "19.3", "wall": "2846"}
[2024-09-27 17:22:07,634][train_inner][INFO] - {"epoch": 14, "update": 13.474, "loss": "103.894", "ntokens": "7378.61", "nsentences": "39.6", "nll_loss": "0.558", "wps": "24829.5", "ups": "3.37", "wpb": "7378.6", "bsz": "39.6", "num_updates": "9600", "lr": "3e-05", "gnorm": "204.986", "loss_scale": "4", "train_wall": "59", "gb_free": "17.5", "wall": "2905"}
[2024-09-27 17:23:06,540][train_inner][INFO] - {"epoch": 14, "update": 13.755, "loss": "102.521", "ntokens": "7340.22", "nsentences": "39.8", "nll_loss": "0.556", "wps": "24921.8", "ups": "3.4", "wpb": "7340.2", "bsz": "39.8", "num_updates": "9800", "lr": "3e-05", "gnorm": "199.943", "loss_scale": "4", "train_wall": "59", "gb_free": "19.4", "wall": "2964"}
[2024-09-27 17:23:58,711][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:23:58,712][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:23:58,746][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 15
[2024-09-27 17:24:01,389][valid][INFO] - {"epoch": 14, "valid_loss": "21.645", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.113", "valid_uer": "2.111", "valid_wer": "7.42", "valid_raw_wer": "7.42", "valid_wps": "22024.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "9975", "valid_best_wer": "7.42"}
[2024-09-27 17:24:01,390][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 9975 updates
[2024-09-27 17:24:01,390][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:24:02,444][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:24:03,030][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 14 @ 9975 updates, score 7.42) (writing took 1.6399560950230807 seconds)
[2024-09-27 17:24:03,030][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2024-09-27 17:24:03,030][train][INFO] - {"epoch": 14, "train_loss": "102.805", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.554", "train_wps": "24306.8", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "9975", "train_lr": "3e-05", "train_gnorm": "201.053", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "18.8", "train_wall": "3020"}
[2024-09-27 17:24:03,031][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:24:03,073][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 15
[2024-09-27 17:24:03,191][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:24:03,193][fairseq.trainer][INFO] - begin training epoch 15
[2024-09-27 17:24:03,194][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:24:10,572][train_inner][INFO] - {"epoch": 15, "update": 14.035, "loss": "100.357", "ntokens": "7347.41", "nsentences": "39.64", "nll_loss": "0.541", "wps": "22949.2", "ups": "3.12", "wpb": "7347.4", "bsz": "39.6", "num_updates": "10000", "lr": "3e-05", "gnorm": "197.076", "loss_scale": "4", "train_wall": "59", "gb_free": "19.1", "wall": "3028"}
[2024-09-27 17:25:10,009][train_inner][INFO] - {"epoch": 15, "update": 14.316, "loss": "103.006", "ntokens": "7412.19", "nsentences": "39.32", "nll_loss": "0.546", "wps": "24941.3", "ups": "3.36", "wpb": "7412.2", "bsz": "39.3", "num_updates": "10200", "lr": "3e-05", "gnorm": "200.003", "loss_scale": "4", "train_wall": "59", "gb_free": "19.3", "wall": "3087"}
[2024-09-27 17:26:09,214][train_inner][INFO] - {"epoch": 15, "update": 14.596, "loss": "97.679", "ntokens": "7346.44", "nsentences": "40.52", "nll_loss": "0.539", "wps": "24817.4", "ups": "3.38", "wpb": "7346.4", "bsz": "40.5", "num_updates": "10400", "lr": "3e-05", "gnorm": "193.049", "loss_scale": "4", "train_wall": "59", "gb_free": "19.2", "wall": "3147"}
[2024-09-27 17:27:08,492][train_inner][INFO] - {"epoch": 15, "update": 14.877, "loss": "98.841", "ntokens": "7349.05", "nsentences": "39.68", "nll_loss": "0.534", "wps": "24795.1", "ups": "3.37", "wpb": "7349", "bsz": "39.7", "num_updates": "10600", "lr": "3e-05", "gnorm": "198.4", "loss_scale": "4", "train_wall": "59", "gb_free": "19.2", "wall": "3206"}
[2024-09-27 17:27:34,252][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:27:34,252][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:27:34,286][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 16
[2024-09-27 17:27:36,950][valid][INFO] - {"epoch": 15, "valid_loss": "20.929", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.109", "valid_uer": "2.046", "valid_wer": "7.204", "valid_raw_wer": "7.204", "valid_wps": "22069.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "10688", "valid_best_wer": "7.204"}
[2024-09-27 17:27:36,951][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 10688 updates
[2024-09-27 17:27:36,951][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:27:38,009][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:27:38,590][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 15 @ 10688 updates, score 7.204) (writing took 1.6393211269751191 seconds)
[2024-09-27 17:27:38,590][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2024-09-27 17:27:38,591][train][INFO] - {"epoch": 15, "train_loss": "100.685", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.542", "train_wps": "24316.7", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "10688", "train_lr": "3e-05", "train_gnorm": "197.664", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "19", "train_wall": "3236"}
[2024-09-27 17:27:38,592][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:27:38,633][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 16
[2024-09-27 17:27:38,753][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:27:38,755][fairseq.trainer][INFO] - begin training epoch 16
[2024-09-27 17:27:38,755][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:28:11,866][train_inner][INFO] - {"epoch": 16, "update": 15.157, "loss": "101.776", "ntokens": "7309.26", "nsentences": "38.92", "nll_loss": "0.542", "wps": "23067.3", "ups": "3.16", "wpb": "7309.3", "bsz": "38.9", "num_updates": "10800", "lr": "3e-05", "gnorm": "201.216", "loss_scale": "4", "train_wall": "58", "gb_free": "18.1", "wall": "3269"}
[2024-09-27 17:29:10,650][train_inner][INFO] - {"epoch": 16, "update": 15.438, "loss": "99.546", "ntokens": "7287.64", "nsentences": "39.2", "nll_loss": "0.535", "wps": "24794.4", "ups": "3.4", "wpb": "7287.6", "bsz": "39.2", "num_updates": "11000", "lr": "3e-05", "gnorm": "196.984", "loss_scale": "4", "train_wall": "58", "gb_free": "19.2", "wall": "3328"}
[2024-09-27 17:30:10,345][train_inner][INFO] - {"epoch": 16, "update": 15.718, "loss": "96.543", "ntokens": "7391.69", "nsentences": "39.84", "nll_loss": "0.52", "wps": "24765", "ups": "3.35", "wpb": "7391.7", "bsz": "39.8", "num_updates": "11200", "lr": "3e-05", "gnorm": "195.625", "loss_scale": "4", "train_wall": "59", "gb_free": "18.7", "wall": "3388"}
[2024-09-27 17:31:10,106][train_inner][INFO] - {"epoch": 16, "update": 15.999, "loss": "96.848", "ntokens": "7393.32", "nsentences": "40.12", "nll_loss": "0.526", "wps": "24743.2", "ups": "3.35", "wpb": "7393.3", "bsz": "40.1", "num_updates": "11400", "lr": "3e-05", "gnorm": "196.404", "loss_scale": "4", "train_wall": "59", "gb_free": "18.1", "wall": "3448"}
[2024-09-27 17:31:10,330][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:31:10,330][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:31:10,364][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 17
[2024-09-27 17:31:13,002][valid][INFO] - {"epoch": 16, "valid_loss": "20.085", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.105", "valid_uer": "1.959", "valid_wer": "6.912", "valid_raw_wer": "6.912", "valid_wps": "22078.7", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "11401", "valid_best_wer": "6.912"}
[2024-09-27 17:31:13,002][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 11401 updates
[2024-09-27 17:31:13,003][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:31:14,104][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:31:14,693][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 16 @ 11401 updates, score 6.912) (writing took 1.690334386890754 seconds)
[2024-09-27 17:31:14,693][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2024-09-27 17:31:14,693][train][INFO] - {"epoch": 16, "train_loss": "97.948", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.528", "train_wps": "24255.7", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "11401", "train_lr": "3e-05", "train_gnorm": "197.132", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "18.9", "train_wall": "3452"}
[2024-09-27 17:31:14,694][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:31:14,735][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 17
[2024-09-27 17:31:14,858][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:31:14,860][fairseq.trainer][INFO] - begin training epoch 17
[2024-09-27 17:31:14,860][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:32:13,525][train_inner][INFO] - {"epoch": 17, "update": 16.279, "loss": "97.823", "ntokens": "7283.69", "nsentences": "38.88", "nll_loss": "0.522", "wps": "22970.1", "ups": "3.15", "wpb": "7283.7", "bsz": "38.9", "num_updates": "11600", "lr": "3e-05", "gnorm": "197.644", "loss_scale": "4", "train_wall": "58", "gb_free": "19.5", "wall": "3511"}
[2024-09-27 17:33:13,263][train_inner][INFO] - {"epoch": 17, "update": 16.56, "loss": "93.204", "ntokens": "7399.11", "nsentences": "40.44", "nll_loss": "0.509", "wps": "24771.9", "ups": "3.35", "wpb": "7399.1", "bsz": "40.4", "num_updates": "11800", "lr": "3e-05", "gnorm": "194.752", "loss_scale": "4", "train_wall": "59", "gb_free": "19.3", "wall": "3571"}
[2024-09-27 17:34:12,583][train_inner][INFO] - {"epoch": 17, "update": 16.84, "loss": "96.423", "ntokens": "7370.42", "nsentences": "39.8", "nll_loss": "0.521", "wps": "24849.8", "ups": "3.37", "wpb": "7370.4", "bsz": "39.8", "num_updates": "12000", "lr": "3e-05", "gnorm": "194.416", "loss_scale": "4", "train_wall": "59", "gb_free": "19.4", "wall": "3630"}
[2024-09-27 17:34:46,219][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:34:46,219][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:34:46,253][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 18
[2024-09-27 17:34:48,899][valid][INFO] - {"epoch": 17, "valid_loss": "19.443", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.102", "valid_uer": "1.862", "valid_wer": "6.555", "valid_raw_wer": "6.555", "valid_wps": "22112.9", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "12114", "valid_best_wer": "6.555"}
[2024-09-27 17:34:48,899][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 12114 updates
[2024-09-27 17:34:48,900][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:34:50,001][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:34:50,587][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 17 @ 12114 updates, score 6.555) (writing took 1.6879957560449839 seconds)
[2024-09-27 17:34:50,588][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2024-09-27 17:34:50,588][train][INFO] - {"epoch": 17, "train_loss": "95.994", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.517", "train_wps": "24279", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "12114", "train_lr": "3e-05", "train_gnorm": "195.618", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "19.4", "train_wall": "3668"}
[2024-09-27 17:34:50,589][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:34:50,630][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 18
[2024-09-27 17:34:50,753][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:34:50,755][fairseq.trainer][INFO] - begin training epoch 18
[2024-09-27 17:34:50,755][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:35:16,573][train_inner][INFO] - {"epoch": 18, "update": 17.121, "loss": "94.779", "ntokens": "7380.06", "nsentences": "39.88", "nll_loss": "0.512", "wps": "23066.1", "ups": "3.13", "wpb": "7380.1", "bsz": "39.9", "num_updates": "12200", "lr": "3e-05", "gnorm": "191.633", "loss_scale": "4", "train_wall": "59", "gb_free": "19.1", "wall": "3694"}
[2024-09-27 17:36:15,764][train_inner][INFO] - {"epoch": 18, "update": 17.401, "loss": "96.897", "ntokens": "7326.63", "nsentences": "38.8", "nll_loss": "0.513", "wps": "24756", "ups": "3.38", "wpb": "7326.6", "bsz": "38.8", "num_updates": "12400", "lr": "3e-05", "gnorm": "196.386", "loss_scale": "4", "train_wall": "59", "gb_free": "18.4", "wall": "3753"}
[2024-09-27 17:37:15,534][train_inner][INFO] - {"epoch": 18, "update": 17.682, "loss": "93.061", "ntokens": "7424.11", "nsentences": "40.6", "nll_loss": "0.509", "wps": "24842.4", "ups": "3.35", "wpb": "7424.1", "bsz": "40.6", "num_updates": "12600", "lr": "3e-05", "gnorm": "193.876", "loss_scale": "8", "train_wall": "59", "gb_free": "18.5", "wall": "3813"}
[2024-09-27 17:38:14,229][train_inner][INFO] - {"epoch": 18, "update": 17.962, "loss": "95.691", "ntokens": "7290.87", "nsentences": "39.12", "nll_loss": "0.513", "wps": "24843.4", "ups": "3.41", "wpb": "7290.9", "bsz": "39.1", "num_updates": "12800", "lr": "3e-05", "gnorm": "196.654", "loss_scale": "8", "train_wall": "58", "gb_free": "19.2", "wall": "3872"}
[2024-09-27 17:38:22,023][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:38:22,023][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:38:22,063][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 19
[2024-09-27 17:38:24,710][valid][INFO] - {"epoch": 18, "valid_loss": "19.037", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.1", "valid_uer": "1.816", "valid_wer": "6.452", "valid_raw_wer": "6.452", "valid_wps": "22066.6", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "12827", "valid_best_wer": "6.452"}
[2024-09-27 17:38:24,710][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 12827 updates
[2024-09-27 17:38:24,711][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:38:25,881][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:38:26,475][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 18 @ 12827 updates, score 6.452) (writing took 1.764457313111052 seconds)
[2024-09-27 17:38:26,475][fairseq_cli.train][INFO] - end of epoch 18 (average epoch stats below)
[2024-09-27 17:38:26,475][train][INFO] - {"epoch": 18, "train_loss": "94.825", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.511", "train_wps": "24279.9", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "12827", "train_lr": "3e-05", "train_gnorm": "194.56", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "18.9", "train_wall": "3884"}
[2024-09-27 17:38:26,477][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:38:26,519][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 19
[2024-09-27 17:38:26,638][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:38:26,640][fairseq.trainer][INFO] - begin training epoch 19
[2024-09-27 17:38:26,640][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:39:18,154][train_inner][INFO] - {"epoch": 19, "update": 18.243, "loss": "95.829", "ntokens": "7344.5", "nsentences": "39.08", "nll_loss": "0.51", "wps": "22978.4", "ups": "3.13", "wpb": "7344.5", "bsz": "39.1", "num_updates": "13000", "lr": "3e-05", "gnorm": "195.565", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "3936"}
[2024-09-27 17:40:17,357][train_inner][INFO] - {"epoch": 19, "update": 18.523, "loss": "93.66", "ntokens": "7391.23", "nsentences": "40.28", "nll_loss": "0.51", "wps": "24969.3", "ups": "3.38", "wpb": "7391.2", "bsz": "40.3", "num_updates": "13200", "lr": "3e-05", "gnorm": "193.658", "loss_scale": "8", "train_wall": "59", "gb_free": "17.8", "wall": "3995"}
[2024-09-27 17:41:16,518][train_inner][INFO] - {"epoch": 19, "update": 18.804, "loss": "93.504", "ntokens": "7308.12", "nsentences": "39.08", "nll_loss": "0.5", "wps": "24705.7", "ups": "3.38", "wpb": "7308.1", "bsz": "39.1", "num_updates": "13400", "lr": "3e-05", "gnorm": "195.257", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "4054"}
[2024-09-27 17:41:57,644][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:41:57,644][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:41:57,679][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 20
[2024-09-27 17:42:00,332][valid][INFO] - {"epoch": 19, "valid_loss": "18.757", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.098", "valid_uer": "1.849", "valid_wer": "6.527", "valid_raw_wer": "6.527", "valid_wps": "22129.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "13540", "valid_best_wer": "6.452"}
[2024-09-27 17:42:00,332][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 19 @ 13540 updates
[2024-09-27 17:42:00,333][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 17:42:01,473][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 17:42:01,484][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 19 @ 13540 updates, score 6.527) (writing took 1.15130720497109 seconds)
[2024-09-27 17:42:01,484][fairseq_cli.train][INFO] - end of epoch 19 (average epoch stats below)
[2024-09-27 17:42:01,484][train][INFO] - {"epoch": 19, "train_loss": "94.06", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.507", "train_wps": "24379.1", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "13540", "train_lr": "3e-05", "train_gnorm": "194.059", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "17.9", "train_wall": "4099"}
[2024-09-27 17:42:01,485][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:42:01,526][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 20
[2024-09-27 17:42:01,647][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:42:01,649][fairseq.trainer][INFO] - begin training epoch 20
[2024-09-27 17:42:01,649][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:42:19,731][train_inner][INFO] - {"epoch": 20, "update": 19.084, "loss": "90.952", "ntokens": "7373.15", "nsentences": "40.6", "nll_loss": "0.501", "wps": "23328.2", "ups": "3.16", "wpb": "7373.2", "bsz": "40.6", "num_updates": "13600", "lr": "3e-05", "gnorm": "189.027", "loss_scale": "8", "train_wall": "59", "gb_free": "18.1", "wall": "4117"}
[2024-09-27 17:43:19,090][train_inner][INFO] - {"epoch": 20, "update": 19.365, "loss": "92.1", "ntokens": "7342.44", "nsentences": "39.32", "nll_loss": "0.493", "wps": "24739.2", "ups": "3.37", "wpb": "7342.4", "bsz": "39.3", "num_updates": "13800", "lr": "3e-05", "gnorm": "193.409", "loss_scale": "8", "train_wall": "59", "gb_free": "19.5", "wall": "4176"}
[2024-09-27 17:44:18,340][train_inner][INFO] - {"epoch": 20, "update": 19.645, "loss": "92.012", "ntokens": "7345.12", "nsentences": "39.56", "nll_loss": "0.496", "wps": "24793.8", "ups": "3.38", "wpb": "7345.1", "bsz": "39.6", "num_updates": "14000", "lr": "3e-05", "gnorm": "191.582", "loss_scale": "8", "train_wall": "59", "gb_free": "17.5", "wall": "4236"}
[2024-09-27 17:45:17,416][train_inner][INFO] - {"epoch": 20, "update": 19.926, "loss": "97.248", "ntokens": "7323.26", "nsentences": "37.88", "nll_loss": "0.503", "wps": "24792.6", "ups": "3.39", "wpb": "7323.3", "bsz": "37.9", "num_updates": "14200", "lr": "3e-05", "gnorm": "199.203", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "4295"}
[2024-09-27 17:45:33,012][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:45:33,012][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:45:33,046][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 21
[2024-09-27 17:45:35,703][valid][INFO] - {"epoch": 20, "valid_loss": "18.786", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.098", "valid_uer": "1.807", "valid_wer": "6.273", "valid_raw_wer": "6.273", "valid_wps": "22115.6", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "14253", "valid_best_wer": "6.273"}
[2024-09-27 17:45:35,704][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 20 @ 14253 updates
[2024-09-27 17:45:35,704][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:45:36,853][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:45:37,427][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 20 @ 14253 updates, score 6.273) (writing took 1.7230402831919491 seconds)
[2024-09-27 17:45:37,427][fairseq_cli.train][INFO] - end of epoch 20 (average epoch stats below)
[2024-09-27 17:45:37,427][train][INFO] - {"epoch": 20, "train_loss": "92.043", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.496", "train_wps": "24273.6", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "14253", "train_lr": "3e-05", "train_gnorm": "192.881", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.6", "train_wall": "4315"}
[2024-09-27 17:45:37,428][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:45:37,469][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 21
[2024-09-27 17:45:37,591][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:45:37,593][fairseq.trainer][INFO] - begin training epoch 21
[2024-09-27 17:45:37,594][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:46:21,101][train_inner][INFO] - {"epoch": 21, "update": 20.206, "loss": "90.042", "ntokens": "7359.56", "nsentences": "41.04", "nll_loss": "0.502", "wps": "23112.6", "ups": "3.14", "wpb": "7359.6", "bsz": "41", "num_updates": "14400", "lr": "3e-05", "gnorm": "190.773", "loss_scale": "8", "train_wall": "59", "gb_free": "17.8", "wall": "4359"}
[2024-09-27 17:47:20,252][train_inner][INFO] - {"epoch": 21, "update": 20.487, "loss": "93.993", "ntokens": "7326.06", "nsentences": "38.72", "nll_loss": "0.497", "wps": "24770.5", "ups": "3.38", "wpb": "7326.1", "bsz": "38.7", "num_updates": "14600", "lr": "3e-05", "gnorm": "197.269", "loss_scale": "8", "train_wall": "59", "gb_free": "17.9", "wall": "4418"}
[2024-09-27 17:48:19,572][train_inner][INFO] - {"epoch": 21, "update": 20.767, "loss": "88.762", "ntokens": "7355.69", "nsentences": "40.24", "nll_loss": "0.486", "wps": "24800.2", "ups": "3.37", "wpb": "7355.7", "bsz": "40.2", "num_updates": "14800", "lr": "3e-05", "gnorm": "191.338", "loss_scale": "8", "train_wall": "59", "gb_free": "18.2", "wall": "4477"}
[2024-09-27 17:49:09,115][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:49:09,116][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:49:09,150][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 22
[2024-09-27 17:49:11,793][valid][INFO] - {"epoch": 21, "valid_loss": "18.439", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.096", "valid_uer": "1.805", "valid_wer": "6.198", "valid_raw_wer": "6.198", "valid_wps": "22075.2", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "14966", "valid_best_wer": "6.198"}
[2024-09-27 17:49:11,794][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 21 @ 14966 updates
[2024-09-27 17:49:11,794][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:49:12,900][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:49:13,479][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 21 @ 14966 updates, score 6.198) (writing took 1.6848690649494529 seconds)
[2024-09-27 17:49:13,479][fairseq_cli.train][INFO] - end of epoch 21 (average epoch stats below)
[2024-09-27 17:49:13,479][train][INFO] - {"epoch": 21, "train_loss": "91.363", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.492", "train_wps": "24261.4", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "14966", "train_lr": "3e-05", "train_gnorm": "193.617", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.2", "train_wall": "4531"}
[2024-09-27 17:49:13,480][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:49:13,521][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 22
[2024-09-27 17:49:13,641][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:49:13,643][fairseq.trainer][INFO] - begin training epoch 22
[2024-09-27 17:49:13,643][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:49:24,010][train_inner][INFO] - {"epoch": 22, "update": 21.048, "loss": "88.983", "ntokens": "7403.84", "nsentences": "40.2", "nll_loss": "0.483", "wps": "22979.9", "ups": "3.1", "wpb": "7403.8", "bsz": "40.2", "num_updates": "15000", "lr": "3e-05", "gnorm": "190.133", "loss_scale": "8", "train_wall": "59", "gb_free": "17.8", "wall": "4541"}
[2024-09-27 17:50:23,449][train_inner][INFO] - {"epoch": 22, "update": 21.328, "loss": "92.347", "ntokens": "7363.65", "nsentences": "39.08", "nll_loss": "0.49", "wps": "24777.2", "ups": "3.36", "wpb": "7363.6", "bsz": "39.1", "num_updates": "15200", "lr": "3e-05", "gnorm": "195.916", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "4601"}
[2024-09-27 17:51:22,998][train_inner][INFO] - {"epoch": 22, "update": 21.609, "loss": "89.279", "ntokens": "7382.78", "nsentences": "39.72", "nll_loss": "0.48", "wps": "24795.6", "ups": "3.36", "wpb": "7382.8", "bsz": "39.7", "num_updates": "15400", "lr": "3e-05", "gnorm": "195.855", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "4660"}
[2024-09-27 17:52:21,664][train_inner][INFO] - {"epoch": 22, "update": 21.889, "loss": "88.326", "ntokens": "7289.47", "nsentences": "39.8", "nll_loss": "0.482", "wps": "24851.1", "ups": "3.41", "wpb": "7289.5", "bsz": "39.8", "num_updates": "15600", "lr": "3e-05", "gnorm": "190.532", "loss_scale": "8", "train_wall": "58", "gb_free": "19.2", "wall": "4719"}
[2024-09-27 17:52:44,877][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:52:44,877][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:52:44,912][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 23
[2024-09-27 17:52:47,562][valid][INFO] - {"epoch": 22, "valid_loss": "18.111", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.095", "valid_uer": "1.754", "valid_wer": "6", "valid_raw_wer": "6", "valid_wps": "22086", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "15679", "valid_best_wer": "6"}
[2024-09-27 17:52:47,563][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 22 @ 15679 updates
[2024-09-27 17:52:47,563][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:52:48,729][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:52:49,299][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 22 @ 15679 updates, score 6.0) (writing took 1.736222682055086 seconds)
[2024-09-27 17:52:49,299][fairseq_cli.train][INFO] - end of epoch 22 (average epoch stats below)
[2024-09-27 17:52:49,300][train][INFO] - {"epoch": 22, "train_loss": "90.205", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.486", "train_wps": "24287.4", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "15679", "train_lr": "3e-05", "train_gnorm": "193.668", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19", "train_wall": "4747"}
[2024-09-27 17:52:49,300][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:52:49,341][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 23
[2024-09-27 17:52:49,461][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:52:49,463][fairseq.trainer][INFO] - begin training epoch 23
[2024-09-27 17:52:49,463][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:53:25,516][train_inner][INFO] - {"epoch": 23, "update": 22.17, "loss": "91.999", "ntokens": "7354.27", "nsentences": "38.56", "nll_loss": "0.482", "wps": "23035.3", "ups": "3.13", "wpb": "7354.3", "bsz": "38.6", "num_updates": "15800", "lr": "3e-05", "gnorm": "197.589", "loss_scale": "8", "train_wall": "59", "gb_free": "17.8", "wall": "4783"}
[2024-09-27 17:54:25,248][train_inner][INFO] - {"epoch": 23, "update": 22.45, "loss": "85.773", "ntokens": "7417.32", "nsentences": "41.68", "nll_loss": "0.482", "wps": "24835.3", "ups": "3.35", "wpb": "7417.3", "bsz": "41.7", "num_updates": "16000", "lr": "3e-05", "gnorm": "188.814", "loss_scale": "8", "train_wall": "59", "gb_free": "18", "wall": "4843"}
[2024-09-27 17:55:24,002][train_inner][INFO] - {"epoch": 23, "update": 22.731, "loss": "91.002", "ntokens": "7294.84", "nsentences": "38.56", "nll_loss": "0.481", "wps": "24832.1", "ups": "3.4", "wpb": "7294.8", "bsz": "38.6", "num_updates": "16200", "lr": "3e-05", "gnorm": "196.062", "loss_scale": "8", "train_wall": "58", "gb_free": "19.1", "wall": "4901"}
[2024-09-27 17:56:20,555][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:56:20,555][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:56:20,593][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 24
[2024-09-27 17:56:23,233][valid][INFO] - {"epoch": 23, "valid_loss": "17.433", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.091", "valid_uer": "1.726", "valid_wer": "5.887", "valid_raw_wer": "5.887", "valid_wps": "22119.6", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "16392", "valid_best_wer": "5.887"}
[2024-09-27 17:56:23,234][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 23 @ 16392 updates
[2024-09-27 17:56:23,234][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:56:24,350][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 17:56:24,942][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 23 @ 16392 updates, score 5.887) (writing took 1.708348501008004 seconds)
[2024-09-27 17:56:24,942][fairseq_cli.train][INFO] - end of epoch 23 (average epoch stats below)
[2024-09-27 17:56:24,943][train][INFO] - {"epoch": 23, "train_loss": "89.745", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.484", "train_wps": "24307.4", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "16392", "train_lr": "3e-05", "train_gnorm": "194.137", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.1", "train_wall": "4962"}
[2024-09-27 17:56:24,943][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:56:24,985][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 24
[2024-09-27 17:56:25,104][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:56:25,106][fairseq.trainer][INFO] - begin training epoch 24
[2024-09-27 17:56:25,106][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 17:56:27,583][train_inner][INFO] - {"epoch": 24, "update": 23.011, "loss": "91.388", "ntokens": "7332.65", "nsentences": "39.48", "nll_loss": "0.492", "wps": "23065.4", "ups": "3.15", "wpb": "7332.6", "bsz": "39.5", "num_updates": "16400", "lr": "3e-05", "gnorm": "193.298", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "4965"}
[2024-09-27 17:57:26,778][train_inner][INFO] - {"epoch": 24, "update": 23.292, "loss": "90.632", "ntokens": "7331.73", "nsentences": "39.16", "nll_loss": "0.484", "wps": "24771.5", "ups": "3.38", "wpb": "7331.7", "bsz": "39.2", "num_updates": "16600", "lr": "3e-05", "gnorm": "193.45", "loss_scale": "16", "train_wall": "59", "gb_free": "18.2", "wall": "5024"}
[2024-09-27 17:58:26,024][train_inner][INFO] - {"epoch": 24, "update": 23.572, "loss": "88.24", "ntokens": "7394.33", "nsentences": "40.04", "nll_loss": "0.478", "wps": "24961.8", "ups": "3.38", "wpb": "7394.3", "bsz": "40", "num_updates": "16800", "lr": "3e-05", "gnorm": "190.584", "loss_scale": "16", "train_wall": "59", "gb_free": "19.3", "wall": "5083"}
[2024-09-27 17:59:24,842][train_inner][INFO] - {"epoch": 24, "update": 23.853, "loss": "90.966", "ntokens": "7353.09", "nsentences": "39.32", "nll_loss": "0.486", "wps": "25002.9", "ups": "3.4", "wpb": "7353.1", "bsz": "39.3", "num_updates": "17000", "lr": "3e-05", "gnorm": "194.896", "loss_scale": "16", "train_wall": "58", "gb_free": "17.8", "wall": "5142"}
[2024-09-27 17:59:55,944][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 17:59:55,945][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:59:55,989][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 25
[2024-09-27 17:59:58,658][valid][INFO] - {"epoch": 24, "valid_loss": "17.237", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.09", "valid_uer": "1.701", "valid_wer": "5.953", "valid_raw_wer": "5.953", "valid_wps": "22108", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "17105", "valid_best_wer": "5.887"}
[2024-09-27 17:59:58,659][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 24 @ 17105 updates
[2024-09-27 17:59:58,659][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 17:59:59,821][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 17:59:59,832][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 24 @ 17105 updates, score 5.953) (writing took 1.1730450780596584 seconds)
[2024-09-27 17:59:59,832][fairseq_cli.train][INFO] - end of epoch 24 (average epoch stats below)
[2024-09-27 17:59:59,832][train][INFO] - {"epoch": 24, "train_loss": "89.64", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.483", "train_wps": "24392.6", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "17105", "train_lr": "3e-05", "train_gnorm": "192.989", "train_loss_scale": "16", "train_train_wall": "209", "train_gb_free": "18.4", "train_wall": "5177"}
[2024-09-27 17:59:59,833][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 17:59:59,874][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 25
[2024-09-27 17:59:59,991][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 17:59:59,993][fairseq.trainer][INFO] - begin training epoch 25
[2024-09-27 17:59:59,993][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:00:28,401][train_inner][INFO] - {"epoch": 25, "update": 24.133, "loss": "88.148", "ntokens": "7348.41", "nsentences": "39.64", "nll_loss": "0.476", "wps": "23123.2", "ups": "3.15", "wpb": "7348.4", "bsz": "39.6", "num_updates": "17200", "lr": "3e-05", "gnorm": "192.05", "loss_scale": "16", "train_wall": "59", "gb_free": "19.3", "wall": "5206"}
[2024-09-27 18:00:31,357][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-09-27 18:01:27,954][train_inner][INFO] - {"epoch": 25, "update": 24.415, "loss": "89.2", "ntokens": "7373.5", "nsentences": "39.36", "nll_loss": "0.476", "wps": "24762.9", "ups": "3.36", "wpb": "7373.5", "bsz": "39.4", "num_updates": "17400", "lr": "3e-05", "gnorm": "191.875", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "5265"}
[2024-09-27 18:01:51,945][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-09-27 18:02:27,485][train_inner][INFO] - {"epoch": 25, "update": 24.697, "loss": "90.667", "ntokens": "7312.57", "nsentences": "38.68", "nll_loss": "0.48", "wps": "24567.4", "ups": "3.36", "wpb": "7312.6", "bsz": "38.7", "num_updates": "17600", "lr": "3e-05", "gnorm": "197.263", "loss_scale": "4", "train_wall": "59", "gb_free": "18.9", "wall": "5325"}
[2024-09-27 18:03:26,931][train_inner][INFO] - {"epoch": 25, "update": 24.978, "loss": "84.168", "ntokens": "7347.27", "nsentences": "40.36", "nll_loss": "0.462", "wps": "24719.1", "ups": "3.36", "wpb": "7347.3", "bsz": "40.4", "num_updates": "17800", "lr": "3e-05", "gnorm": "193.452", "loss_scale": "4", "train_wall": "59", "gb_free": "18.5", "wall": "5384"}
[2024-09-27 18:03:31,556][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:03:31,557][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:03:31,591][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 26
[2024-09-27 18:03:34,250][valid][INFO] - {"epoch": 25, "valid_loss": "17.298", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.09", "valid_uer": "1.683", "valid_wer": "5.897", "valid_raw_wer": "5.897", "valid_wps": "22101.9", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "17816", "valid_best_wer": "5.887"}
[2024-09-27 18:03:34,251][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 25 @ 17816 updates
[2024-09-27 18:03:34,251][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:03:35,421][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:03:35,433][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 25 @ 17816 updates, score 5.897) (writing took 1.1822906360030174 seconds)
[2024-09-27 18:03:35,434][fairseq_cli.train][INFO] - end of epoch 25 (average epoch stats below)
[2024-09-27 18:03:35,434][train][INFO] - {"epoch": 25, "train_loss": "87.416", "train_ntokens": "7348.83", "train_nsentences": "39.5499", "train_nll_loss": "0.47", "train_wps": "24234.6", "train_ups": "3.3", "train_wpb": "7348.8", "train_bsz": "39.5", "train_num_updates": "17816", "train_lr": "3e-05", "train_gnorm": "193.081", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "19.6", "train_wall": "5393"}
[2024-09-27 18:03:35,435][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:03:35,476][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 26
[2024-09-27 18:03:35,598][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:03:35,600][fairseq.trainer][INFO] - begin training epoch 26
[2024-09-27 18:03:35,600][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:04:30,721][train_inner][INFO] - {"epoch": 26, "update": 25.258, "loss": "82.749", "ntokens": "7441.6", "nsentences": "41.76", "nll_loss": "0.464", "wps": "23331.7", "ups": "3.14", "wpb": "7441.6", "bsz": "41.8", "num_updates": "18000", "lr": "3e-05", "gnorm": "186.645", "loss_scale": "4", "train_wall": "59", "gb_free": "19", "wall": "5448"}
[2024-09-27 18:05:30,084][train_inner][INFO] - {"epoch": 26, "update": 25.539, "loss": "86.932", "ntokens": "7343.53", "nsentences": "39.24", "nll_loss": "0.465", "wps": "24740.9", "ups": "3.37", "wpb": "7343.5", "bsz": "39.2", "num_updates": "18200", "lr": "3e-05", "gnorm": "208.483", "loss_scale": "4", "train_wall": "59", "gb_free": "18.4", "wall": "5507"}
[2024-09-27 18:06:28,861][train_inner][INFO] - {"epoch": 26, "update": 25.819, "loss": "90.325", "ntokens": "7301.73", "nsentences": "39", "nll_loss": "0.482", "wps": "24845.8", "ups": "3.4", "wpb": "7301.7", "bsz": "39", "num_updates": "18400", "lr": "3e-05", "gnorm": "193.631", "loss_scale": "4", "train_wall": "58", "gb_free": "18", "wall": "5566"}
[2024-09-27 18:07:06,728][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:07:06,728][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:07:06,762][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 27
[2024-09-27 18:07:09,408][valid][INFO] - {"epoch": 26, "valid_loss": "16.889", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.088", "valid_uer": "1.637", "valid_wer": "5.596", "valid_raw_wer": "5.596", "valid_wps": "22102.1", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "18529", "valid_best_wer": "5.596"}
[2024-09-27 18:07:09,408][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 26 @ 18529 updates
[2024-09-27 18:07:09,409][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 18:07:10,483][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 18:07:11,068][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 26 @ 18529 updates, score 5.596) (writing took 1.660051478073001 seconds)
[2024-09-27 18:07:11,068][fairseq_cli.train][INFO] - end of epoch 26 (average epoch stats below)
[2024-09-27 18:07:11,069][train][INFO] - {"epoch": 26, "train_loss": "87.577", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.472", "train_wps": "24308.3", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "18529", "train_lr": "3e-05", "train_gnorm": "198.817", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "17.9", "train_wall": "5608"}
[2024-09-27 18:07:11,069][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:07:11,110][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 27
[2024-09-27 18:07:11,231][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:07:11,233][fairseq.trainer][INFO] - begin training epoch 27
[2024-09-27 18:07:11,233][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:07:32,330][train_inner][INFO] - {"epoch": 27, "update": 26.1, "loss": "89.541", "ntokens": "7323.24", "nsentences": "38.6", "nll_loss": "0.472", "wps": "23076.7", "ups": "3.15", "wpb": "7323.2", "bsz": "38.6", "num_updates": "18600", "lr": "3e-05", "gnorm": "202.219", "loss_scale": "4", "train_wall": "59", "gb_free": "18.4", "wall": "5630"}
[2024-09-27 18:08:31,506][train_inner][INFO] - {"epoch": 27, "update": 26.38, "loss": "86.657", "ntokens": "7364.31", "nsentences": "40.12", "nll_loss": "0.472", "wps": "24889.5", "ups": "3.38", "wpb": "7364.3", "bsz": "40.1", "num_updates": "18800", "lr": "3e-05", "gnorm": "189.74", "loss_scale": "4", "train_wall": "59", "gb_free": "19.2", "wall": "5689"}
[2024-09-27 18:09:30,989][train_inner][INFO] - {"epoch": 27, "update": 26.661, "loss": "85.786", "ntokens": "7384.44", "nsentences": "39.68", "nll_loss": "0.461", "wps": "24828.7", "ups": "3.36", "wpb": "7384.4", "bsz": "39.7", "num_updates": "19000", "lr": "3e-05", "gnorm": "191.273", "loss_scale": "4", "train_wall": "59", "gb_free": "18.1", "wall": "5748"}
[2024-09-27 18:10:30,074][train_inner][INFO] - {"epoch": 27, "update": 26.941, "loss": "86.602", "ntokens": "7294.33", "nsentences": "39.16", "nll_loss": "0.465", "wps": "24690.8", "ups": "3.38", "wpb": "7294.3", "bsz": "39.2", "num_updates": "19200", "lr": "3e-05", "gnorm": "192.71", "loss_scale": "4", "train_wall": "59", "gb_free": "19.5", "wall": "5807"}
[2024-09-27 18:10:42,507][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:10:42,508][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:10:42,549][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 28
[2024-09-27 18:10:45,196][valid][INFO] - {"epoch": 27, "valid_loss": "16.993", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.089", "valid_uer": "1.554", "valid_wer": "5.408", "valid_raw_wer": "5.408", "valid_wps": "22046.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "19242", "valid_best_wer": "5.408"}
[2024-09-27 18:10:45,197][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 27 @ 19242 updates
[2024-09-27 18:10:45,197][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 18:10:46,270][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 18:10:46,857][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 27 @ 19242 updates, score 5.408) (writing took 1.660257807932794 seconds)
[2024-09-27 18:10:46,857][fairseq_cli.train][INFO] - end of epoch 27 (average epoch stats below)
[2024-09-27 18:10:46,858][train][INFO] - {"epoch": 27, "train_loss": "86.442", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.466", "train_wps": "24291", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "19242", "train_lr": "3e-05", "train_gnorm": "191.608", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "19.4", "train_wall": "5824"}
[2024-09-27 18:10:46,858][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:10:46,900][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 28
[2024-09-27 18:10:47,019][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:10:47,021][fairseq.trainer][INFO] - begin training epoch 28
[2024-09-27 18:10:47,021][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:11:33,838][train_inner][INFO] - {"epoch": 28, "update": 27.222, "loss": "82.915", "ntokens": "7336.7", "nsentences": "40.24", "nll_loss": "0.455", "wps": "23012.2", "ups": "3.14", "wpb": "7336.7", "bsz": "40.2", "num_updates": "19400", "lr": "3e-05", "gnorm": "190.286", "loss_scale": "4", "train_wall": "59", "gb_free": "18.4", "wall": "5871"}
[2024-09-27 18:12:32,946][train_inner][INFO] - {"epoch": 28, "update": 27.502, "loss": "90.239", "ntokens": "7329.28", "nsentences": "37.76", "nll_loss": "0.465", "wps": "24799.6", "ups": "3.38", "wpb": "7329.3", "bsz": "37.8", "num_updates": "19600", "lr": "3e-05", "gnorm": "199.959", "loss_scale": "4", "train_wall": "59", "gb_free": "17.9", "wall": "5930"}
[2024-09-27 18:13:32,239][train_inner][INFO] - {"epoch": 28, "update": 27.783, "loss": "83.363", "ntokens": "7358.86", "nsentences": "40.08", "nll_loss": "0.454", "wps": "24822.2", "ups": "3.37", "wpb": "7358.9", "bsz": "40.1", "num_updates": "19800", "lr": "3e-05", "gnorm": "189.279", "loss_scale": "4", "train_wall": "59", "gb_free": "19.3", "wall": "5990"}
[2024-09-27 18:14:18,339][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:14:18,339][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:14:18,374][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 29
[2024-09-27 18:14:21,025][valid][INFO] - {"epoch": 28, "valid_loss": "16.701", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.087", "valid_uer": "1.59", "valid_wer": "5.539", "valid_raw_wer": "5.539", "valid_wps": "22083", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "19955", "valid_best_wer": "5.408"}
[2024-09-27 18:14:21,025][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 28 @ 19955 updates
[2024-09-27 18:14:21,026][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:14:22,061][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:14:22,072][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 28 @ 19955 updates, score 5.539) (writing took 1.0462479628622532 seconds)
[2024-09-27 18:14:22,072][fairseq_cli.train][INFO] - end of epoch 28 (average epoch stats below)
[2024-09-27 18:14:22,072][train][INFO] - {"epoch": 28, "train_loss": "84.856", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.457", "train_wps": "24355.8", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "19955", "train_lr": "3e-05", "train_gnorm": "191.321", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "17.7", "train_wall": "6039"}
[2024-09-27 18:14:22,073][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:14:22,114][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 29
[2024-09-27 18:14:22,233][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:14:22,235][fairseq.trainer][INFO] - begin training epoch 29
[2024-09-27 18:14:22,235][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:14:35,494][train_inner][INFO] - {"epoch": 29, "update": 28.063, "loss": "82.941", "ntokens": "7358.65", "nsentences": "40.16", "nll_loss": "0.453", "wps": "23266.7", "ups": "3.16", "wpb": "7358.7", "bsz": "40.2", "num_updates": "20000", "lr": "3e-05", "gnorm": "185.572", "loss_scale": "4", "train_wall": "59", "gb_free": "17.4", "wall": "6053"}
[2024-09-27 18:15:34,764][train_inner][INFO] - {"epoch": 29, "update": 28.344, "loss": "84.671", "ntokens": "7367.48", "nsentences": "39.64", "nll_loss": "0.456", "wps": "24860.7", "ups": "3.37", "wpb": "7367.5", "bsz": "39.6", "num_updates": "20200", "lr": "3e-05", "gnorm": "190.752", "loss_scale": "4", "train_wall": "59", "gb_free": "19.3", "wall": "6112"}
[2024-09-27 18:16:34,001][train_inner][INFO] - {"epoch": 29, "update": 28.624, "loss": "87.782", "ntokens": "7334.23", "nsentences": "39.04", "nll_loss": "0.467", "wps": "24762.3", "ups": "3.38", "wpb": "7334.2", "bsz": "39", "num_updates": "20400", "lr": "3e-05", "gnorm": "196.256", "loss_scale": "4", "train_wall": "59", "gb_free": "18.9", "wall": "6171"}
[2024-09-27 18:17:33,120][train_inner][INFO] - {"epoch": 29, "update": 28.905, "loss": "84.247", "ntokens": "7372.05", "nsentences": "40.44", "nll_loss": "0.462", "wps": "24940", "ups": "3.38", "wpb": "7372", "bsz": "40.4", "num_updates": "20600", "lr": "3e-05", "gnorm": "189.641", "loss_scale": "4", "train_wall": "59", "gb_free": "18.3", "wall": "6231"}
[2024-09-27 18:17:53,368][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:17:53,369][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:17:53,404][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 30
[2024-09-27 18:17:56,040][valid][INFO] - {"epoch": 29, "valid_loss": "16.945", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.089", "valid_uer": "1.556", "valid_wer": "5.361", "valid_raw_wer": "5.361", "valid_wps": "22107.8", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "20668", "valid_best_wer": "5.361"}
[2024-09-27 18:17:56,040][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 29 @ 20668 updates
[2024-09-27 18:17:56,041][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 18:17:57,075][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 18:17:57,668][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 29 @ 20668 updates, score 5.361) (writing took 1.6278422179166228 seconds)
[2024-09-27 18:17:57,669][fairseq_cli.train][INFO] - end of epoch 29 (average epoch stats below)
[2024-09-27 18:17:57,669][train][INFO] - {"epoch": 29, "train_loss": "85.433", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.46", "train_wps": "24312.6", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "20668", "train_lr": "3e-05", "train_gnorm": "192.387", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "19", "train_wall": "6255"}
[2024-09-27 18:17:57,670][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:17:57,712][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 30
[2024-09-27 18:17:57,832][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:17:57,834][fairseq.trainer][INFO] - begin training epoch 30
[2024-09-27 18:17:57,834][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:18:37,104][train_inner][INFO] - {"epoch": 30, "update": 29.185, "loss": "84.481", "ntokens": "7382.16", "nsentences": "40.04", "nll_loss": "0.458", "wps": "23075", "ups": "3.13", "wpb": "7382.2", "bsz": "40", "num_updates": "20800", "lr": "3e-05", "gnorm": "193.995", "loss_scale": "4", "train_wall": "59", "gb_free": "18.8", "wall": "6295"}
[2024-09-27 18:19:36,309][train_inner][INFO] - {"epoch": 30, "update": 29.466, "loss": "84.495", "ntokens": "7340.57", "nsentences": "39.32", "nll_loss": "0.453", "wps": "24797.2", "ups": "3.38", "wpb": "7340.6", "bsz": "39.3", "num_updates": "21000", "lr": "3e-05", "gnorm": "192.209", "loss_scale": "4", "train_wall": "59", "gb_free": "19.2", "wall": "6354"}
[2024-09-27 18:20:35,887][train_inner][INFO] - {"epoch": 30, "update": 29.746, "loss": "82.145", "ntokens": "7384.1", "nsentences": "40.16", "nll_loss": "0.447", "wps": "24788.1", "ups": "3.36", "wpb": "7384.1", "bsz": "40.2", "num_updates": "21200", "lr": "3e-05", "gnorm": "189.208", "loss_scale": "4", "train_wall": "59", "gb_free": "19.2", "wall": "6413"}
[2024-09-27 18:21:29,060][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:21:29,061][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:21:29,095][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 31
[2024-09-27 18:21:31,738][valid][INFO] - {"epoch": 30, "valid_loss": "16.314", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.085", "valid_uer": "1.501", "valid_wer": "5.191", "valid_raw_wer": "5.191", "valid_wps": "22133.5", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "21381", "valid_best_wer": "5.191"}
[2024-09-27 18:21:31,738][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 30 @ 21381 updates
[2024-09-27 18:21:31,739][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 18:21:32,761][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 18:21:33,343][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 30 @ 21381 updates, score 5.191) (writing took 1.6048538098111749 seconds)
[2024-09-27 18:21:33,343][fairseq_cli.train][INFO] - end of epoch 30 (average epoch stats below)
[2024-09-27 18:21:33,344][train][INFO] - {"epoch": 30, "train_loss": "84.125", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.453", "train_wps": "24303.8", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "21381", "train_lr": "3e-05", "train_gnorm": "191.846", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "18.7", "train_wall": "6471"}
[2024-09-27 18:21:33,344][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:21:33,385][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 31
[2024-09-27 18:21:33,507][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:21:33,509][fairseq.trainer][INFO] - begin training epoch 31
[2024-09-27 18:21:33,509][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:21:39,305][train_inner][INFO] - {"epoch": 31, "update": 30.027, "loss": "84.157", "ntokens": "7295.73", "nsentences": "39.16", "nll_loss": "0.452", "wps": "23008.5", "ups": "3.15", "wpb": "7295.7", "bsz": "39.2", "num_updates": "21400", "lr": "3e-05", "gnorm": "192.677", "loss_scale": "4", "train_wall": "59", "gb_free": "19.1", "wall": "6477"}
[2024-09-27 18:22:38,133][train_inner][INFO] - {"epoch": 31, "update": 30.307, "loss": "86.135", "ntokens": "7316.43", "nsentences": "38.92", "nll_loss": "0.458", "wps": "24874.1", "ups": "3.4", "wpb": "7316.4", "bsz": "38.9", "num_updates": "21600", "lr": "3e-05", "gnorm": "194.8", "loss_scale": "8", "train_wall": "58", "gb_free": "18", "wall": "6536"}
[2024-09-27 18:23:37,320][train_inner][INFO] - {"epoch": 31, "update": 30.588, "loss": "84.723", "ntokens": "7364.64", "nsentences": "39.96", "nll_loss": "0.46", "wps": "24886.1", "ups": "3.38", "wpb": "7364.6", "bsz": "40", "num_updates": "21800", "lr": "3e-05", "gnorm": "195.088", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "6595"}
[2024-09-27 18:24:36,403][train_inner][INFO] - {"epoch": 31, "update": 30.868, "loss": "83.936", "ntokens": "7341.35", "nsentences": "39.44", "nll_loss": "0.451", "wps": "24851", "ups": "3.39", "wpb": "7341.4", "bsz": "39.4", "num_updates": "22000", "lr": "3e-05", "gnorm": "192.635", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "6654"}
[2024-09-27 18:25:04,521][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:25:04,522][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:25:04,556][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 32
[2024-09-27 18:25:07,183][valid][INFO] - {"epoch": 31, "valid_loss": "16.382", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.086", "valid_uer": "1.583", "valid_wer": "5.455", "valid_raw_wer": "5.455", "valid_wps": "22109.9", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "22094", "valid_best_wer": "5.191"}
[2024-09-27 18:25:07,184][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 31 @ 22094 updates
[2024-09-27 18:25:07,184][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:25:08,293][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:25:08,304][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 31 @ 22094 updates, score 5.455) (writing took 1.1204997759778053 seconds)
[2024-09-27 18:25:08,304][fairseq_cli.train][INFO] - end of epoch 31 (average epoch stats below)
[2024-09-27 18:25:08,305][train][INFO] - {"epoch": 31, "train_loss": "84.367", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.455", "train_wps": "24384.5", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "22094", "train_lr": "3e-05", "train_gnorm": "192.902", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "18.5", "train_wall": "6686"}
[2024-09-27 18:25:08,305][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:25:08,346][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 32
[2024-09-27 18:25:08,467][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:25:08,469][fairseq.trainer][INFO] - begin training epoch 32
[2024-09-27 18:25:08,470][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:25:39,717][train_inner][INFO] - {"epoch": 32, "update": 31.149, "loss": "84.635", "ntokens": "7385.44", "nsentences": "39.6", "nll_loss": "0.454", "wps": "23329.7", "ups": "3.16", "wpb": "7385.4", "bsz": "39.6", "num_updates": "22200", "lr": "3e-05", "gnorm": "190.289", "loss_scale": "8", "train_wall": "59", "gb_free": "17.8", "wall": "6717"}
[2024-09-27 18:26:39,146][train_inner][INFO] - {"epoch": 32, "update": 31.429, "loss": "82.038", "ntokens": "7382.6", "nsentences": "39.76", "nll_loss": "0.442", "wps": "24845.1", "ups": "3.37", "wpb": "7382.6", "bsz": "39.8", "num_updates": "22400", "lr": "3e-05", "gnorm": "191.216", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "6777"}
[2024-09-27 18:27:38,364][train_inner][INFO] - {"epoch": 32, "update": 31.71, "loss": "81.494", "ntokens": "7379.09", "nsentences": "40.16", "nll_loss": "0.444", "wps": "24921.9", "ups": "3.38", "wpb": "7379.1", "bsz": "40.2", "num_updates": "22600", "lr": "3e-05", "gnorm": "188.929", "loss_scale": "8", "train_wall": "59", "gb_free": "17.7", "wall": "6836"}
[2024-09-27 18:28:37,604][train_inner][INFO] - {"epoch": 32, "update": 31.99, "loss": "86.299", "ntokens": "7310.59", "nsentences": "39.16", "nll_loss": "0.462", "wps": "24681.3", "ups": "3.38", "wpb": "7310.6", "bsz": "39.2", "num_updates": "22800", "lr": "3e-05", "gnorm": "199.992", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "6895"}
[2024-09-27 18:28:39,522][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:28:39,522][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:28:39,556][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 33
[2024-09-27 18:28:42,188][valid][INFO] - {"epoch": 32, "valid_loss": "16.38", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.086", "valid_uer": "1.531", "valid_wer": "5.351", "valid_raw_wer": "5.351", "valid_wps": "22082.1", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "22807", "valid_best_wer": "5.191"}
[2024-09-27 18:28:42,189][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 32 @ 22807 updates
[2024-09-27 18:28:42,189][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:28:43,154][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:28:43,167][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 32 @ 22807 updates, score 5.351) (writing took 0.9780187029391527 seconds)
[2024-09-27 18:28:43,167][fairseq_cli.train][INFO] - end of epoch 32 (average epoch stats below)
[2024-09-27 18:28:43,167][train][INFO] - {"epoch": 32, "train_loss": "83.504", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.45", "train_wps": "24395.6", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "22807", "train_lr": "3e-05", "train_gnorm": "193.373", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "6901"}
[2024-09-27 18:28:43,168][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:28:43,209][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 33
[2024-09-27 18:28:43,332][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:28:43,333][fairseq.trainer][INFO] - begin training epoch 33
[2024-09-27 18:28:43,334][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:29:40,374][train_inner][INFO] - {"epoch": 33, "update": 32.271, "loss": "81.557", "ntokens": "7317.23", "nsentences": "40.16", "nll_loss": "0.448", "wps": "23314.7", "ups": "3.19", "wpb": "7317.2", "bsz": "40.2", "num_updates": "23000", "lr": "3e-05", "gnorm": "191.258", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "6958"}
[2024-09-27 18:30:39,328][train_inner][INFO] - {"epoch": 33, "update": 32.551, "loss": "84.33", "ntokens": "7328.56", "nsentences": "39.2", "nll_loss": "0.451", "wps": "24861.8", "ups": "3.39", "wpb": "7328.6", "bsz": "39.2", "num_updates": "23200", "lr": "3e-05", "gnorm": "199.914", "loss_scale": "8", "train_wall": "59", "gb_free": "18.1", "wall": "7017"}
[2024-09-27 18:31:39,098][train_inner][INFO] - {"epoch": 33, "update": 32.832, "loss": "81.237", "ntokens": "7428.85", "nsentences": "40.12", "nll_loss": "0.439", "wps": "24858.4", "ups": "3.35", "wpb": "7428.8", "bsz": "40.1", "num_updates": "23400", "lr": "3e-05", "gnorm": "190.237", "loss_scale": "8", "train_wall": "59", "gb_free": "17.6", "wall": "7076"}
[2024-09-27 18:32:14,403][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:32:14,403][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:32:14,439][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 34
[2024-09-27 18:32:17,100][valid][INFO] - {"epoch": 33, "valid_loss": "16.254", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.085", "valid_uer": "1.477", "valid_wer": "5.126", "valid_raw_wer": "5.126", "valid_wps": "22017.2", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "23520", "valid_best_wer": "5.126"}
[2024-09-27 18:32:17,101][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 33 @ 23520 updates
[2024-09-27 18:32:17,102][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 18:32:18,206][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 18:32:18,783][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 33 @ 23520 updates, score 5.126) (writing took 1.6816124219913036 seconds)
[2024-09-27 18:32:18,783][fairseq_cli.train][INFO] - end of epoch 33 (average epoch stats below)
[2024-09-27 18:32:18,783][train][INFO] - {"epoch": 33, "train_loss": "82.539", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.445", "train_wps": "24310.5", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "23520", "train_lr": "3e-05", "train_gnorm": "194.022", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "17.7", "train_wall": "7116"}
[2024-09-27 18:32:18,784][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:32:18,826][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 34
[2024-09-27 18:32:18,947][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:32:18,949][fairseq.trainer][INFO] - begin training epoch 34
[2024-09-27 18:32:18,949][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:32:42,741][train_inner][INFO] - {"epoch": 34, "update": 33.112, "loss": "82.14", "ntokens": "7315.03", "nsentences": "38.84", "nll_loss": "0.436", "wps": "22987.5", "ups": "3.14", "wpb": "7315", "bsz": "38.8", "num_updates": "23600", "lr": "3e-05", "gnorm": "195.105", "loss_scale": "8", "train_wall": "59", "gb_free": "17.6", "wall": "7140"}
[2024-09-27 18:33:42,297][train_inner][INFO] - {"epoch": 34, "update": 33.393, "loss": "77.965", "ntokens": "7401.88", "nsentences": "41.52", "nll_loss": "0.437", "wps": "24857", "ups": "3.36", "wpb": "7401.9", "bsz": "41.5", "num_updates": "23800", "lr": "3e-05", "gnorm": "187.461", "loss_scale": "8", "train_wall": "59", "gb_free": "17.9", "wall": "7200"}
[2024-09-27 18:34:41,171][train_inner][INFO] - {"epoch": 34, "update": 33.673, "loss": "84.042", "ntokens": "7329.21", "nsentences": "38.48", "nll_loss": "0.441", "wps": "24898.1", "ups": "3.4", "wpb": "7329.2", "bsz": "38.5", "num_updates": "24000", "lr": "3e-05", "gnorm": "196.802", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "7259"}
[2024-09-27 18:35:40,422][train_inner][INFO] - {"epoch": 34, "update": 33.954, "loss": "83.485", "ntokens": "7344.36", "nsentences": "39.08", "nll_loss": "0.444", "wps": "24790.5", "ups": "3.38", "wpb": "7344.4", "bsz": "39.1", "num_updates": "24200", "lr": "3e-05", "gnorm": "197.228", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "7318"}
[2024-09-27 18:35:49,999][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:35:49,999][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:35:50,035][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 35
[2024-09-27 18:35:52,663][valid][INFO] - {"epoch": 34, "valid_loss": "16.043", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.084", "valid_uer": "1.484", "valid_wer": "5.182", "valid_raw_wer": "5.182", "valid_wps": "22081.7", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "24233", "valid_best_wer": "5.126"}
[2024-09-27 18:35:52,663][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 34 @ 24233 updates
[2024-09-27 18:35:52,664][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:35:53,709][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:35:53,721][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 34 @ 24233 updates, score 5.182) (writing took 1.0570897459983826 seconds)
[2024-09-27 18:35:53,721][fairseq_cli.train][INFO] - end of epoch 34 (average epoch stats below)
[2024-09-27 18:35:53,721][train][INFO] - {"epoch": 34, "train_loss": "81.577", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.44", "train_wps": "24387.2", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "24233", "train_lr": "3e-05", "train_gnorm": "193.97", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "7331"}
[2024-09-27 18:35:53,722][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:35:53,763][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 35
[2024-09-27 18:35:53,882][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:35:53,884][fairseq.trainer][INFO] - begin training epoch 35
[2024-09-27 18:35:53,884][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:36:43,044][train_inner][INFO] - {"epoch": 35, "update": 34.234, "loss": "82.3", "ntokens": "7328.26", "nsentences": "39.52", "nll_loss": "0.444", "wps": "23405", "ups": "3.19", "wpb": "7328.3", "bsz": "39.5", "num_updates": "24400", "lr": "3e-05", "gnorm": "196.394", "loss_scale": "8", "train_wall": "58", "gb_free": "17.8", "wall": "7380"}
[2024-09-27 18:37:41,891][train_inner][INFO] - {"epoch": 35, "update": 34.515, "loss": "83.251", "ntokens": "7275.52", "nsentences": "38.84", "nll_loss": "0.444", "wps": "24726.8", "ups": "3.4", "wpb": "7275.5", "bsz": "38.8", "num_updates": "24600", "lr": "3e-05", "gnorm": "197.243", "loss_scale": "8", "train_wall": "58", "gb_free": "19.3", "wall": "7439"}
[2024-09-27 18:38:41,545][train_inner][INFO] - {"epoch": 35, "update": 34.795, "loss": "80.177", "ntokens": "7411.74", "nsentences": "40.24", "nll_loss": "0.435", "wps": "24849.1", "ups": "3.35", "wpb": "7411.7", "bsz": "40.2", "num_updates": "24800", "lr": "3e-05", "gnorm": "192.585", "loss_scale": "8", "train_wall": "59", "gb_free": "19", "wall": "7499"}
[2024-09-27 18:39:25,085][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:39:25,086][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:39:25,120][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 36
[2024-09-27 18:39:27,749][valid][INFO] - {"epoch": 35, "valid_loss": "15.798", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.083", "valid_uer": "1.546", "valid_wer": "5.314", "valid_raw_wer": "5.314", "valid_wps": "22127.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "24946", "valid_best_wer": "5.126"}
[2024-09-27 18:39:27,749][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 35 @ 24946 updates
[2024-09-27 18:39:27,750][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:39:28,844][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:39:28,857][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 35 @ 24946 updates, score 5.314) (writing took 1.1070207029115409 seconds)
[2024-09-27 18:39:28,857][fairseq_cli.train][INFO] - end of epoch 35 (average epoch stats below)
[2024-09-27 18:39:28,857][train][INFO] - {"epoch": 35, "train_loss": "82.159", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.443", "train_wps": "24364.7", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "24946", "train_lr": "3e-05", "train_gnorm": "194.999", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "7546"}
[2024-09-27 18:39:28,858][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:39:28,899][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 36
[2024-09-27 18:39:29,022][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:39:29,024][fairseq.trainer][INFO] - begin training epoch 36
[2024-09-27 18:39:29,024][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:39:45,061][train_inner][INFO] - {"epoch": 36, "update": 35.076, "loss": "83.838", "ntokens": "7372.39", "nsentences": "39.32", "nll_loss": "0.447", "wps": "23214.5", "ups": "3.15", "wpb": "7372.4", "bsz": "39.3", "num_updates": "25000", "lr": "3e-05", "gnorm": "195.323", "loss_scale": "8", "train_wall": "59", "gb_free": "18.9", "wall": "7562"}
[2024-09-27 18:40:44,692][train_inner][INFO] - {"epoch": 36, "update": 35.356, "loss": "80.389", "ntokens": "7421.53", "nsentences": "40.44", "nll_loss": "0.438", "wps": "24891.6", "ups": "3.35", "wpb": "7421.5", "bsz": "40.4", "num_updates": "25200", "lr": "3e-05", "gnorm": "190.708", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "7622"}
[2024-09-27 18:41:43,599][train_inner][INFO] - {"epoch": 36, "update": 35.637, "loss": "80.247", "ntokens": "7289.44", "nsentences": "38.88", "nll_loss": "0.428", "wps": "24748.9", "ups": "3.4", "wpb": "7289.4", "bsz": "38.9", "num_updates": "25400", "lr": "3e-05", "gnorm": "193.687", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "7681"}
[2024-09-27 18:42:42,361][train_inner][INFO] - {"epoch": 36, "update": 35.917, "loss": "83.62", "ntokens": "7329", "nsentences": "39.04", "nll_loss": "0.445", "wps": "24944.6", "ups": "3.4", "wpb": "7329", "bsz": "39", "num_updates": "25600", "lr": "3e-05", "gnorm": "196.12", "loss_scale": "8", "train_wall": "58", "gb_free": "19.5", "wall": "7740"}
[2024-09-27 18:42:59,802][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:42:59,802][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:42:59,838][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 37
[2024-09-27 18:43:02,467][valid][INFO] - {"epoch": 36, "valid_loss": "15.519", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.081", "valid_uer": "1.503", "valid_wer": "5.267", "valid_raw_wer": "5.267", "valid_wps": "22072.2", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "25659", "valid_best_wer": "5.126"}
[2024-09-27 18:43:02,467][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 36 @ 25659 updates
[2024-09-27 18:43:02,468][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:43:03,496][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:43:03,509][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 36 @ 25659 updates, score 5.267) (writing took 1.0413393729832023 seconds)
[2024-09-27 18:43:03,509][fairseq_cli.train][INFO] - end of epoch 36 (average epoch stats below)
[2024-09-27 18:43:03,509][train][INFO] - {"epoch": 36, "train_loss": "81.531", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.439", "train_wps": "24419.6", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "25659", "train_lr": "3e-05", "train_gnorm": "193.34", "train_loss_scale": "8", "train_train_wall": "209", "train_gb_free": "19.5", "train_wall": "7761"}
[2024-09-27 18:43:03,510][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:43:03,551][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 37
[2024-09-27 18:43:03,670][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:43:03,672][fairseq.trainer][INFO] - begin training epoch 37
[2024-09-27 18:43:03,672][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:43:45,402][train_inner][INFO] - {"epoch": 37, "update": 36.198, "loss": "79.657", "ntokens": "7347.75", "nsentences": "40.12", "nll_loss": "0.435", "wps": "23311.4", "ups": "3.17", "wpb": "7347.8", "bsz": "40.1", "num_updates": "25800", "lr": "3e-05", "gnorm": "191.437", "loss_scale": "16", "train_wall": "59", "gb_free": "17.5", "wall": "7803"}
[2024-09-27 18:44:44,783][train_inner][INFO] - {"epoch": 37, "update": 36.478, "loss": "80.866", "ntokens": "7392.01", "nsentences": "40.36", "nll_loss": "0.442", "wps": "24896.8", "ups": "3.37", "wpb": "7392", "bsz": "40.4", "num_updates": "26000", "lr": "3e-05", "gnorm": "192.25", "loss_scale": "16", "train_wall": "59", "gb_free": "19", "wall": "7862"}
[2024-09-27 18:45:44,146][train_inner][INFO] - {"epoch": 37, "update": 36.759, "loss": "80.854", "ntokens": "7379.12", "nsentences": "39.8", "nll_loss": "0.436", "wps": "24860.9", "ups": "3.37", "wpb": "7379.1", "bsz": "39.8", "num_updates": "26200", "lr": "3e-05", "gnorm": "192.219", "loss_scale": "16", "train_wall": "59", "gb_free": "19", "wall": "7922"}
[2024-09-27 18:46:34,752][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:46:34,753][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:46:34,788][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 38
[2024-09-27 18:46:37,420][valid][INFO] - {"epoch": 37, "valid_loss": "15.801", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.083", "valid_uer": "1.471", "valid_wer": "5.135", "valid_raw_wer": "5.135", "valid_wps": "22108.7", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "26372", "valid_best_wer": "5.126"}
[2024-09-27 18:46:37,421][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 37 @ 26372 updates
[2024-09-27 18:46:37,421][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:46:38,535][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:46:38,547][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 37 @ 26372 updates, score 5.135) (writing took 1.1264268979430199 seconds)
[2024-09-27 18:46:38,548][fairseq_cli.train][INFO] - end of epoch 37 (average epoch stats below)
[2024-09-27 18:46:38,548][train][INFO] - {"epoch": 37, "train_loss": "81.141", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.437", "train_wps": "24375.7", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "26372", "train_lr": "3e-05", "train_gnorm": "193.834", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "19.1", "train_wall": "7976"}
[2024-09-27 18:46:38,548][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:46:38,590][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 38
[2024-09-27 18:46:38,711][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:46:38,713][fairseq.trainer][INFO] - begin training epoch 38
[2024-09-27 18:46:38,713][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:46:47,250][train_inner][INFO] - {"epoch": 38, "update": 37.039, "loss": "81.359", "ntokens": "7310.56", "nsentences": "39.32", "nll_loss": "0.438", "wps": "23169.9", "ups": "3.17", "wpb": "7310.6", "bsz": "39.3", "num_updates": "26400", "lr": "3e-05", "gnorm": "196.759", "loss_scale": "16", "train_wall": "59", "gb_free": "18", "wall": "7985"}
[2024-09-27 18:47:46,259][train_inner][INFO] - {"epoch": 38, "update": 37.32, "loss": "82.855", "ntokens": "7321.06", "nsentences": "38.28", "nll_loss": "0.433", "wps": "24813.6", "ups": "3.39", "wpb": "7321.1", "bsz": "38.3", "num_updates": "26600", "lr": "3e-05", "gnorm": "195.553", "loss_scale": "16", "train_wall": "59", "gb_free": "17.7", "wall": "8044"}
[2024-09-27 18:47:54,749][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-09-27 18:48:45,887][train_inner][INFO] - {"epoch": 38, "update": 37.602, "loss": "80.553", "ntokens": "7367.72", "nsentences": "39.64", "nll_loss": "0.433", "wps": "24712.4", "ups": "3.35", "wpb": "7367.7", "bsz": "39.6", "num_updates": "26800", "lr": "3e-05", "gnorm": "193.885", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "8103"}
[2024-09-27 18:49:44,938][train_inner][INFO] - {"epoch": 38, "update": 37.882, "loss": "80.248", "ntokens": "7375.31", "nsentences": "40.64", "nll_loss": "0.442", "wps": "24979.4", "ups": "3.39", "wpb": "7375.3", "bsz": "40.6", "num_updates": "27000", "lr": "3e-05", "gnorm": "188.08", "loss_scale": "8", "train_wall": "59", "gb_free": "19", "wall": "8162"}
[2024-09-27 18:50:09,586][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:50:09,586][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:50:09,622][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 39
[2024-09-27 18:50:12,247][valid][INFO] - {"epoch": 38, "valid_loss": "15.544", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.081", "valid_uer": "1.509", "valid_wer": "5.238", "valid_raw_wer": "5.238", "valid_wps": "22068", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "27084", "valid_best_wer": "5.126"}
[2024-09-27 18:50:12,247][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 38 @ 27084 updates
[2024-09-27 18:50:12,248][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:50:13,315][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:50:13,328][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 38 @ 27084 updates, score 5.238) (writing took 1.0806321341078728 seconds)
[2024-09-27 18:50:13,328][fairseq_cli.train][INFO] - end of epoch 38 (average epoch stats below)
[2024-09-27 18:50:13,328][train][INFO] - {"epoch": 38, "train_loss": "81.263", "train_ntokens": "7351.57", "train_nsentences": "39.5056", "train_nll_loss": "0.437", "train_wps": "24370.6", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.5", "train_num_updates": "27084", "train_lr": "3e-05", "train_gnorm": "192.768", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "18.8", "train_wall": "8191"}
[2024-09-27 18:50:13,329][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:50:13,370][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 39
[2024-09-27 18:50:13,491][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:50:13,493][fairseq.trainer][INFO] - begin training epoch 39
[2024-09-27 18:50:13,493][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:50:48,128][train_inner][INFO] - {"epoch": 39, "update": 38.163, "loss": "82.35", "ntokens": "7362.6", "nsentences": "39.4", "nll_loss": "0.441", "wps": "23303.4", "ups": "3.17", "wpb": "7362.6", "bsz": "39.4", "num_updates": "27200", "lr": "3e-05", "gnorm": "195.06", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "8226"}
[2024-09-27 18:51:47,005][train_inner][INFO] - {"epoch": 39, "update": 38.443, "loss": "81.508", "ntokens": "7339.53", "nsentences": "39.24", "nll_loss": "0.436", "wps": "24931.8", "ups": "3.4", "wpb": "7339.5", "bsz": "39.2", "num_updates": "27400", "lr": "3e-05", "gnorm": "193.838", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "8284"}
[2024-09-27 18:52:46,810][train_inner][INFO] - {"epoch": 39, "update": 38.724, "loss": "79.237", "ntokens": "7400.15", "nsentences": "40.24", "nll_loss": "0.431", "wps": "24747.3", "ups": "3.34", "wpb": "7400.1", "bsz": "40.2", "num_updates": "27600", "lr": "3e-05", "gnorm": "192.363", "loss_scale": "8", "train_wall": "59", "gb_free": "18.4", "wall": "8344"}
[2024-09-27 18:53:44,769][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:53:44,769][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:53:44,804][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 40
[2024-09-27 18:53:47,448][valid][INFO] - {"epoch": 39, "valid_loss": "16.118", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.084", "valid_uer": "1.473", "valid_wer": "5.107", "valid_raw_wer": "5.107", "valid_wps": "22091.7", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "27797", "valid_best_wer": "5.107"}
[2024-09-27 18:53:47,449][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 39 @ 27797 updates
[2024-09-27 18:53:47,449][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 18:53:48,514][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 18:53:49,096][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 39 @ 27797 updates, score 5.107) (writing took 1.6477038019802421 seconds)
[2024-09-27 18:53:49,097][fairseq_cli.train][INFO] - end of epoch 39 (average epoch stats below)
[2024-09-27 18:53:49,097][train][INFO] - {"epoch": 39, "train_loss": "80.22", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.432", "train_wps": "24293.3", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "27797", "train_lr": "3e-05", "train_gnorm": "194.853", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.5", "train_wall": "8406"}
[2024-09-27 18:53:49,098][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:53:49,139][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 40
[2024-09-27 18:53:49,259][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:53:49,261][fairseq.trainer][INFO] - begin training epoch 40
[2024-09-27 18:53:49,261][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:53:50,233][train_inner][INFO] - {"epoch": 40, "update": 39.004, "loss": "80.015", "ntokens": "7288.54", "nsentences": "38.88", "nll_loss": "0.427", "wps": "22984.1", "ups": "3.15", "wpb": "7288.5", "bsz": "38.9", "num_updates": "27800", "lr": "3e-05", "gnorm": "198.73", "loss_scale": "8", "train_wall": "59", "gb_free": "18.8", "wall": "8408"}
[2024-09-27 18:54:49,231][train_inner][INFO] - {"epoch": 40, "update": 39.285, "loss": "80.815", "ntokens": "7339.44", "nsentences": "39.6", "nll_loss": "0.436", "wps": "24880.3", "ups": "3.39", "wpb": "7339.4", "bsz": "39.6", "num_updates": "28000", "lr": "3e-05", "gnorm": "193.788", "loss_scale": "8", "train_wall": "59", "gb_free": "18.8", "wall": "8467"}
[2024-09-27 18:55:48,772][train_inner][INFO] - {"epoch": 40, "update": 39.565, "loss": "76.796", "ntokens": "7414.64", "nsentences": "40.72", "nll_loss": "0.422", "wps": "24906.1", "ups": "3.36", "wpb": "7414.6", "bsz": "40.7", "num_updates": "28200", "lr": "3e-05", "gnorm": "189.988", "loss_scale": "8", "train_wall": "59", "gb_free": "19", "wall": "8526"}
[2024-09-27 18:56:47,867][train_inner][INFO] - {"epoch": 40, "update": 39.846, "loss": "80.844", "ntokens": "7318.88", "nsentences": "39.32", "nll_loss": "0.434", "wps": "24770.1", "ups": "3.38", "wpb": "7318.9", "bsz": "39.3", "num_updates": "28400", "lr": "3e-05", "gnorm": "196.697", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "8585"}
[2024-09-27 18:57:20,317][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 18:57:20,318][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:57:20,352][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 41
[2024-09-27 18:57:22,991][valid][INFO] - {"epoch": 40, "valid_loss": "15.588", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.082", "valid_uer": "1.514", "valid_wer": "5.351", "valid_raw_wer": "5.351", "valid_wps": "22077.7", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "28510", "valid_best_wer": "5.107"}
[2024-09-27 18:57:22,991][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 40 @ 28510 updates
[2024-09-27 18:57:22,992][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:57:24,046][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 18:57:24,056][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 40 @ 28510 updates, score 5.351) (writing took 1.0650695250369608 seconds)
[2024-09-27 18:57:24,056][fairseq_cli.train][INFO] - end of epoch 40 (average epoch stats below)
[2024-09-27 18:57:24,057][train][INFO] - {"epoch": 40, "train_loss": "80.049", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.431", "train_wps": "24384.6", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "28510", "train_lr": "3e-05", "train_gnorm": "193.776", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.5", "train_wall": "8621"}
[2024-09-27 18:57:24,057][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 18:57:24,099][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 41
[2024-09-27 18:57:24,220][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 18:57:24,222][fairseq.trainer][INFO] - begin training epoch 41
[2024-09-27 18:57:24,222][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 18:57:50,709][train_inner][INFO] - {"epoch": 41, "update": 40.126, "loss": "82.068", "ntokens": "7265.9", "nsentences": "37.84", "nll_loss": "0.427", "wps": "23124.3", "ups": "3.18", "wpb": "7265.9", "bsz": "37.8", "num_updates": "28600", "lr": "3e-05", "gnorm": "199.317", "loss_scale": "8", "train_wall": "59", "gb_free": "19.7", "wall": "8648"}
[2024-09-27 18:58:49,686][train_inner][INFO] - {"epoch": 41, "update": 40.407, "loss": "80.552", "ntokens": "7286.65", "nsentences": "39.16", "nll_loss": "0.433", "wps": "24710.3", "ups": "3.39", "wpb": "7286.7", "bsz": "39.2", "num_updates": "28800", "lr": "3e-05", "gnorm": "197.584", "loss_scale": "8", "train_wall": "59", "gb_free": "19", "wall": "8707"}
[2024-09-27 18:59:49,317][train_inner][INFO] - {"epoch": 41, "update": 40.687, "loss": "77.322", "ntokens": "7452.46", "nsentences": "40.92", "nll_loss": "0.425", "wps": "24995.2", "ups": "3.35", "wpb": "7452.5", "bsz": "40.9", "num_updates": "29000", "lr": "3e-05", "gnorm": "189.295", "loss_scale": "8", "train_wall": "59", "gb_free": "19.6", "wall": "8767"}
[2024-09-27 19:00:48,871][train_inner][INFO] - {"epoch": 41, "update": 40.968, "loss": "78.618", "ntokens": "7401.59", "nsentences": "40.08", "nll_loss": "0.426", "wps": "24857", "ups": "3.36", "wpb": "7401.6", "bsz": "40.1", "num_updates": "29200", "lr": "3e-05", "gnorm": "197.71", "loss_scale": "8", "train_wall": "59", "gb_free": "18.7", "wall": "8826"}
[2024-09-27 19:00:55,512][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:00:55,512][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:00:55,546][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 42
[2024-09-27 19:00:58,192][valid][INFO] - {"epoch": 41, "valid_loss": "15.726", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.082", "valid_uer": "1.461", "valid_wer": "4.994", "valid_raw_wer": "4.994", "valid_wps": "22109.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "29223", "valid_best_wer": "4.994"}
[2024-09-27 19:00:58,193][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 41 @ 29223 updates
[2024-09-27 19:00:58,193][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 19:00:59,273][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 19:00:59,849][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 41 @ 29223 updates, score 4.994) (writing took 1.6561993591021746 seconds)
[2024-09-27 19:00:59,849][fairseq_cli.train][INFO] - end of epoch 41 (average epoch stats below)
[2024-09-27 19:00:59,849][train][INFO] - {"epoch": 41, "train_loss": "79.225", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.427", "train_wps": "24290.5", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "29223", "train_lr": "3e-05", "train_gnorm": "196.427", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "8837"}
[2024-09-27 19:00:59,851][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:00:59,893][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 42
[2024-09-27 19:01:00,013][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:01:00,015][fairseq.trainer][INFO] - begin training epoch 42
[2024-09-27 19:01:00,015][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:01:51,875][train_inner][INFO] - {"epoch": 42, "update": 41.248, "loss": "80.99", "ntokens": "7250.31", "nsentences": "38", "nll_loss": "0.424", "wps": "23015.4", "ups": "3.17", "wpb": "7250.3", "bsz": "38", "num_updates": "29400", "lr": "3e-05", "gnorm": "198.626", "loss_scale": "8", "train_wall": "58", "gb_free": "18.2", "wall": "8889"}
[2024-09-27 19:02:51,117][train_inner][INFO] - {"epoch": 42, "update": 41.529, "loss": "81.497", "ntokens": "7370.05", "nsentences": "39.04", "nll_loss": "0.432", "wps": "24881.2", "ups": "3.38", "wpb": "7370.1", "bsz": "39", "num_updates": "29600", "lr": "3e-05", "gnorm": "196.589", "loss_scale": "8", "train_wall": "59", "gb_free": "18.6", "wall": "8949"}
[2024-09-27 19:03:50,636][train_inner][INFO] - {"epoch": 42, "update": 41.809, "loss": "76.411", "ntokens": "7405.68", "nsentences": "40.6", "nll_loss": "0.419", "wps": "24885", "ups": "3.36", "wpb": "7405.7", "bsz": "40.6", "num_updates": "29800", "lr": "3e-05", "gnorm": "193.147", "loss_scale": "8", "train_wall": "59", "gb_free": "18.3", "wall": "9008"}
[2024-09-27 19:04:30,930][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:04:30,931][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:04:30,965][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 43
[2024-09-27 19:04:33,616][valid][INFO] - {"epoch": 42, "valid_loss": "15.409", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.081", "valid_uer": "1.466", "valid_wer": "5.06", "valid_raw_wer": "5.06", "valid_wps": "22094.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "29936", "valid_best_wer": "4.994"}
[2024-09-27 19:04:33,617][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 42 @ 29936 updates
[2024-09-27 19:04:33,617][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:04:34,717][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:04:34,727][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 42 @ 29936 updates, score 5.06) (writing took 1.1106248318683356 seconds)
[2024-09-27 19:04:34,727][fairseq_cli.train][INFO] - end of epoch 42 (average epoch stats below)
[2024-09-27 19:04:34,728][train][INFO] - {"epoch": 42, "train_loss": "78.681", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.424", "train_wps": "24393.9", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "29936", "train_lr": "3e-05", "train_gnorm": "194.814", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "9052"}
[2024-09-27 19:04:34,728][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:04:34,769][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 43
[2024-09-27 19:04:34,892][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:04:34,894][fairseq.trainer][INFO] - begin training epoch 43
[2024-09-27 19:04:34,894][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:04:53,828][train_inner][INFO] - {"epoch": 43, "update": 42.09, "loss": "79.442", "ntokens": "7368.8", "nsentences": "40.16", "nll_loss": "0.433", "wps": "23322.1", "ups": "3.16", "wpb": "7368.8", "bsz": "40.2", "num_updates": "30000", "lr": "3e-05", "gnorm": "193.259", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "9071"}
[2024-09-27 19:05:52,908][train_inner][INFO] - {"epoch": 43, "update": 42.37, "loss": "76.62", "ntokens": "7334.3", "nsentences": "40.24", "nll_loss": "0.42", "wps": "24828.6", "ups": "3.39", "wpb": "7334.3", "bsz": "40.2", "num_updates": "30200", "lr": "3e-05", "gnorm": "193.856", "loss_scale": "8", "train_wall": "59", "gb_free": "18.9", "wall": "9130"}
[2024-09-27 19:06:52,349][train_inner][INFO] - {"epoch": 43, "update": 42.651, "loss": "79.358", "ntokens": "7375.33", "nsentences": "39.8", "nll_loss": "0.428", "wps": "24815.8", "ups": "3.36", "wpb": "7375.3", "bsz": "39.8", "num_updates": "30400", "lr": "3e-05", "gnorm": "195.118", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "9190"}
[2024-09-27 19:07:51,347][train_inner][INFO] - {"epoch": 43, "update": 42.931, "loss": "79.607", "ntokens": "7346.15", "nsentences": "39.6", "nll_loss": "0.429", "wps": "24902.9", "ups": "3.39", "wpb": "7346.1", "bsz": "39.6", "num_updates": "30600", "lr": "3e-05", "gnorm": "193.439", "loss_scale": "8", "train_wall": "59", "gb_free": "18.9", "wall": "9249"}
[2024-09-27 19:08:05,679][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:08:05,679][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:08:05,714][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 44
[2024-09-27 19:08:08,357][valid][INFO] - {"epoch": 43, "valid_loss": "16.007", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.084", "valid_uer": "1.468", "valid_wer": "4.947", "valid_raw_wer": "4.947", "valid_wps": "22080.1", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "30649", "valid_best_wer": "4.947"}
[2024-09-27 19:08:08,357][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 43 @ 30649 updates
[2024-09-27 19:08:08,358][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 19:08:09,386][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 19:08:09,960][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 43 @ 30649 updates, score 4.947) (writing took 1.6028595950920135 seconds)
[2024-09-27 19:08:09,961][fairseq_cli.train][INFO] - end of epoch 43 (average epoch stats below)
[2024-09-27 19:08:09,961][train][INFO] - {"epoch": 43, "train_loss": "80.068", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.431", "train_wps": "24353.7", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "30649", "train_lr": "3e-05", "train_gnorm": "195.162", "train_loss_scale": "8", "train_train_wall": "209", "train_gb_free": "18.5", "train_wall": "9267"}
[2024-09-27 19:08:09,962][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:08:10,003][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 44
[2024-09-27 19:08:10,125][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:08:10,127][fairseq.trainer][INFO] - begin training epoch 44
[2024-09-27 19:08:10,127][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:08:55,123][train_inner][INFO] - {"epoch": 44, "update": 43.212, "loss": "81.446", "ntokens": "7363.67", "nsentences": "39.36", "nll_loss": "0.435", "wps": "23092.2", "ups": "3.14", "wpb": "7363.7", "bsz": "39.4", "num_updates": "30800", "lr": "3e-05", "gnorm": "197.61", "loss_scale": "16", "train_wall": "59", "gb_free": "19.2", "wall": "9313"}
[2024-09-27 19:09:54,454][train_inner][INFO] - {"epoch": 44, "update": 43.492, "loss": "78.772", "ntokens": "7380.66", "nsentences": "39.96", "nll_loss": "0.426", "wps": "24880", "ups": "3.37", "wpb": "7380.7", "bsz": "40", "num_updates": "31000", "lr": "3e-05", "gnorm": "196.41", "loss_scale": "16", "train_wall": "59", "gb_free": "18.8", "wall": "9372"}
[2024-09-27 19:10:53,343][train_inner][INFO] - {"epoch": 44, "update": 43.773, "loss": "79.059", "ntokens": "7312.02", "nsentences": "39.4", "nll_loss": "0.426", "wps": "24833.3", "ups": "3.4", "wpb": "7312", "bsz": "39.4", "num_updates": "31200", "lr": "3e-05", "gnorm": "196.145", "loss_scale": "16", "train_wall": "59", "gb_free": "19.1", "wall": "9431"}
[2024-09-27 19:11:41,146][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:11:41,146][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:11:41,181][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 45
[2024-09-27 19:11:43,824][valid][INFO] - {"epoch": 44, "valid_loss": "15.303", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.08", "valid_uer": "1.411", "valid_wer": "4.862", "valid_raw_wer": "4.862", "valid_wps": "22082.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "31362", "valid_best_wer": "4.862"}
[2024-09-27 19:11:43,825][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 44 @ 31362 updates
[2024-09-27 19:11:43,825][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 19:11:44,844][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 19:11:45,409][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 44 @ 31362 updates, score 4.862) (writing took 1.584061702946201 seconds)
[2024-09-27 19:11:45,409][fairseq_cli.train][INFO] - end of epoch 44 (average epoch stats below)
[2024-09-27 19:11:45,409][train][INFO] - {"epoch": 44, "train_loss": "78.759", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.424", "train_wps": "24329.4", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "31362", "train_lr": "3e-05", "train_gnorm": "196.148", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "17.8", "train_wall": "9483"}
[2024-09-27 19:11:45,410][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:11:45,451][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 45
[2024-09-27 19:11:45,572][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:11:45,574][fairseq.trainer][INFO] - begin training epoch 45
[2024-09-27 19:11:45,574][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:11:57,177][train_inner][INFO] - {"epoch": 45, "update": 44.053, "loss": "78.609", "ntokens": "7394.61", "nsentences": "39.72", "nll_loss": "0.422", "wps": "23168.2", "ups": "3.13", "wpb": "7394.6", "bsz": "39.7", "num_updates": "31400", "lr": "3e-05", "gnorm": "194.455", "loss_scale": "16", "train_wall": "59", "gb_free": "19.2", "wall": "9495"}
[2024-09-27 19:12:56,762][train_inner][INFO] - {"epoch": 45, "update": 44.334, "loss": "74.366", "ntokens": "7367.77", "nsentences": "41.04", "nll_loss": "0.414", "wps": "24730.3", "ups": "3.36", "wpb": "7367.8", "bsz": "41", "num_updates": "31600", "lr": "3e-05", "gnorm": "193.012", "loss_scale": "16", "train_wall": "59", "gb_free": "17.7", "wall": "9554"}
[2024-09-27 19:13:55,851][train_inner][INFO] - {"epoch": 45, "update": 44.614, "loss": "80.332", "ntokens": "7326.91", "nsentences": "38.8", "nll_loss": "0.425", "wps": "24799.7", "ups": "3.38", "wpb": "7326.9", "bsz": "38.8", "num_updates": "31800", "lr": "3e-05", "gnorm": "199.086", "loss_scale": "16", "train_wall": "59", "gb_free": "19.2", "wall": "9613"}
[2024-09-27 19:14:54,488][train_inner][INFO] - {"epoch": 45, "update": 44.895, "loss": "78.019", "ntokens": "7257.37", "nsentences": "38.52", "nll_loss": "0.414", "wps": "24753.6", "ups": "3.41", "wpb": "7257.4", "bsz": "38.5", "num_updates": "32000", "lr": "3e-05", "gnorm": "197.964", "loss_scale": "16", "train_wall": "58", "gb_free": "19.3", "wall": "9672"}
[2024-09-27 19:15:17,038][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:15:17,039][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:15:17,073][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 46
[2024-09-27 19:15:19,728][valid][INFO] - {"epoch": 45, "valid_loss": "15.375", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.08", "valid_uer": "1.438", "valid_wer": "5.079", "valid_raw_wer": "5.079", "valid_wps": "22087", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "32075", "valid_best_wer": "4.862"}
[2024-09-27 19:15:19,728][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 45 @ 32075 updates
[2024-09-27 19:15:19,729][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:15:20,852][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:15:20,863][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 45 @ 32075 updates, score 5.079) (writing took 1.134548950009048 seconds)
[2024-09-27 19:15:20,863][fairseq_cli.train][INFO] - end of epoch 45 (average epoch stats below)
[2024-09-27 19:15:20,863][train][INFO] - {"epoch": 45, "train_loss": "77.791", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.419", "train_wps": "24328.7", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "32075", "train_lr": "3e-05", "train_gnorm": "195.888", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "17.5", "train_wall": "9698"}
[2024-09-27 19:15:20,864][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:15:20,905][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 46
[2024-09-27 19:15:21,028][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:15:21,030][fairseq.trainer][INFO] - begin training epoch 46
[2024-09-27 19:15:21,030][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:15:58,380][train_inner][INFO] - {"epoch": 46, "update": 45.175, "loss": "79.477", "ntokens": "7447.09", "nsentences": "38.88", "nll_loss": "0.415", "wps": "23311.6", "ups": "3.13", "wpb": "7447.1", "bsz": "38.9", "num_updates": "32200", "lr": "3e-05", "gnorm": "194.264", "loss_scale": "16", "train_wall": "59", "gb_free": "18.8", "wall": "9736"}
[2024-09-27 19:16:57,401][train_inner][INFO] - {"epoch": 46, "update": 45.456, "loss": "79.258", "ntokens": "7293.31", "nsentences": "38", "nll_loss": "0.413", "wps": "24714.1", "ups": "3.39", "wpb": "7293.3", "bsz": "38", "num_updates": "32400", "lr": "3e-05", "gnorm": "203.273", "loss_scale": "16", "train_wall": "59", "gb_free": "19", "wall": "9795"}
[2024-09-27 19:17:56,917][train_inner][INFO] - {"epoch": 46, "update": 45.736, "loss": "76.694", "ntokens": "7382.63", "nsentences": "39.96", "nll_loss": "0.415", "wps": "24809.1", "ups": "3.36", "wpb": "7382.6", "bsz": "40", "num_updates": "32600", "lr": "3e-05", "gnorm": "192.519", "loss_scale": "16", "train_wall": "59", "gb_free": "18.3", "wall": "9854"}
[2024-09-27 19:18:52,428][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:18:52,428][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:18:52,462][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 47
[2024-09-27 19:18:55,103][valid][INFO] - {"epoch": 46, "valid_loss": "14.984", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.078", "valid_uer": "1.39", "valid_wer": "4.759", "valid_raw_wer": "4.759", "valid_wps": "22039.8", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "32788", "valid_best_wer": "4.759"}
[2024-09-27 19:18:55,104][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 46 @ 32788 updates
[2024-09-27 19:18:55,104][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 19:18:56,210][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 19:18:56,787][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 46 @ 32788 updates, score 4.759) (writing took 1.683165153954178 seconds)
[2024-09-27 19:18:56,787][fairseq_cli.train][INFO] - end of epoch 46 (average epoch stats below)
[2024-09-27 19:18:56,788][train][INFO] - {"epoch": 46, "train_loss": "76.779", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.414", "train_wps": "24275.7", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "32788", "train_lr": "3e-05", "train_gnorm": "195.463", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "19.1", "train_wall": "9914"}
[2024-09-27 19:18:56,788][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:18:56,829][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 47
[2024-09-27 19:18:56,949][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:18:56,951][fairseq.trainer][INFO] - begin training epoch 47
[2024-09-27 19:18:56,951][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:19:00,619][train_inner][INFO] - {"epoch": 47, "update": 46.017, "loss": "73.725", "ntokens": "7347.19", "nsentences": "41.4", "nll_loss": "0.415", "wps": "23067.4", "ups": "3.14", "wpb": "7347.2", "bsz": "41.4", "num_updates": "32800", "lr": "3e-05", "gnorm": "191.46", "loss_scale": "16", "train_wall": "59", "gb_free": "19.3", "wall": "9918"}
[2024-09-27 19:19:59,912][train_inner][INFO] - {"epoch": 47, "update": 46.297, "loss": "76.644", "ntokens": "7358.81", "nsentences": "39.56", "nll_loss": "0.412", "wps": "24822.1", "ups": "3.37", "wpb": "7358.8", "bsz": "39.6", "num_updates": "33000", "lr": "3e-05", "gnorm": "193.639", "loss_scale": "16", "train_wall": "59", "gb_free": "19", "wall": "9977"}
[2024-09-27 19:20:59,301][train_inner][INFO] - {"epoch": 47, "update": 46.578, "loss": "75.479", "ntokens": "7387.95", "nsentences": "40.16", "nll_loss": "0.41", "wps": "24879.9", "ups": "3.37", "wpb": "7388", "bsz": "40.2", "num_updates": "33200", "lr": "3e-05", "gnorm": "197.424", "loss_scale": "16", "train_wall": "59", "gb_free": "18.8", "wall": "10037"}
[2024-09-27 19:21:58,928][train_inner][INFO] - {"epoch": 47, "update": 46.858, "loss": "76.027", "ntokens": "7384.42", "nsentences": "39.96", "nll_loss": "0.411", "wps": "24768.8", "ups": "3.35", "wpb": "7384.4", "bsz": "40", "num_updates": "33400", "lr": "3e-05", "gnorm": "194.827", "loss_scale": "16", "train_wall": "59", "gb_free": "18", "wall": "10096"}
[2024-09-27 19:22:28,276][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:22:28,276][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:22:28,310][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 48
[2024-09-27 19:22:30,961][valid][INFO] - {"epoch": 47, "valid_loss": "15.21", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.08", "valid_uer": "1.38", "valid_wer": "4.759", "valid_raw_wer": "4.759", "valid_wps": "22064.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "33501", "valid_best_wer": "4.759"}
[2024-09-27 19:22:30,962][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 47 @ 33501 updates
[2024-09-27 19:22:30,962][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 19:22:32,057][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 19:22:32,645][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 47 @ 33501 updates, score 4.759) (writing took 1.6836596049834043 seconds)
[2024-09-27 19:22:32,646][fairseq_cli.train][INFO] - end of epoch 47 (average epoch stats below)
[2024-09-27 19:22:32,646][train][INFO] - {"epoch": 47, "train_loss": "76.695", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.413", "train_wps": "24283.1", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "33501", "train_lr": "3e-05", "train_gnorm": "196.843", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "18", "train_wall": "10130"}
[2024-09-27 19:22:32,647][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:22:32,689][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 48
[2024-09-27 19:22:32,807][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:22:32,809][fairseq.trainer][INFO] - begin training epoch 48
[2024-09-27 19:22:32,809][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:22:35,920][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-09-27 19:23:02,607][train_inner][INFO] - {"epoch": 48, "update": 47.14, "loss": "76.956", "ntokens": "7287.05", "nsentences": "39.24", "nll_loss": "0.414", "wps": "22886.8", "ups": "3.14", "wpb": "7287.1", "bsz": "39.2", "num_updates": "33600", "lr": "3e-05", "gnorm": "197.108", "loss_scale": "8", "train_wall": "59", "gb_free": "18.8", "wall": "10160"}
[2024-09-27 19:24:01,971][train_inner][INFO] - {"epoch": 48, "update": 47.421, "loss": "74.877", "ntokens": "7345.84", "nsentences": "39.76", "nll_loss": "0.405", "wps": "24748.5", "ups": "3.37", "wpb": "7345.8", "bsz": "39.8", "num_updates": "33800", "lr": "3e-05", "gnorm": "194.205", "loss_scale": "8", "train_wall": "59", "gb_free": "17.5", "wall": "10219"}
[2024-09-27 19:25:00,660][train_inner][INFO] - {"epoch": 48, "update": 47.701, "loss": "79.456", "ntokens": "7281.98", "nsentences": "38.28", "nll_loss": "0.418", "wps": "24815.5", "ups": "3.41", "wpb": "7282", "bsz": "38.3", "num_updates": "34000", "lr": "3e-05", "gnorm": "200.646", "loss_scale": "8", "train_wall": "58", "gb_free": "18.6", "wall": "10278"}
[2024-09-27 19:26:00,421][train_inner][INFO] - {"epoch": 48, "update": 47.982, "loss": "75.388", "ntokens": "7428.4", "nsentences": "40.8", "nll_loss": "0.414", "wps": "24860.3", "ups": "3.35", "wpb": "7428.4", "bsz": "40.8", "num_updates": "34200", "lr": "3e-05", "gnorm": "191.002", "loss_scale": "8", "train_wall": "59", "gb_free": "19", "wall": "10338"}
[2024-09-27 19:26:04,088][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:26:04,089][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:26:04,124][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 49
[2024-09-27 19:26:06,773][valid][INFO] - {"epoch": 48, "valid_loss": "14.916", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.078", "valid_uer": "1.427", "valid_wer": "4.909", "valid_raw_wer": "4.909", "valid_wps": "22119.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "34213", "valid_best_wer": "4.759"}
[2024-09-27 19:26:06,773][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 48 @ 34213 updates
[2024-09-27 19:26:06,774][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:26:07,848][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:26:07,859][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 48 @ 34213 updates, score 4.909) (writing took 1.0852268349844962 seconds)
[2024-09-27 19:26:07,859][fairseq_cli.train][INFO] - end of epoch 48 (average epoch stats below)
[2024-09-27 19:26:07,859][train][INFO] - {"epoch": 48, "train_loss": "76.461", "train_ntokens": "7352.05", "train_nsentences": "39.618", "train_nll_loss": "0.412", "train_wps": "24323.2", "train_ups": "3.31", "train_wpb": "7352.1", "train_bsz": "39.6", "train_num_updates": "34213", "train_lr": "3e-05", "train_gnorm": "194.647", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19", "train_wall": "10345"}
[2024-09-27 19:26:07,860][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:26:07,901][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 49
[2024-09-27 19:26:08,020][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:26:08,022][fairseq.trainer][INFO] - begin training epoch 49
[2024-09-27 19:26:08,022][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:27:03,764][train_inner][INFO] - {"epoch": 49, "update": 48.262, "loss": "77.772", "ntokens": "7364.94", "nsentences": "39.44", "nll_loss": "0.416", "wps": "23254.4", "ups": "3.16", "wpb": "7364.9", "bsz": "39.4", "num_updates": "34400", "lr": "3e-05", "gnorm": "196.486", "loss_scale": "8", "train_wall": "59", "gb_free": "19", "wall": "10401"}
[2024-09-27 19:28:02,820][train_inner][INFO] - {"epoch": 49, "update": 48.543, "loss": "78.749", "ntokens": "7354.82", "nsentences": "38.96", "nll_loss": "0.417", "wps": "24908", "ups": "3.39", "wpb": "7354.8", "bsz": "39", "num_updates": "34600", "lr": "3e-05", "gnorm": "199.269", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "10460"}
[2024-09-27 19:29:02,080][train_inner][INFO] - {"epoch": 49, "update": 48.823, "loss": "75.192", "ntokens": "7349.31", "nsentences": "39.92", "nll_loss": "0.408", "wps": "24803.5", "ups": "3.37", "wpb": "7349.3", "bsz": "39.9", "num_updates": "34800", "lr": "3e-05", "gnorm": "196.95", "loss_scale": "8", "train_wall": "59", "gb_free": "18.7", "wall": "10519"}
[2024-09-27 19:29:39,182][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:29:39,182][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:29:39,217][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 50
[2024-09-27 19:29:41,847][valid][INFO] - {"epoch": 49, "valid_loss": "15.881", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.083", "valid_uer": "1.438", "valid_wer": "5.032", "valid_raw_wer": "5.032", "valid_wps": "22077.6", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "34926", "valid_best_wer": "4.759"}
[2024-09-27 19:29:41,847][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 49 @ 34926 updates
[2024-09-27 19:29:41,848][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:29:42,955][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:29:42,968][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 49 @ 34926 updates, score 5.032) (writing took 1.1204898178111762 seconds)
[2024-09-27 19:29:42,968][fairseq_cli.train][INFO] - end of epoch 49 (average epoch stats below)
[2024-09-27 19:29:42,968][train][INFO] - {"epoch": 49, "train_loss": "76.725", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.413", "train_wps": "24367.7", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "34926", "train_lr": "3e-05", "train_gnorm": "196.675", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "10560"}
[2024-09-27 19:29:42,969][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:29:43,010][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 50
[2024-09-27 19:29:43,131][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:29:43,133][fairseq.trainer][INFO] - begin training epoch 50
[2024-09-27 19:29:43,133][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:30:05,624][train_inner][INFO] - {"epoch": 50, "update": 49.104, "loss": "73.643", "ntokens": "7385.72", "nsentences": "40.96", "nll_loss": "0.408", "wps": "23246.4", "ups": "3.15", "wpb": "7385.7", "bsz": "41", "num_updates": "35000", "lr": "3e-05", "gnorm": "193.734", "loss_scale": "8", "train_wall": "59", "gb_free": "18.1", "wall": "10583"}
[2024-09-27 19:31:04,616][train_inner][INFO] - {"epoch": 50, "update": 49.384, "loss": "74.121", "ntokens": "7359.74", "nsentences": "40.44", "nll_loss": "0.407", "wps": "24951.6", "ups": "3.39", "wpb": "7359.7", "bsz": "40.4", "num_updates": "35200", "lr": "3e-05", "gnorm": "195.509", "loss_scale": "8", "train_wall": "59", "gb_free": "18.3", "wall": "10642"}
[2024-09-27 19:32:03,812][train_inner][INFO] - {"epoch": 50, "update": 49.665, "loss": "77.611", "ntokens": "7349.2", "nsentences": "39.28", "nll_loss": "0.415", "wps": "24829.9", "ups": "3.38", "wpb": "7349.2", "bsz": "39.3", "num_updates": "35400", "lr": "3e-05", "gnorm": "201.062", "loss_scale": "8", "train_wall": "59", "gb_free": "19.6", "wall": "10701"}
[2024-09-27 19:33:02,444][train_inner][INFO] - {"epoch": 50, "update": 49.945, "loss": "78.167", "ntokens": "7266.39", "nsentences": "38.16", "nll_loss": "0.41", "wps": "24786.9", "ups": "3.41", "wpb": "7266.4", "bsz": "38.2", "num_updates": "35600", "lr": "3e-05", "gnorm": "201.025", "loss_scale": "8", "train_wall": "58", "gb_free": "18.3", "wall": "10760"}
[2024-09-27 19:33:13,972][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:33:13,973][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:33:14,008][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 51
[2024-09-27 19:33:16,644][valid][INFO] - {"epoch": 50, "valid_loss": "15.327", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.08", "valid_uer": "1.417", "valid_wer": "4.853", "valid_raw_wer": "4.853", "valid_wps": "22139", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "35639", "valid_best_wer": "4.759"}
[2024-09-27 19:33:16,644][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 50 @ 35639 updates
[2024-09-27 19:33:16,645][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:33:17,734][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:33:17,746][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 50 @ 35639 updates, score 4.853) (writing took 1.1019195378758013 seconds)
[2024-09-27 19:33:17,746][fairseq_cli.train][INFO] - end of epoch 50 (average epoch stats below)
[2024-09-27 19:33:17,747][train][INFO] - {"epoch": 50, "train_loss": "75.948", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.409", "train_wps": "24405.3", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "35639", "train_lr": "3e-05", "train_gnorm": "198.646", "train_loss_scale": "8", "train_train_wall": "209", "train_gb_free": "19.7", "train_wall": "10775"}
[2024-09-27 19:33:17,747][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:33:17,788][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 51
[2024-09-27 19:33:17,910][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:33:17,912][fairseq.trainer][INFO] - begin training epoch 51
[2024-09-27 19:33:17,912][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:34:05,434][train_inner][INFO] - {"epoch": 51, "update": 50.226, "loss": "78.319", "ntokens": "7294.26", "nsentences": "37.56", "nll_loss": "0.403", "wps": "23159.9", "ups": "3.18", "wpb": "7294.3", "bsz": "37.6", "num_updates": "35800", "lr": "3e-05", "gnorm": "207.739", "loss_scale": "8", "train_wall": "59", "gb_free": "19.6", "wall": "10823"}
[2024-09-27 19:35:05,627][train_inner][INFO] - {"epoch": 51, "update": 50.506, "loss": "74.132", "ntokens": "7465.82", "nsentences": "41.52", "nll_loss": "0.412", "wps": "24806.5", "ups": "3.32", "wpb": "7465.8", "bsz": "41.5", "num_updates": "36000", "lr": "3e-05", "gnorm": "201.164", "loss_scale": "8", "train_wall": "60", "gb_free": "18.5", "wall": "10883"}
[2024-09-27 19:36:04,516][train_inner][INFO] - {"epoch": 51, "update": 50.787, "loss": "75.832", "ntokens": "7304.1", "nsentences": "38.84", "nll_loss": "0.403", "wps": "24806.4", "ups": "3.4", "wpb": "7304.1", "bsz": "38.8", "num_updates": "36200", "lr": "3e-05", "gnorm": "198.401", "loss_scale": "8", "train_wall": "59", "gb_free": "17.8", "wall": "10942"}
[2024-09-27 19:36:49,353][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:36:49,353][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:36:49,387][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 52
[2024-09-27 19:36:52,024][valid][INFO] - {"epoch": 51, "valid_loss": "14.911", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.078", "valid_uer": "1.436", "valid_wer": "4.862", "valid_raw_wer": "4.862", "valid_wps": "22090.8", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "36352", "valid_best_wer": "4.759"}
[2024-09-27 19:36:52,025][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 51 @ 36352 updates
[2024-09-27 19:36:52,025][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:36:53,077][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:36:53,089][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 51 @ 36352 updates, score 4.862) (writing took 1.0647897941526026 seconds)
[2024-09-27 19:36:53,090][fairseq_cli.train][INFO] - end of epoch 51 (average epoch stats below)
[2024-09-27 19:36:53,090][train][INFO] - {"epoch": 51, "train_loss": "75.461", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.407", "train_wps": "24341.2", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "36352", "train_lr": "3e-05", "train_gnorm": "200.939", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "18.1", "train_wall": "10990"}
[2024-09-27 19:36:53,091][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:36:53,132][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 52
[2024-09-27 19:36:53,253][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:36:53,255][fairseq.trainer][INFO] - begin training epoch 52
[2024-09-27 19:36:53,255][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:37:07,438][train_inner][INFO] - {"epoch": 52, "update": 51.067, "loss": "76.189", "ntokens": "7347.76", "nsentences": "40.44", "nll_loss": "0.419", "wps": "23355.2", "ups": "3.18", "wpb": "7347.8", "bsz": "40.4", "num_updates": "36400", "lr": "3e-05", "gnorm": "195.778", "loss_scale": "8", "train_wall": "59", "gb_free": "17.7", "wall": "11005"}
[2024-09-27 19:38:06,735][train_inner][INFO] - {"epoch": 52, "update": 51.348, "loss": "76.519", "ntokens": "7327.05", "nsentences": "38.76", "nll_loss": "0.405", "wps": "24713", "ups": "3.37", "wpb": "7327.1", "bsz": "38.8", "num_updates": "36600", "lr": "3e-05", "gnorm": "201.46", "loss_scale": "8", "train_wall": "59", "gb_free": "17.8", "wall": "11064"}
[2024-09-27 19:39:06,045][train_inner][INFO] - {"epoch": 52, "update": 51.628, "loss": "75.116", "ntokens": "7351.29", "nsentences": "39.48", "nll_loss": "0.403", "wps": "24789.4", "ups": "3.37", "wpb": "7351.3", "bsz": "39.5", "num_updates": "36800", "lr": "3e-05", "gnorm": "199.506", "loss_scale": "8", "train_wall": "59", "gb_free": "18", "wall": "11123"}
[2024-09-27 19:40:05,111][train_inner][INFO] - {"epoch": 52, "update": 51.909, "loss": "74.984", "ntokens": "7415.68", "nsentences": "40.88", "nll_loss": "0.413", "wps": "25110", "ups": "3.39", "wpb": "7415.7", "bsz": "40.9", "num_updates": "37000", "lr": "3e-05", "gnorm": "192.433", "loss_scale": "8", "train_wall": "59", "gb_free": "17.9", "wall": "11183"}
[2024-09-27 19:40:24,086][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:40:24,087][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:40:24,121][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 53
[2024-09-27 19:40:26,755][valid][INFO] - {"epoch": 52, "valid_loss": "14.899", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.078", "valid_uer": "1.383", "valid_wer": "4.872", "valid_raw_wer": "4.872", "valid_wps": "22106", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "37065", "valid_best_wer": "4.759"}
[2024-09-27 19:40:26,755][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 52 @ 37065 updates
[2024-09-27 19:40:26,756][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:40:27,942][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:40:27,966][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 52 @ 37065 updates, score 4.872) (writing took 1.2104649350512773 seconds)
[2024-09-27 19:40:27,966][fairseq_cli.train][INFO] - end of epoch 52 (average epoch stats below)
[2024-09-27 19:40:27,966][train][INFO] - {"epoch": 52, "train_loss": "76.921", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.414", "train_wps": "24394.1", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "37065", "train_lr": "3e-05", "train_gnorm": "199.313", "train_loss_scale": "8", "train_train_wall": "209", "train_gb_free": "17.8", "train_wall": "11205"}
[2024-09-27 19:40:27,967][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:40:28,046][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 53
[2024-09-27 19:40:28,234][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:40:28,236][fairseq.trainer][INFO] - begin training epoch 53
[2024-09-27 19:40:28,237][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:41:08,660][train_inner][INFO] - {"epoch": 53, "update": 52.189, "loss": "77.837", "ntokens": "7324.75", "nsentences": "39.04", "nll_loss": "0.415", "wps": "23052.2", "ups": "3.15", "wpb": "7324.8", "bsz": "39", "num_updates": "37200", "lr": "3e-05", "gnorm": "206.679", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "11246"}
[2024-09-27 19:42:07,582][train_inner][INFO] - {"epoch": 53, "update": 52.47, "loss": "75.9", "ntokens": "7295.28", "nsentences": "38.88", "nll_loss": "0.405", "wps": "24762.7", "ups": "3.39", "wpb": "7295.3", "bsz": "38.9", "num_updates": "37400", "lr": "3e-05", "gnorm": "200.952", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "11305"}
[2024-09-27 19:43:07,036][train_inner][INFO] - {"epoch": 53, "update": 52.75, "loss": "74.41", "ntokens": "7413.44", "nsentences": "40.8", "nll_loss": "0.41", "wps": "24938.2", "ups": "3.36", "wpb": "7413.4", "bsz": "40.8", "num_updates": "37600", "lr": "3e-05", "gnorm": "194.865", "loss_scale": "8", "train_wall": "59", "gb_free": "19", "wall": "11364"}
[2024-09-27 19:43:59,762][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:43:59,762][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:43:59,799][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 54
[2024-09-27 19:44:02,455][valid][INFO] - {"epoch": 53, "valid_loss": "15.384", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.08", "valid_uer": "1.399", "valid_wer": "4.919", "valid_raw_wer": "4.919", "valid_wps": "22084.2", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "37778", "valid_best_wer": "4.759"}
[2024-09-27 19:44:02,455][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 53 @ 37778 updates
[2024-09-27 19:44:02,456][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:44:03,565][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:44:03,577][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 53 @ 37778 updates, score 4.919) (writing took 1.1221916358917952 seconds)
[2024-09-27 19:44:03,578][fairseq_cli.train][INFO] - end of epoch 53 (average epoch stats below)
[2024-09-27 19:44:03,578][train][INFO] - {"epoch": 53, "train_loss": "75.974", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.409", "train_wps": "24310.9", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "37778", "train_lr": "3e-05", "train_gnorm": "201.508", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "11421"}
[2024-09-27 19:44:03,579][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:44:03,621][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 54
[2024-09-27 19:44:03,741][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:44:03,743][fairseq.trainer][INFO] - begin training epoch 54
[2024-09-27 19:44:03,743][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:44:10,630][train_inner][INFO] - {"epoch": 54, "update": 53.031, "loss": "77.976", "ntokens": "7370.58", "nsentences": "39.64", "nll_loss": "0.419", "wps": "23180.4", "ups": "3.14", "wpb": "7370.6", "bsz": "39.6", "num_updates": "37800", "lr": "3e-05", "gnorm": "205.805", "loss_scale": "16", "train_wall": "59", "gb_free": "19.1", "wall": "11428"}
[2024-09-27 19:45:10,429][train_inner][INFO] - {"epoch": 54, "update": 53.311, "loss": "73.54", "ntokens": "7462.91", "nsentences": "41", "nll_loss": "0.404", "wps": "24959.9", "ups": "3.34", "wpb": "7462.9", "bsz": "41", "num_updates": "38000", "lr": "3e-05", "gnorm": "195.059", "loss_scale": "16", "train_wall": "59", "gb_free": "18.3", "wall": "11488"}
[2024-09-27 19:46:09,471][train_inner][INFO] - {"epoch": 54, "update": 53.592, "loss": "74.421", "ntokens": "7272.58", "nsentences": "38.84", "nll_loss": "0.397", "wps": "24635.5", "ups": "3.39", "wpb": "7272.6", "bsz": "38.8", "num_updates": "38200", "lr": "3e-05", "gnorm": "205.457", "loss_scale": "16", "train_wall": "59", "gb_free": "18", "wall": "11547"}
[2024-09-27 19:46:57,087][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-09-27 19:47:08,493][train_inner][INFO] - {"epoch": 54, "update": 53.874, "loss": "76.762", "ntokens": "7271.2", "nsentences": "38.48", "nll_loss": "0.406", "wps": "24638.8", "ups": "3.39", "wpb": "7271.2", "bsz": "38.5", "num_updates": "38400", "lr": "3e-05", "gnorm": "201.3", "loss_scale": "8", "train_wall": "59", "gb_free": "18.4", "wall": "11606"}
[2024-09-27 19:47:35,354][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:47:35,355][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:47:35,391][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 55
[2024-09-27 19:47:38,014][valid][INFO] - {"epoch": 54, "valid_loss": "15.233", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.08", "valid_uer": "1.431", "valid_wer": "4.947", "valid_raw_wer": "4.947", "valid_wps": "22126.5", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "38490", "valid_best_wer": "4.759"}
[2024-09-27 19:47:38,014][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 54 @ 38490 updates
[2024-09-27 19:47:38,015][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:47:39,086][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:47:39,100][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 54 @ 38490 updates, score 4.947) (writing took 1.0854972400702536 seconds)
[2024-09-27 19:47:39,100][fairseq_cli.train][INFO] - end of epoch 54 (average epoch stats below)
[2024-09-27 19:47:39,101][train][INFO] - {"epoch": 54, "train_loss": "74.385", "train_ntokens": "7351.8", "train_nsentences": "39.618", "train_nll_loss": "0.401", "train_wps": "24287.4", "train_ups": "3.3", "train_wpb": "7351.8", "train_bsz": "39.6", "train_num_updates": "38490", "train_lr": "3e-05", "train_gnorm": "199.633", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.2", "train_wall": "11637"}
[2024-09-27 19:47:39,101][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:47:39,142][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 55
[2024-09-27 19:47:39,262][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:47:39,264][fairseq.trainer][INFO] - begin training epoch 55
[2024-09-27 19:47:39,264][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:48:11,807][train_inner][INFO] - {"epoch": 55, "update": 54.154, "loss": "74.332", "ntokens": "7333.74", "nsentences": "38.84", "nll_loss": "0.394", "wps": "23166.6", "ups": "3.16", "wpb": "7333.7", "bsz": "38.8", "num_updates": "38600", "lr": "3e-05", "gnorm": "200.814", "loss_scale": "8", "train_wall": "59", "gb_free": "18.4", "wall": "11669"}
[2024-09-27 19:49:11,383][train_inner][INFO] - {"epoch": 55, "update": 54.435, "loss": "72.344", "ntokens": "7405.84", "nsentences": "41.36", "nll_loss": "0.404", "wps": "24861.9", "ups": "3.36", "wpb": "7405.8", "bsz": "41.4", "num_updates": "38800", "lr": "3e-05", "gnorm": "191.95", "loss_scale": "8", "train_wall": "59", "gb_free": "19.6", "wall": "11729"}
[2024-09-27 19:50:10,609][train_inner][INFO] - {"epoch": 55, "update": 54.715, "loss": "75.651", "ntokens": "7355.31", "nsentences": "38.56", "nll_loss": "0.397", "wps": "24838.1", "ups": "3.38", "wpb": "7355.3", "bsz": "38.6", "num_updates": "39000", "lr": "3e-05", "gnorm": "211.13", "loss_scale": "8", "train_wall": "59", "gb_free": "18.3", "wall": "11788"}
[2024-09-27 19:51:09,650][train_inner][INFO] - {"epoch": 55, "update": 54.996, "loss": "72.68", "ntokens": "7330.74", "nsentences": "39.88", "nll_loss": "0.395", "wps": "24832.5", "ups": "3.39", "wpb": "7330.7", "bsz": "39.9", "num_updates": "39200", "lr": "3e-05", "gnorm": "201.038", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "11847"}
[2024-09-27 19:51:10,473][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:51:10,473][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:51:10,509][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 56
[2024-09-27 19:51:13,132][valid][INFO] - {"epoch": 55, "valid_loss": "15.159", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.079", "valid_uer": "1.397", "valid_wer": "4.89", "valid_raw_wer": "4.89", "valid_wps": "22108.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "39203", "valid_best_wer": "4.759"}
[2024-09-27 19:51:13,133][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 55 @ 39203 updates
[2024-09-27 19:51:13,133][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:51:14,222][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:51:14,234][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 55 @ 39203 updates, score 4.89) (writing took 1.101539809955284 seconds)
[2024-09-27 19:51:14,235][fairseq_cli.train][INFO] - end of epoch 55 (average epoch stats below)
[2024-09-27 19:51:14,235][train][INFO] - {"epoch": 55, "train_loss": "74.036", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.399", "train_wps": "24364.9", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "39203", "train_lr": "3e-05", "train_gnorm": "201.829", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "18", "train_wall": "11852"}
[2024-09-27 19:51:14,235][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:51:14,277][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 56
[2024-09-27 19:51:14,397][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:51:14,399][fairseq.trainer][INFO] - begin training epoch 56
[2024-09-27 19:51:14,399][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:52:13,094][train_inner][INFO] - {"epoch": 56, "update": 55.276, "loss": "73.937", "ntokens": "7364.6", "nsentences": "39.64", "nll_loss": "0.398", "wps": "23216.2", "ups": "3.15", "wpb": "7364.6", "bsz": "39.6", "num_updates": "39400", "lr": "3e-05", "gnorm": "199.116", "loss_scale": "8", "train_wall": "59", "gb_free": "18.5", "wall": "11910"}
[2024-09-27 19:53:11,892][train_inner][INFO] - {"epoch": 56, "update": 55.557, "loss": "75.557", "ntokens": "7285.28", "nsentences": "39.2", "nll_loss": "0.407", "wps": "24780.9", "ups": "3.4", "wpb": "7285.3", "bsz": "39.2", "num_updates": "39600", "lr": "3e-05", "gnorm": "202.823", "loss_scale": "8", "train_wall": "58", "gb_free": "19.2", "wall": "11969"}
[2024-09-27 19:54:11,198][train_inner][INFO] - {"epoch": 56, "update": 55.837, "loss": "76.321", "ntokens": "7386.04", "nsentences": "39.36", "nll_loss": "0.407", "wps": "24908.4", "ups": "3.37", "wpb": "7386", "bsz": "39.4", "num_updates": "39800", "lr": "3e-05", "gnorm": "198.43", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "12029"}
[2024-09-27 19:54:45,468][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:54:45,469][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:54:45,503][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 57
[2024-09-27 19:54:48,148][valid][INFO] - {"epoch": 56, "valid_loss": "15.428", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.081", "valid_uer": "1.392", "valid_wer": "4.843", "valid_raw_wer": "4.843", "valid_wps": "22086.6", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "39916", "valid_best_wer": "4.759"}
[2024-09-27 19:54:48,148][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 56 @ 39916 updates
[2024-09-27 19:54:48,149][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:54:49,248][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:54:49,261][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 56 @ 39916 updates, score 4.843) (writing took 1.1126348960679024 seconds)
[2024-09-27 19:54:49,261][fairseq_cli.train][INFO] - end of epoch 56 (average epoch stats below)
[2024-09-27 19:54:49,261][train][INFO] - {"epoch": 56, "train_loss": "75.394", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.406", "train_wps": "24377.1", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "39916", "train_lr": "3e-05", "train_gnorm": "199.114", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "18.7", "train_wall": "12067"}
[2024-09-27 19:54:49,262][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:54:49,303][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 57
[2024-09-27 19:54:49,422][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:54:49,424][fairseq.trainer][INFO] - begin training epoch 57
[2024-09-27 19:54:49,424][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:55:14,128][train_inner][INFO] - {"epoch": 57, "update": 56.118, "loss": "76.44", "ntokens": "7354.96", "nsentences": "40", "nll_loss": "0.416", "wps": "23375", "ups": "3.18", "wpb": "7355", "bsz": "40", "num_updates": "40000", "lr": "3e-05", "gnorm": "196.091", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "12092"}
[2024-09-27 19:56:13,415][train_inner][INFO] - {"epoch": 57, "update": 56.398, "loss": "75.31", "ntokens": "7317.56", "nsentences": "38.76", "nll_loss": "0.399", "wps": "24685.3", "ups": "3.37", "wpb": "7317.6", "bsz": "38.8", "num_updates": "40200", "lr": "2.9554e-05", "gnorm": "204.471", "loss_scale": "8", "train_wall": "59", "gb_free": "19.5", "wall": "12151"}
[2024-09-27 19:57:13,145][train_inner][INFO] - {"epoch": 57, "update": 56.679, "loss": "69.82", "ntokens": "7432.7", "nsentences": "41.92", "nll_loss": "0.394", "wps": "24887.6", "ups": "3.35", "wpb": "7432.7", "bsz": "41.9", "num_updates": "40400", "lr": "2.91146e-05", "gnorm": "191.86", "loss_scale": "8", "train_wall": "59", "gb_free": "19.5", "wall": "12211"}
[2024-09-27 19:58:12,352][train_inner][INFO] - {"epoch": 57, "update": 56.959, "loss": "78.372", "ntokens": "7313.23", "nsentences": "38.08", "nll_loss": "0.408", "wps": "24704.3", "ups": "3.38", "wpb": "7313.2", "bsz": "38.1", "num_updates": "40600", "lr": "2.86818e-05", "gnorm": "206.168", "loss_scale": "8", "train_wall": "59", "gb_free": "18.9", "wall": "12270"}
[2024-09-27 19:58:20,910][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 19:58:20,911][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:58:20,945][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 58
[2024-09-27 19:58:23,615][valid][INFO] - {"epoch": 57, "valid_loss": "15.306", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.08", "valid_uer": "1.411", "valid_wer": "4.89", "valid_raw_wer": "4.89", "valid_wps": "22084.8", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "40629", "valid_best_wer": "4.759"}
[2024-09-27 19:58:23,616][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 57 @ 40629 updates
[2024-09-27 19:58:23,616][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:58:24,671][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 19:58:24,685][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 57 @ 40629 updates, score 4.89) (writing took 1.0692821950651705 seconds)
[2024-09-27 19:58:24,686][fairseq_cli.train][INFO] - end of epoch 57 (average epoch stats below)
[2024-09-27 19:58:24,686][train][INFO] - {"epoch": 57, "train_loss": "74.562", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.402", "train_wps": "24332.1", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "40629", "train_lr": "2.86195e-05", "train_gnorm": "200.411", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "17.7", "train_wall": "12282"}
[2024-09-27 19:58:24,686][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 19:58:24,728][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 58
[2024-09-27 19:58:24,848][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 19:58:24,850][fairseq.trainer][INFO] - begin training epoch 58
[2024-09-27 19:58:24,851][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 19:59:15,777][train_inner][INFO] - {"epoch": 58, "update": 57.24, "loss": "76.636", "ntokens": "7404.65", "nsentences": "39.64", "nll_loss": "0.41", "wps": "23349.2", "ups": "3.15", "wpb": "7404.6", "bsz": "39.6", "num_updates": "40800", "lr": "2.82553e-05", "gnorm": "197.766", "loss_scale": "8", "train_wall": "59", "gb_free": "19.5", "wall": "12333"}
[2024-09-27 20:00:14,933][train_inner][INFO] - {"epoch": 58, "update": 57.52, "loss": "76.413", "ntokens": "7331.65", "nsentences": "39.56", "nll_loss": "0.412", "wps": "24787.8", "ups": "3.38", "wpb": "7331.7", "bsz": "39.6", "num_updates": "41000", "lr": "2.78353e-05", "gnorm": "203.978", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "12392"}
[2024-09-27 20:01:14,015][train_inner][INFO] - {"epoch": 58, "update": 57.801, "loss": "73.122", "ntokens": "7332.73", "nsentences": "39.68", "nll_loss": "0.396", "wps": "24822", "ups": "3.39", "wpb": "7332.7", "bsz": "39.7", "num_updates": "41200", "lr": "2.74214e-05", "gnorm": "198.625", "loss_scale": "8", "train_wall": "59", "gb_free": "18.6", "wall": "12451"}
[2024-09-27 20:01:55,624][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:01:55,625][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:01:55,659][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 59
[2024-09-27 20:01:58,302][valid][INFO] - {"epoch": 58, "valid_loss": "14.977", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.078", "valid_uer": "1.41", "valid_wer": "4.853", "valid_raw_wer": "4.853", "valid_wps": "22111.5", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "41342", "valid_best_wer": "4.759"}
[2024-09-27 20:01:58,302][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 58 @ 41342 updates
[2024-09-27 20:01:58,303][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:01:59,301][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:01:59,313][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 58 @ 41342 updates, score 4.853) (writing took 1.0109130730852485 seconds)
[2024-09-27 20:01:59,314][fairseq_cli.train][INFO] - end of epoch 58 (average epoch stats below)
[2024-09-27 20:01:59,314][train][INFO] - {"epoch": 58, "train_loss": "75.211", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.405", "train_wps": "24422.3", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "41342", "train_lr": "2.71314e-05", "train_gnorm": "199.645", "train_loss_scale": "8", "train_train_wall": "209", "train_gb_free": "19.3", "train_wall": "12497"}
[2024-09-27 20:01:59,315][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:01:59,356][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 59
[2024-09-27 20:01:59,477][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:01:59,479][fairseq.trainer][INFO] - begin training epoch 59
[2024-09-27 20:01:59,479][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:02:16,638][train_inner][INFO] - {"epoch": 59, "update": 58.081, "loss": "74.824", "ntokens": "7314.25", "nsentences": "39.28", "nll_loss": "0.402", "wps": "23359.7", "ups": "3.19", "wpb": "7314.2", "bsz": "39.3", "num_updates": "41400", "lr": "2.70138e-05", "gnorm": "198.918", "loss_scale": "8", "train_wall": "58", "gb_free": "19.2", "wall": "12514"}
[2024-09-27 20:03:16,076][train_inner][INFO] - {"epoch": 59, "update": 58.362, "loss": "73.751", "ntokens": "7377.44", "nsentences": "40.24", "nll_loss": "0.402", "wps": "24824", "ups": "3.36", "wpb": "7377.4", "bsz": "40.2", "num_updates": "41600", "lr": "2.66122e-05", "gnorm": "202.968", "loss_scale": "8", "train_wall": "59", "gb_free": "18", "wall": "12573"}
[2024-09-27 20:04:15,569][train_inner][INFO] - {"epoch": 59, "update": 58.642, "loss": "72.707", "ntokens": "7426.68", "nsentences": "40.76", "nll_loss": "0.399", "wps": "24966.6", "ups": "3.36", "wpb": "7426.7", "bsz": "40.8", "num_updates": "41800", "lr": "2.62165e-05", "gnorm": "196.156", "loss_scale": "8", "train_wall": "59", "gb_free": "18.3", "wall": "12633"}
[2024-09-27 20:05:14,184][train_inner][INFO] - {"epoch": 59, "update": 58.923, "loss": "75.005", "ntokens": "7258.36", "nsentences": "38.44", "nll_loss": "0.397", "wps": "24766.3", "ups": "3.41", "wpb": "7258.4", "bsz": "38.4", "num_updates": "42000", "lr": "2.58267e-05", "gnorm": "198.782", "loss_scale": "8", "train_wall": "58", "gb_free": "18.4", "wall": "12692"}
[2024-09-27 20:05:30,477][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:05:30,477][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:05:30,513][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 60
[2024-09-27 20:05:33,167][valid][INFO] - {"epoch": 59, "valid_loss": "14.908", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.078", "valid_uer": "1.35", "valid_wer": "4.731", "valid_raw_wer": "4.731", "valid_wps": "22119.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "42055", "valid_best_wer": "4.731"}
[2024-09-27 20:05:33,168][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 59 @ 42055 updates
[2024-09-27 20:05:33,168][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:05:34,245][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:05:34,830][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 59 @ 42055 updates, score 4.731) (writing took 1.6620952708180994 seconds)
[2024-09-27 20:05:34,830][fairseq_cli.train][INFO] - end of epoch 59 (average epoch stats below)
[2024-09-27 20:05:34,830][train][INFO] - {"epoch": 59, "train_loss": "74.067", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.399", "train_wps": "24321.6", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "42055", "train_lr": "2.57206e-05", "train_gnorm": "199.539", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "17.9", "train_wall": "12712"}
[2024-09-27 20:05:34,831][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:05:34,873][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 60
[2024-09-27 20:05:34,994][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:05:34,996][fairseq.trainer][INFO] - begin training epoch 60
[2024-09-27 20:05:34,996][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:06:17,942][train_inner][INFO] - {"epoch": 60, "update": 59.203, "loss": "75.744", "ntokens": "7360.6", "nsentences": "38.68", "nll_loss": "0.398", "wps": "23089.3", "ups": "3.14", "wpb": "7360.6", "bsz": "38.7", "num_updates": "42200", "lr": "2.54428e-05", "gnorm": "201.459", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "12755"}
[2024-09-27 20:07:17,517][train_inner][INFO] - {"epoch": 60, "update": 59.484, "loss": "71.692", "ntokens": "7402.73", "nsentences": "40.2", "nll_loss": "0.389", "wps": "24851.7", "ups": "3.36", "wpb": "7402.7", "bsz": "40.2", "num_updates": "42400", "lr": "2.50645e-05", "gnorm": "196.864", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "12815"}
[2024-09-27 20:08:17,014][train_inner][INFO] - {"epoch": 60, "update": 59.764, "loss": "70.218", "ntokens": "7389.78", "nsentences": "41.12", "nll_loss": "0.391", "wps": "24841.3", "ups": "3.36", "wpb": "7389.8", "bsz": "41.1", "num_updates": "42600", "lr": "2.46919e-05", "gnorm": "194.973", "loss_scale": "16", "train_wall": "59", "gb_free": "18", "wall": "12874"}
[2024-09-27 20:09:06,325][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:09:06,326][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:09:06,360][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 61
[2024-09-27 20:09:09,007][valid][INFO] - {"epoch": 60, "valid_loss": "14.833", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.078", "valid_uer": "1.344", "valid_wer": "4.655", "valid_raw_wer": "4.655", "valid_wps": "22096", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "42768", "valid_best_wer": "4.655"}
[2024-09-27 20:09:09,007][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 60 @ 42768 updates
[2024-09-27 20:09:09,008][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:09:10,097][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:09:10,687][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 60 @ 42768 updates, score 4.655) (writing took 1.680013369070366 seconds)
[2024-09-27 20:09:10,688][fairseq_cli.train][INFO] - end of epoch 60 (average epoch stats below)
[2024-09-27 20:09:10,688][train][INFO] - {"epoch": 60, "train_loss": "72.715", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.392", "train_wps": "24283.2", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "42768", "train_lr": "2.43832e-05", "train_gnorm": "198.404", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "18.4", "train_wall": "12928"}
[2024-09-27 20:09:10,689][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:09:10,731][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 61
[2024-09-27 20:09:10,851][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:09:10,853][fairseq.trainer][INFO] - begin training epoch 61
[2024-09-27 20:09:10,853][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:09:20,295][train_inner][INFO] - {"epoch": 61, "update": 60.045, "loss": "74.59", "ntokens": "7257.13", "nsentences": "37.76", "nll_loss": "0.388", "wps": "22936", "ups": "3.16", "wpb": "7257.1", "bsz": "37.8", "num_updates": "42800", "lr": "2.43248e-05", "gnorm": "203.473", "loss_scale": "16", "train_wall": "58", "gb_free": "18.3", "wall": "12938"}
[2024-09-27 20:10:18,832][train_inner][INFO] - {"epoch": 61, "update": 60.325, "loss": "74.671", "ntokens": "7264.34", "nsentences": "38.28", "nll_loss": "0.393", "wps": "24819.8", "ups": "3.42", "wpb": "7264.3", "bsz": "38.3", "num_updates": "43000", "lr": "2.39632e-05", "gnorm": "206.362", "loss_scale": "16", "train_wall": "58", "gb_free": "19", "wall": "12996"}
[2024-09-27 20:11:17,824][train_inner][INFO] - {"epoch": 61, "update": 60.606, "loss": "72.682", "ntokens": "7299.09", "nsentences": "39.28", "nll_loss": "0.391", "wps": "24746.2", "ups": "3.39", "wpb": "7299.1", "bsz": "39.3", "num_updates": "43200", "lr": "2.36069e-05", "gnorm": "200.17", "loss_scale": "16", "train_wall": "59", "gb_free": "18.6", "wall": "13055"}
[2024-09-27 20:12:17,915][train_inner][INFO] - {"epoch": 61, "update": 60.886, "loss": "69.86", "ntokens": "7473.27", "nsentences": "41.12", "nll_loss": "0.384", "wps": "24873.1", "ups": "3.33", "wpb": "7473.3", "bsz": "41.1", "num_updates": "43400", "lr": "2.32559e-05", "gnorm": "198.127", "loss_scale": "16", "train_wall": "60", "gb_free": "18.1", "wall": "13115"}
[2024-09-27 20:12:41,929][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:12:41,930][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:12:41,964][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 62
[2024-09-27 20:12:44,623][valid][INFO] - {"epoch": 61, "valid_loss": "14.796", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.077", "valid_uer": "1.351", "valid_wer": "4.721", "valid_raw_wer": "4.721", "valid_wps": "22083.2", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "43481", "valid_best_wer": "4.655"}
[2024-09-27 20:12:44,623][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 61 @ 43481 updates
[2024-09-27 20:12:44,624][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:12:45,699][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:12:45,709][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 61 @ 43481 updates, score 4.721) (writing took 1.086158648133278 seconds)
[2024-09-27 20:12:45,710][fairseq_cli.train][INFO] - end of epoch 61 (average epoch stats below)
[2024-09-27 20:12:45,710][train][INFO] - {"epoch": 61, "train_loss": "72.474", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.39", "train_wps": "24377.6", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "43481", "train_lr": "2.31153e-05", "train_gnorm": "201.195", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "13143"}
[2024-09-27 20:12:45,711][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:12:45,752][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 62
[2024-09-27 20:12:45,873][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:12:45,875][fairseq.trainer][INFO] - begin training epoch 62
[2024-09-27 20:12:45,875][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:13:20,988][train_inner][INFO] - {"epoch": 62, "update": 61.167, "loss": "74.255", "ntokens": "7359.9", "nsentences": "39.64", "nll_loss": "0.4", "wps": "23337.7", "ups": "3.17", "wpb": "7359.9", "bsz": "39.6", "num_updates": "43600", "lr": "2.29102e-05", "gnorm": "202.527", "loss_scale": "16", "train_wall": "59", "gb_free": "19.1", "wall": "13178"}
[2024-09-27 20:13:37,235][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-09-27 20:14:20,537][train_inner][INFO] - {"epoch": 62, "update": 61.449, "loss": "72.241", "ntokens": "7350.95", "nsentences": "39.64", "nll_loss": "0.39", "wps": "24688.8", "ups": "3.36", "wpb": "7351", "bsz": "39.6", "num_updates": "43800", "lr": "2.25696e-05", "gnorm": "201.803", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "13238"}
[2024-09-27 20:15:20,128][train_inner][INFO] - {"epoch": 62, "update": 61.729, "loss": "73.88", "ntokens": "7393.11", "nsentences": "38.88", "nll_loss": "0.389", "wps": "24813.1", "ups": "3.36", "wpb": "7393.1", "bsz": "38.9", "num_updates": "44000", "lr": "2.2234e-05", "gnorm": "205.249", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "13298"}
[2024-09-27 20:16:16,866][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:16:16,867][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:16:16,903][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 63
[2024-09-27 20:16:19,539][valid][INFO] - {"epoch": 62, "valid_loss": "14.776", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.077", "valid_uer": "1.325", "valid_wer": "4.58", "valid_raw_wer": "4.58", "valid_wps": "22106.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "44193", "valid_best_wer": "4.58"}
[2024-09-27 20:16:19,539][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 62 @ 44193 updates
[2024-09-27 20:16:19,540][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:16:20,553][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:16:21,128][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 62 @ 44193 updates, score 4.58) (writing took 1.5884999809786677 seconds)
[2024-09-27 20:16:21,128][fairseq_cli.train][INFO] - end of epoch 62 (average epoch stats below)
[2024-09-27 20:16:21,129][train][INFO] - {"epoch": 62, "train_loss": "72.745", "train_ntokens": "7350.81", "train_nsentences": "39.6067", "train_nll_loss": "0.392", "train_wps": "24295.9", "train_ups": "3.31", "train_wpb": "7350.8", "train_bsz": "39.6", "train_num_updates": "44193", "train_lr": "2.1915e-05", "train_gnorm": "201.662", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.2", "train_wall": "13359"}
[2024-09-27 20:16:21,129][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:16:21,170][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 63
[2024-09-27 20:16:21,289][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:16:21,291][fairseq.trainer][INFO] - begin training epoch 63
[2024-09-27 20:16:21,291][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:16:23,422][train_inner][INFO] - {"epoch": 63, "update": 62.01, "loss": "69.797", "ntokens": "7328.49", "nsentences": "41.04", "nll_loss": "0.391", "wps": "23157.1", "ups": "3.16", "wpb": "7328.5", "bsz": "41", "num_updates": "44200", "lr": "2.19035e-05", "gnorm": "192.357", "loss_scale": "8", "train_wall": "58", "gb_free": "19.1", "wall": "13361"}
[2024-09-27 20:17:23,097][train_inner][INFO] - {"epoch": 63, "update": 62.29, "loss": "72.767", "ntokens": "7403.91", "nsentences": "39.52", "nll_loss": "0.388", "wps": "24813.9", "ups": "3.35", "wpb": "7403.9", "bsz": "39.5", "num_updates": "44400", "lr": "2.15778e-05", "gnorm": "198.391", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "13420"}
[2024-09-27 20:18:22,310][train_inner][INFO] - {"epoch": 63, "update": 62.571, "loss": "71.988", "ntokens": "7296.44", "nsentences": "38.88", "nll_loss": "0.384", "wps": "24645", "ups": "3.38", "wpb": "7296.4", "bsz": "38.9", "num_updates": "44600", "lr": "2.1257e-05", "gnorm": "208.723", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "13480"}
[2024-09-27 20:19:21,590][train_inner][INFO] - {"epoch": 63, "update": 62.851, "loss": "72.151", "ntokens": "7392.11", "nsentences": "40.2", "nll_loss": "0.392", "wps": "24939.8", "ups": "3.37", "wpb": "7392.1", "bsz": "40.2", "num_updates": "44800", "lr": "2.0941e-05", "gnorm": "199.023", "loss_scale": "8", "train_wall": "59", "gb_free": "17.7", "wall": "13539"}
[2024-09-27 20:19:52,789][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:19:52,789][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:19:52,825][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 64
[2024-09-27 20:19:55,494][valid][INFO] - {"epoch": 63, "valid_loss": "15.134", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.079", "valid_uer": "1.305", "valid_wer": "4.542", "valid_raw_wer": "4.542", "valid_wps": "22081.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "44906", "valid_best_wer": "4.542"}
[2024-09-27 20:19:55,495][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 63 @ 44906 updates
[2024-09-27 20:19:55,503][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:19:56,657][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:19:57,241][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 63 @ 44906 updates, score 4.542) (writing took 1.7463476068805903 seconds)
[2024-09-27 20:19:57,241][fairseq_cli.train][INFO] - end of epoch 63 (average epoch stats below)
[2024-09-27 20:19:57,242][train][INFO] - {"epoch": 63, "train_loss": "71.971", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.388", "train_wps": "24254.5", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "44906", "train_lr": "2.07754e-05", "train_gnorm": "201.286", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "18.5", "train_wall": "13575"}
[2024-09-27 20:19:57,242][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:19:57,284][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 64
[2024-09-27 20:19:57,405][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:19:57,407][fairseq.trainer][INFO] - begin training epoch 64
[2024-09-27 20:19:57,407][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:20:25,425][train_inner][INFO] - {"epoch": 64, "update": 63.132, "loss": "70.139", "ntokens": "7326.39", "nsentences": "40.6", "nll_loss": "0.389", "wps": "22954", "ups": "3.13", "wpb": "7326.4", "bsz": "40.6", "num_updates": "45000", "lr": "2.06297e-05", "gnorm": "200.144", "loss_scale": "8", "train_wall": "59", "gb_free": "19", "wall": "13603"}
[2024-09-27 20:21:24,445][train_inner][INFO] - {"epoch": 64, "update": 63.412, "loss": "76.117", "ntokens": "7347.33", "nsentences": "37.92", "nll_loss": "0.393", "wps": "24898", "ups": "3.39", "wpb": "7347.3", "bsz": "37.9", "num_updates": "45200", "lr": "2.0323e-05", "gnorm": "204.377", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "13662"}
[2024-09-27 20:22:24,015][train_inner][INFO] - {"epoch": 64, "update": 63.693, "loss": "72.115", "ntokens": "7390.19", "nsentences": "39.72", "nll_loss": "0.388", "wps": "24811.8", "ups": "3.36", "wpb": "7390.2", "bsz": "39.7", "num_updates": "45400", "lr": "2.00208e-05", "gnorm": "198.866", "loss_scale": "8", "train_wall": "59", "gb_free": "19", "wall": "13721"}
[2024-09-27 20:23:23,042][train_inner][INFO] - {"epoch": 64, "update": 63.973, "loss": "68.21", "ntokens": "7316.68", "nsentences": "40.28", "nll_loss": "0.376", "wps": "24791.1", "ups": "3.39", "wpb": "7316.7", "bsz": "40.3", "num_updates": "45600", "lr": "1.97232e-05", "gnorm": "196.778", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "13780"}
[2024-09-27 20:23:28,594][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:23:28,595][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:23:28,629][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 65
[2024-09-27 20:23:31,259][valid][INFO] - {"epoch": 64, "valid_loss": "14.644", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.077", "valid_uer": "1.325", "valid_wer": "4.552", "valid_raw_wer": "4.552", "valid_wps": "22072.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "45619", "valid_best_wer": "4.542"}
[2024-09-27 20:23:31,260][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 64 @ 45619 updates
[2024-09-27 20:23:31,261][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:23:32,273][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:23:32,285][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 64 @ 45619 updates, score 4.552) (writing took 1.025089852977544 seconds)
[2024-09-27 20:23:32,285][fairseq_cli.train][INFO] - end of epoch 64 (average epoch stats below)
[2024-09-27 20:23:32,286][train][INFO] - {"epoch": 64, "train_loss": "71.612", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.386", "train_wps": "24375.1", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "45619", "train_lr": "1.96951e-05", "train_gnorm": "200.006", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "13790"}
[2024-09-27 20:23:32,286][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:23:32,328][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 65
[2024-09-27 20:23:32,446][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:23:32,448][fairseq.trainer][INFO] - begin training epoch 65
[2024-09-27 20:23:32,448][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:24:25,683][train_inner][INFO] - {"epoch": 65, "update": 64.254, "loss": "74.472", "ntokens": "7314.35", "nsentences": "38.68", "nll_loss": "0.394", "wps": "23353.5", "ups": "3.19", "wpb": "7314.3", "bsz": "38.7", "num_updates": "45800", "lr": "1.943e-05", "gnorm": "198.795", "loss_scale": "8", "train_wall": "58", "gb_free": "18.6", "wall": "13843"}
[2024-09-27 20:25:24,937][train_inner][INFO] - {"epoch": 65, "update": 64.534, "loss": "72.469", "ntokens": "7364.07", "nsentences": "39.84", "nll_loss": "0.392", "wps": "24855.9", "ups": "3.38", "wpb": "7364.1", "bsz": "39.8", "num_updates": "46000", "lr": "1.91411e-05", "gnorm": "202.492", "loss_scale": "8", "train_wall": "59", "gb_free": "18.3", "wall": "13902"}
[2024-09-27 20:26:24,090][train_inner][INFO] - {"epoch": 65, "update": 64.815, "loss": "70.892", "ntokens": "7331.24", "nsentences": "39.68", "nll_loss": "0.384", "wps": "24787.3", "ups": "3.38", "wpb": "7331.2", "bsz": "39.7", "num_updates": "46200", "lr": "1.88565e-05", "gnorm": "202.625", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "13961"}
[2024-09-27 20:27:03,412][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:27:03,412][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:27:03,447][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 66
[2024-09-27 20:27:06,089][valid][INFO] - {"epoch": 65, "valid_loss": "14.519", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.076", "valid_uer": "1.309", "valid_wer": "4.524", "valid_raw_wer": "4.524", "valid_wps": "22097.9", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "46332", "valid_best_wer": "4.524"}
[2024-09-27 20:27:06,089][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 65 @ 46332 updates
[2024-09-27 20:27:06,090][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:27:07,188][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:27:07,767][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 65 @ 46332 updates, score 4.524) (writing took 1.6776769240386784 seconds)
[2024-09-27 20:27:07,767][fairseq_cli.train][INFO] - end of epoch 65 (average epoch stats below)
[2024-09-27 20:27:07,768][train][INFO] - {"epoch": 65, "train_loss": "72.344", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.39", "train_wps": "24325.6", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "46332", "train_lr": "1.8671e-05", "train_gnorm": "200.516", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "14005"}
[2024-09-27 20:27:07,768][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:27:07,809][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 66
[2024-09-27 20:27:07,931][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:27:07,933][fairseq.trainer][INFO] - begin training epoch 66
[2024-09-27 20:27:07,933][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:27:28,496][train_inner][INFO] - {"epoch": 66, "update": 65.095, "loss": "69.529", "ntokens": "7423.8", "nsentences": "41.16", "nll_loss": "0.385", "wps": "23053.3", "ups": "3.11", "wpb": "7423.8", "bsz": "41.2", "num_updates": "46400", "lr": "1.85762e-05", "gnorm": "194.463", "loss_scale": "8", "train_wall": "59", "gb_free": "18.6", "wall": "14026"}
[2024-09-27 20:28:27,300][train_inner][INFO] - {"epoch": 66, "update": 65.376, "loss": "74.162", "ntokens": "7286.52", "nsentences": "38", "nll_loss": "0.387", "wps": "24782.6", "ups": "3.4", "wpb": "7286.5", "bsz": "38", "num_updates": "46600", "lr": "1.83e-05", "gnorm": "206.868", "loss_scale": "8", "train_wall": "58", "gb_free": "19.7", "wall": "14085"}
[2024-09-27 20:29:26,999][train_inner][INFO] - {"epoch": 66, "update": 65.656, "loss": "69.768", "ntokens": "7465.4", "nsentences": "41.44", "nll_loss": "0.387", "wps": "25010.2", "ups": "3.35", "wpb": "7465.4", "bsz": "41.4", "num_updates": "46800", "lr": "1.80279e-05", "gnorm": "191.583", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "14144"}
[2024-09-27 20:30:25,851][train_inner][INFO] - {"epoch": 66, "update": 65.937, "loss": "71.504", "ntokens": "7265.51", "nsentences": "38.4", "nll_loss": "0.378", "wps": "24690.7", "ups": "3.4", "wpb": "7265.5", "bsz": "38.4", "num_updates": "47000", "lr": "1.77599e-05", "gnorm": "201.326", "loss_scale": "8", "train_wall": "58", "gb_free": "19.1", "wall": "14203"}
[2024-09-27 20:30:39,235][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:30:39,236][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:30:39,272][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 67
[2024-09-27 20:30:41,896][valid][INFO] - {"epoch": 66, "valid_loss": "14.311", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.289", "valid_wer": "4.524", "valid_raw_wer": "4.524", "valid_wps": "22105.8", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "47045", "valid_best_wer": "4.524"}
[2024-09-27 20:30:41,896][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 66 @ 47045 updates
[2024-09-27 20:30:41,897][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:30:43,021][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:30:43,595][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 66 @ 47045 updates, score 4.524) (writing took 1.6988954220432788 seconds)
[2024-09-27 20:30:43,595][fairseq_cli.train][INFO] - end of epoch 66 (average epoch stats below)
[2024-09-27 20:30:43,596][train][INFO] - {"epoch": 66, "train_loss": "71.168", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.383", "train_wps": "24286.6", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "47045", "train_lr": "1.77002e-05", "train_gnorm": "198.996", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "14221"}
[2024-09-27 20:30:43,596][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:30:43,638][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 67
[2024-09-27 20:30:43,765][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:30:43,767][fairseq.trainer][INFO] - begin training epoch 67
[2024-09-27 20:30:43,767][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:31:30,162][train_inner][INFO] - {"epoch": 67, "update": 66.217, "loss": "71.734", "ntokens": "7444.42", "nsentences": "40", "nll_loss": "0.385", "wps": "23151.4", "ups": "3.11", "wpb": "7444.4", "bsz": "40", "num_updates": "47200", "lr": "1.74959e-05", "gnorm": "198.431", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "14268"}
[2024-09-27 20:32:29,206][train_inner][INFO] - {"epoch": 67, "update": 66.498, "loss": "72.673", "ntokens": "7320.44", "nsentences": "38.32", "nll_loss": "0.38", "wps": "24796.7", "ups": "3.39", "wpb": "7320.4", "bsz": "38.3", "num_updates": "47400", "lr": "1.72358e-05", "gnorm": "207.904", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "14327"}
[2024-09-27 20:33:28,174][train_inner][INFO] - {"epoch": 67, "update": 66.778, "loss": "68.041", "ntokens": "7308.38", "nsentences": "40.4", "nll_loss": "0.376", "wps": "24787.5", "ups": "3.39", "wpb": "7308.4", "bsz": "40.4", "num_updates": "47600", "lr": "1.69795e-05", "gnorm": "195.465", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "14386"}
[2024-09-27 20:34:14,849][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:34:14,849][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:34:14,885][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 68
[2024-09-27 20:34:17,564][valid][INFO] - {"epoch": 67, "valid_loss": "14.125", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.074", "valid_uer": "1.267", "valid_wer": "4.533", "valid_raw_wer": "4.533", "valid_wps": "22092.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "47758", "valid_best_wer": "4.524"}
[2024-09-27 20:34:17,564][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 67 @ 47758 updates
[2024-09-27 20:34:17,565][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:34:18,724][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:34:18,747][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 67 @ 47758 updates, score 4.533) (writing took 1.1822600469458848 seconds)
[2024-09-27 20:34:18,747][fairseq_cli.train][INFO] - end of epoch 67 (average epoch stats below)
[2024-09-27 20:34:18,747][train][INFO] - {"epoch": 67, "train_loss": "70.417", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.379", "train_wps": "24362.9", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "47758", "train_lr": "1.67798e-05", "train_gnorm": "200.828", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "14436"}
[2024-09-27 20:34:18,748][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:34:18,826][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 68
[2024-09-27 20:34:19,016][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:34:19,018][fairseq.trainer][INFO] - begin training epoch 68
[2024-09-27 20:34:19,018][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:34:31,854][train_inner][INFO] - {"epoch": 68, "update": 67.059, "loss": "69.808", "ntokens": "7393.64", "nsentences": "40.16", "nll_loss": "0.379", "wps": "23221.3", "ups": "3.14", "wpb": "7393.6", "bsz": "40.2", "num_updates": "47800", "lr": "1.67271e-05", "gnorm": "199.773", "loss_scale": "16", "train_wall": "59", "gb_free": "17.6", "wall": "14449"}
[2024-09-27 20:35:31,488][train_inner][INFO] - {"epoch": 68, "update": 67.339, "loss": "67.335", "ntokens": "7397.23", "nsentences": "40.28", "nll_loss": "0.367", "wps": "24809", "ups": "3.35", "wpb": "7397.2", "bsz": "40.3", "num_updates": "48000", "lr": "1.64784e-05", "gnorm": "200.618", "loss_scale": "16", "train_wall": "59", "gb_free": "19.4", "wall": "14509"}
[2024-09-27 20:36:30,285][train_inner][INFO] - {"epoch": 68, "update": 67.62, "loss": "75.082", "ntokens": "7269.98", "nsentences": "37.6", "nll_loss": "0.388", "wps": "24729.3", "ups": "3.4", "wpb": "7270", "bsz": "37.6", "num_updates": "48200", "lr": "1.62334e-05", "gnorm": "208.642", "loss_scale": "16", "train_wall": "58", "gb_free": "18.2", "wall": "14568"}
[2024-09-27 20:37:29,858][train_inner][INFO] - {"epoch": 68, "update": 67.9, "loss": "66.21", "ntokens": "7378.94", "nsentences": "40.88", "nll_loss": "0.367", "wps": "24772.7", "ups": "3.36", "wpb": "7378.9", "bsz": "40.9", "num_updates": "48400", "lr": "1.59921e-05", "gnorm": "197.924", "loss_scale": "16", "train_wall": "59", "gb_free": "17.8", "wall": "14627"}
[2024-09-27 20:37:50,584][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:37:50,585][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:37:50,619][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 69
[2024-09-27 20:37:53,261][valid][INFO] - {"epoch": 68, "valid_loss": "14.733", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.077", "valid_uer": "1.304", "valid_wer": "4.571", "valid_raw_wer": "4.571", "valid_wps": "22146", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "48471", "valid_best_wer": "4.524"}
[2024-09-27 20:37:53,262][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 68 @ 48471 updates
[2024-09-27 20:37:53,262][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:37:54,285][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:37:54,301][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 68 @ 48471 updates, score 4.571) (writing took 1.0389962049666792 seconds)
[2024-09-27 20:37:54,301][fairseq_cli.train][INFO] - end of epoch 68 (average epoch stats below)
[2024-09-27 20:37:54,301][train][INFO] - {"epoch": 68, "train_loss": "70.04", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.377", "train_wps": "24317.5", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "48471", "train_lr": "1.59073e-05", "train_gnorm": "202.045", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "18.6", "train_wall": "14652"}
[2024-09-27 20:37:54,302][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:37:54,343][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 69
[2024-09-27 20:37:54,463][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:37:54,465][fairseq.trainer][INFO] - begin training epoch 69
[2024-09-27 20:37:54,466][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:38:32,950][train_inner][INFO] - {"epoch": 69, "update": 68.181, "loss": "69.876", "ntokens": "7326.87", "nsentences": "39.6", "nll_loss": "0.378", "wps": "23226", "ups": "3.17", "wpb": "7326.9", "bsz": "39.6", "num_updates": "48600", "lr": "1.57543e-05", "gnorm": "199.021", "loss_scale": "16", "train_wall": "59", "gb_free": "19.2", "wall": "14690"}
[2024-09-27 20:39:32,216][train_inner][INFO] - {"epoch": 69, "update": 68.461, "loss": "69.391", "ntokens": "7331.44", "nsentences": "39.76", "nll_loss": "0.376", "wps": "24740.9", "ups": "3.37", "wpb": "7331.4", "bsz": "39.8", "num_updates": "48800", "lr": "1.55201e-05", "gnorm": "198.49", "loss_scale": "16", "train_wall": "59", "gb_free": "19.5", "wall": "14750"}
[2024-09-27 20:40:31,806][train_inner][INFO] - {"epoch": 69, "update": 68.742, "loss": "72.483", "ntokens": "7377.4", "nsentences": "38.96", "nll_loss": "0.383", "wps": "24760.5", "ups": "3.36", "wpb": "7377.4", "bsz": "39", "num_updates": "49000", "lr": "1.52894e-05", "gnorm": "206.573", "loss_scale": "16", "train_wall": "59", "gb_free": "17.7", "wall": "14809"}
[2024-09-27 20:40:42,054][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-09-27 20:41:25,979][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:41:25,980][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:41:26,015][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 70
[2024-09-27 20:41:28,644][valid][INFO] - {"epoch": 69, "valid_loss": "14.873", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.078", "valid_uer": "1.309", "valid_wer": "4.571", "valid_raw_wer": "4.571", "valid_wps": "22116.6", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "49183", "valid_best_wer": "4.524"}
[2024-09-27 20:41:28,644][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 69 @ 49183 updates
[2024-09-27 20:41:28,645][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:41:29,730][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:41:29,747][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 69 @ 49183 updates, score 4.571) (writing took 1.1030983531381935 seconds)
[2024-09-27 20:41:29,748][fairseq_cli.train][INFO] - end of epoch 69 (average epoch stats below)
[2024-09-27 20:41:29,748][train][INFO] - {"epoch": 69, "train_loss": "70.085", "train_ntokens": "7351.11", "train_nsentences": "39.6067", "train_nll_loss": "0.378", "train_wps": "24293.7", "train_ups": "3.3", "train_wpb": "7351.1", "train_bsz": "39.6", "train_num_updates": "49183", "train_lr": "1.50812e-05", "train_gnorm": "200.76", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "14867"}
[2024-09-27 20:41:29,748][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:41:29,789][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 70
[2024-09-27 20:41:29,910][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:41:29,912][fairseq.trainer][INFO] - begin training epoch 70
[2024-09-27 20:41:29,912][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:41:35,102][train_inner][INFO] - {"epoch": 70, "update": 69.024, "loss": "69.92", "ntokens": "7313.94", "nsentences": "39.6", "nll_loss": "0.379", "wps": "23110.3", "ups": "3.16", "wpb": "7313.9", "bsz": "39.6", "num_updates": "49200", "lr": "1.50621e-05", "gnorm": "200.733", "loss_scale": "8", "train_wall": "59", "gb_free": "17.8", "wall": "14873"}
[2024-09-27 20:42:34,090][train_inner][INFO] - {"epoch": 70, "update": 69.304, "loss": "71.326", "ntokens": "7316.15", "nsentences": "39.2", "nll_loss": "0.382", "wps": "24805.5", "ups": "3.39", "wpb": "7316.1", "bsz": "39.2", "num_updates": "49400", "lr": "1.48381e-05", "gnorm": "204.67", "loss_scale": "8", "train_wall": "59", "gb_free": "18.1", "wall": "14931"}
[2024-09-27 20:43:33,561][train_inner][INFO] - {"epoch": 70, "update": 69.585, "loss": "69.59", "ntokens": "7382.23", "nsentences": "40.24", "nll_loss": "0.379", "wps": "24826.5", "ups": "3.36", "wpb": "7382.2", "bsz": "40.2", "num_updates": "49600", "lr": "1.46175e-05", "gnorm": "198.11", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "14991"}
[2024-09-27 20:44:32,479][train_inner][INFO] - {"epoch": 70, "update": 69.865, "loss": "73.546", "ntokens": "7318.5", "nsentences": "38.8", "nll_loss": "0.39", "wps": "24843.2", "ups": "3.39", "wpb": "7318.5", "bsz": "38.8", "num_updates": "49800", "lr": "1.44002e-05", "gnorm": "201.378", "loss_scale": "8", "train_wall": "59", "gb_free": "18.1", "wall": "15050"}
[2024-09-27 20:45:01,383][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:45:01,383][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:45:01,419][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 71
[2024-09-27 20:45:04,055][valid][INFO] - {"epoch": 70, "valid_loss": "14.506", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.076", "valid_uer": "1.312", "valid_wer": "4.589", "valid_raw_wer": "4.589", "valid_wps": "22132.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "49896", "valid_best_wer": "4.524"}
[2024-09-27 20:45:04,055][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 70 @ 49896 updates
[2024-09-27 20:45:04,056][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:45:05,138][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:45:05,157][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 70 @ 49896 updates, score 4.589) (writing took 1.1013590248767287 seconds)
[2024-09-27 20:45:05,157][fairseq_cli.train][INFO] - end of epoch 70 (average epoch stats below)
[2024-09-27 20:45:05,157][train][INFO] - {"epoch": 70, "train_loss": "70.732", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.381", "train_wps": "24333.7", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "49896", "train_lr": "1.4297e-05", "train_gnorm": "202.583", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.4", "train_wall": "15083"}
[2024-09-27 20:45:05,158][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:45:05,199][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 71
[2024-09-27 20:45:05,320][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:45:05,322][fairseq.trainer][INFO] - begin training epoch 71
[2024-09-27 20:45:05,322][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:45:36,284][train_inner][INFO] - {"epoch": 71, "update": 70.146, "loss": "68.883", "ntokens": "7431.95", "nsentences": "40.32", "nll_loss": "0.374", "wps": "23295.7", "ups": "3.13", "wpb": "7431.9", "bsz": "40.3", "num_updates": "50000", "lr": "1.41861e-05", "gnorm": "207.171", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "15114"}
[2024-09-27 20:46:35,311][train_inner][INFO] - {"epoch": 71, "update": 70.426, "loss": "70.073", "ntokens": "7323.18", "nsentences": "39.48", "nll_loss": "0.378", "wps": "24813.2", "ups": "3.39", "wpb": "7323.2", "bsz": "39.5", "num_updates": "50200", "lr": "1.39752e-05", "gnorm": "202.365", "loss_scale": "8", "train_wall": "59", "gb_free": "17.9", "wall": "15173"}
[2024-09-27 20:47:34,465][train_inner][INFO] - {"epoch": 71, "update": 70.707, "loss": "69.31", "ntokens": "7322.46", "nsentences": "39.28", "nll_loss": "0.372", "wps": "24757.2", "ups": "3.38", "wpb": "7322.5", "bsz": "39.3", "num_updates": "50400", "lr": "1.37674e-05", "gnorm": "205.254", "loss_scale": "8", "train_wall": "59", "gb_free": "19", "wall": "15232"}
[2024-09-27 20:48:33,830][train_inner][INFO] - {"epoch": 71, "update": 70.987, "loss": "69.416", "ntokens": "7390.89", "nsentences": "40.4", "nll_loss": "0.379", "wps": "24900", "ups": "3.37", "wpb": "7390.9", "bsz": "40.4", "num_updates": "50600", "lr": "1.35628e-05", "gnorm": "198.342", "loss_scale": "8", "train_wall": "59", "gb_free": "18.1", "wall": "15291"}
[2024-09-27 20:48:36,352][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:48:36,352][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:48:36,388][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 72
[2024-09-27 20:48:39,018][valid][INFO] - {"epoch": 71, "valid_loss": "14.66", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.077", "valid_uer": "1.309", "valid_wer": "4.505", "valid_raw_wer": "4.505", "valid_wps": "22144.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "50609", "valid_best_wer": "4.505"}
[2024-09-27 20:48:39,019][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 71 @ 50609 updates
[2024-09-27 20:48:39,019][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:48:40,112][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:48:40,695][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 71 @ 50609 updates, score 4.505) (writing took 1.6765229229349643 seconds)
[2024-09-27 20:48:40,695][fairseq_cli.train][INFO] - end of epoch 71 (average epoch stats below)
[2024-09-27 20:48:40,696][train][INFO] - {"epoch": 71, "train_loss": "69.922", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.377", "train_wps": "24319.2", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "50609", "train_lr": "1.35536e-05", "train_gnorm": "203.128", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19", "train_wall": "15298"}
[2024-09-27 20:48:40,697][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:48:40,738][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 72
[2024-09-27 20:48:40,860][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:48:40,862][fairseq.trainer][INFO] - begin training epoch 72
[2024-09-27 20:48:40,862][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:49:38,042][train_inner][INFO] - {"epoch": 72, "update": 71.268, "loss": "69.373", "ntokens": "7407.38", "nsentences": "39.52", "nll_loss": "0.37", "wps": "23071.7", "ups": "3.11", "wpb": "7407.4", "bsz": "39.5", "num_updates": "50800", "lr": "1.33611e-05", "gnorm": "208.084", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "15355"}
[2024-09-27 20:50:37,626][train_inner][INFO] - {"epoch": 72, "update": 71.548, "loss": "68.058", "ntokens": "7362.6", "nsentences": "39.88", "nll_loss": "0.369", "wps": "24713.2", "ups": "3.36", "wpb": "7362.6", "bsz": "39.9", "num_updates": "51000", "lr": "1.31625e-05", "gnorm": "196.652", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "15415"}
[2024-09-27 20:51:37,127][train_inner][INFO] - {"epoch": 72, "update": 71.829, "loss": "67.422", "ntokens": "7333.7", "nsentences": "40.16", "nll_loss": "0.369", "wps": "24651", "ups": "3.36", "wpb": "7333.7", "bsz": "40.2", "num_updates": "51200", "lr": "1.29668e-05", "gnorm": "198.239", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "15475"}
[2024-09-27 20:52:12,812][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:52:12,812][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:52:12,849][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 73
[2024-09-27 20:52:15,497][valid][INFO] - {"epoch": 72, "valid_loss": "14.458", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.076", "valid_uer": "1.289", "valid_wer": "4.477", "valid_raw_wer": "4.477", "valid_wps": "22112.2", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "51322", "valid_best_wer": "4.477"}
[2024-09-27 20:52:15,498][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 72 @ 51322 updates
[2024-09-27 20:52:15,498][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:52:16,546][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:52:17,128][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 72 @ 51322 updates, score 4.477) (writing took 1.630379037000239 seconds)
[2024-09-27 20:52:17,129][fairseq_cli.train][INFO] - end of epoch 72 (average epoch stats below)
[2024-09-27 20:52:17,129][train][INFO] - {"epoch": 72, "train_loss": "68.934", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.371", "train_wps": "24218.6", "train_ups": "3.29", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "51322", "train_lr": "1.28489e-05", "train_gnorm": "202.845", "train_loss_scale": "8", "train_train_wall": "211", "train_gb_free": "19.2", "train_wall": "15515"}
[2024-09-27 20:52:17,130][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:52:17,172][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 73
[2024-09-27 20:52:17,294][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:52:17,296][fairseq.trainer][INFO] - begin training epoch 73
[2024-09-27 20:52:17,296][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:52:40,319][train_inner][INFO] - {"epoch": 73, "update": 72.109, "loss": "71.628", "ntokens": "7282.49", "nsentences": "38.76", "nll_loss": "0.381", "wps": "23048.6", "ups": "3.16", "wpb": "7282.5", "bsz": "38.8", "num_updates": "51400", "lr": "1.2774e-05", "gnorm": "211.597", "loss_scale": "8", "train_wall": "58", "gb_free": "18.9", "wall": "15538"}
[2024-09-27 20:53:39,870][train_inner][INFO] - {"epoch": 73, "update": 72.39, "loss": "69.259", "ntokens": "7347.44", "nsentences": "39.48", "nll_loss": "0.372", "wps": "24676.3", "ups": "3.36", "wpb": "7347.4", "bsz": "39.5", "num_updates": "51600", "lr": "1.25841e-05", "gnorm": "204.757", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "15597"}
[2024-09-27 20:54:39,138][train_inner][INFO] - {"epoch": 73, "update": 72.67, "loss": "68.46", "ntokens": "7361.24", "nsentences": "40.4", "nll_loss": "0.376", "wps": "24840.7", "ups": "3.37", "wpb": "7361.2", "bsz": "40.4", "num_updates": "51800", "lr": "1.2397e-05", "gnorm": "198.34", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "15657"}
[2024-09-27 20:55:38,342][train_inner][INFO] - {"epoch": 73, "update": 72.951, "loss": "71.949", "ntokens": "7312.6", "nsentences": "38.24", "nll_loss": "0.376", "wps": "24703", "ups": "3.38", "wpb": "7312.6", "bsz": "38.2", "num_updates": "52000", "lr": "1.22127e-05", "gnorm": "205.899", "loss_scale": "8", "train_wall": "59", "gb_free": "18.1", "wall": "15716"}
[2024-09-27 20:55:49,007][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:55:49,007][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:55:49,044][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 74
[2024-09-27 20:55:51,665][valid][INFO] - {"epoch": 73, "valid_loss": "14.336", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.286", "valid_wer": "4.411", "valid_raw_wer": "4.411", "valid_wps": "22095.7", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "52035", "valid_best_wer": "4.411"}
[2024-09-27 20:55:51,666][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 73 @ 52035 updates
[2024-09-27 20:55:51,666][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:55:52,778][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 20:55:53,364][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 73 @ 52035 updates, score 4.411) (writing took 1.6986477449536324 seconds)
[2024-09-27 20:55:53,365][fairseq_cli.train][INFO] - end of epoch 73 (average epoch stats below)
[2024-09-27 20:55:53,365][train][INFO] - {"epoch": 73, "train_loss": "69.458", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.374", "train_wps": "24240.7", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "52035", "train_lr": "1.21807e-05", "train_gnorm": "203.127", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "15731"}
[2024-09-27 20:55:53,366][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:55:53,407][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 74
[2024-09-27 20:55:53,526][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:55:53,528][fairseq.trainer][INFO] - begin training epoch 74
[2024-09-27 20:55:53,528][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:56:42,426][train_inner][INFO] - {"epoch": 74, "update": 73.231, "loss": "65.673", "ntokens": "7331.95", "nsentences": "40.52", "nll_loss": "0.363", "wps": "22882.3", "ups": "3.12", "wpb": "7331.9", "bsz": "40.5", "num_updates": "52200", "lr": "1.20311e-05", "gnorm": "196.141", "loss_scale": "8", "train_wall": "59", "gb_free": "18.1", "wall": "15780"}
[2024-09-27 20:57:41,673][train_inner][INFO] - {"epoch": 74, "update": 73.512, "loss": "69.212", "ntokens": "7303.68", "nsentences": "38.92", "nll_loss": "0.369", "wps": "24655", "ups": "3.38", "wpb": "7303.7", "bsz": "38.9", "num_updates": "52400", "lr": "1.18523e-05", "gnorm": "203.696", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "15839"}
[2024-09-27 20:58:41,288][train_inner][INFO] - {"epoch": 74, "update": 73.792, "loss": "69.952", "ntokens": "7392.2", "nsentences": "39.72", "nll_loss": "0.376", "wps": "24800.1", "ups": "3.35", "wpb": "7392.2", "bsz": "39.7", "num_updates": "52600", "lr": "1.16761e-05", "gnorm": "205.46", "loss_scale": "8", "train_wall": "59", "gb_free": "18.4", "wall": "15899"}
[2024-09-27 20:59:25,605][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 20:59:25,605][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:59:25,640][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 75
[2024-09-27 20:59:28,264][valid][INFO] - {"epoch": 74, "valid_loss": "14.516", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.076", "valid_uer": "1.297", "valid_wer": "4.458", "valid_raw_wer": "4.458", "valid_wps": "22125", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "52748", "valid_best_wer": "4.411"}
[2024-09-27 20:59:28,264][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 74 @ 52748 updates
[2024-09-27 20:59:28,265][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:59:29,317][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 20:59:29,328][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 74 @ 52748 updates, score 4.458) (writing took 1.0635800920426846 seconds)
[2024-09-27 20:59:29,328][fairseq_cli.train][INFO] - end of epoch 74 (average epoch stats below)
[2024-09-27 20:59:29,328][train][INFO] - {"epoch": 74, "train_loss": "68.556", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.369", "train_wps": "24271.3", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "52748", "train_lr": "1.15474e-05", "train_gnorm": "201.092", "train_loss_scale": "8", "train_train_wall": "211", "train_gb_free": "19", "train_wall": "15947"}
[2024-09-27 20:59:29,329][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 20:59:29,370][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 75
[2024-09-27 20:59:29,491][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 20:59:29,493][fairseq.trainer][INFO] - begin training epoch 75
[2024-09-27 20:59:29,493][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 20:59:45,142][train_inner][INFO] - {"epoch": 75, "update": 74.073, "loss": "68.229", "ntokens": "7441.65", "nsentences": "39.76", "nll_loss": "0.365", "wps": "23308.1", "ups": "3.13", "wpb": "7441.7", "bsz": "39.8", "num_updates": "52800", "lr": "1.15025e-05", "gnorm": "196.424", "loss_scale": "8", "train_wall": "60", "gb_free": "18.9", "wall": "15963"}
[2024-09-27 21:00:44,251][train_inner][INFO] - {"epoch": 75, "update": 74.353, "loss": "68.951", "ntokens": "7349.1", "nsentences": "40.04", "nll_loss": "0.376", "wps": "24866.5", "ups": "3.38", "wpb": "7349.1", "bsz": "40", "num_updates": "53000", "lr": "1.13315e-05", "gnorm": "204.439", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "16022"}
[2024-09-27 21:01:43,137][train_inner][INFO] - {"epoch": 75, "update": 74.634, "loss": "70.534", "ntokens": "7278.13", "nsentences": "37.92", "nll_loss": "0.367", "wps": "24719.4", "ups": "3.4", "wpb": "7278.1", "bsz": "37.9", "num_updates": "53200", "lr": "1.1163e-05", "gnorm": "207.598", "loss_scale": "16", "train_wall": "59", "gb_free": "18.2", "wall": "16081"}
[2024-09-27 21:02:42,731][train_inner][INFO] - {"epoch": 75, "update": 74.914, "loss": "66.176", "ntokens": "7396.24", "nsentences": "40.76", "nll_loss": "0.365", "wps": "24822.4", "ups": "3.36", "wpb": "7396.2", "bsz": "40.8", "num_updates": "53400", "lr": "1.09971e-05", "gnorm": "198.919", "loss_scale": "16", "train_wall": "59", "gb_free": "19", "wall": "16140"}
[2024-09-27 21:03:00,819][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:03:00,819][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:03:00,863][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 76
[2024-09-27 21:03:03,508][valid][INFO] - {"epoch": 75, "valid_loss": "14.357", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.289", "valid_wer": "4.411", "valid_raw_wer": "4.411", "valid_wps": "22134.9", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "53461", "valid_best_wer": "4.411"}
[2024-09-27 21:03:03,508][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 75 @ 53461 updates
[2024-09-27 21:03:03,509][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 21:03:04,625][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 21:03:05,210][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 75 @ 53461 updates, score 4.411) (writing took 1.701870409073308 seconds)
[2024-09-27 21:03:05,210][fairseq_cli.train][INFO] - end of epoch 75 (average epoch stats below)
[2024-09-27 21:03:05,211][train][INFO] - {"epoch": 75, "train_loss": "68.594", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.37", "train_wps": "24280.4", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "53461", "train_lr": "1.09469e-05", "train_gnorm": "203.589", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "19.7", "train_wall": "16163"}
[2024-09-27 21:03:05,211][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:03:05,252][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 76
[2024-09-27 21:03:05,373][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:03:05,375][fairseq.trainer][INFO] - begin training epoch 76
[2024-09-27 21:03:05,375][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:03:46,236][train_inner][INFO] - {"epoch": 76, "update": 75.195, "loss": "70.87", "ntokens": "7281.6", "nsentences": "38.16", "nll_loss": "0.371", "wps": "22932.3", "ups": "3.15", "wpb": "7281.6", "bsz": "38.2", "num_updates": "53600", "lr": "1.08336e-05", "gnorm": "210.31", "loss_scale": "16", "train_wall": "59", "gb_free": "18.9", "wall": "16204"}
[2024-09-27 21:04:45,219][train_inner][INFO] - {"epoch": 76, "update": 75.475, "loss": "68.734", "ntokens": "7337.88", "nsentences": "40.12", "nll_loss": "0.376", "wps": "24881.2", "ups": "3.39", "wpb": "7337.9", "bsz": "40.1", "num_updates": "53800", "lr": "1.06725e-05", "gnorm": "202.319", "loss_scale": "16", "train_wall": "59", "gb_free": "18.4", "wall": "16263"}
[2024-09-27 21:05:44,572][train_inner][INFO] - {"epoch": 76, "update": 75.756, "loss": "70.576", "ntokens": "7384.98", "nsentences": "39.2", "nll_loss": "0.375", "wps": "24884.9", "ups": "3.37", "wpb": "7385", "bsz": "39.2", "num_updates": "54000", "lr": "1.05138e-05", "gnorm": "203.854", "loss_scale": "16", "train_wall": "59", "gb_free": "17.5", "wall": "16322"}
[2024-09-27 21:06:36,597][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:06:36,597][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:06:36,633][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 77
[2024-09-27 21:06:39,267][valid][INFO] - {"epoch": 76, "valid_loss": "14.567", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.076", "valid_uer": "1.307", "valid_wer": "4.505", "valid_raw_wer": "4.505", "valid_wps": "22098.1", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "54174", "valid_best_wer": "4.411"}
[2024-09-27 21:06:39,268][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 76 @ 54174 updates
[2024-09-27 21:06:39,268][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:06:40,341][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:06:40,351][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 76 @ 54174 updates, score 4.505) (writing took 1.0833806251175702 seconds)
[2024-09-27 21:06:40,352][fairseq_cli.train][INFO] - end of epoch 76 (average epoch stats below)
[2024-09-27 21:06:40,352][train][INFO] - {"epoch": 76, "train_loss": "68.987", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.372", "train_wps": "24364.1", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "54174", "train_lr": "1.03777e-05", "train_gnorm": "201.461", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "18.2", "train_wall": "16378"}
[2024-09-27 21:06:40,353][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:06:40,393][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 77
[2024-09-27 21:06:40,515][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:06:40,517][fairseq.trainer][INFO] - begin training epoch 77
[2024-09-27 21:06:40,517][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:06:48,330][train_inner][INFO] - {"epoch": 77, "update": 76.036, "loss": "66.313", "ntokens": "7419.06", "nsentences": "41.24", "nll_loss": "0.369", "wps": "23273", "ups": "3.14", "wpb": "7419.1", "bsz": "41.2", "num_updates": "54200", "lr": "1.03575e-05", "gnorm": "191.952", "loss_scale": "16", "train_wall": "59", "gb_free": "18", "wall": "16386"}
[2024-09-27 21:07:47,526][train_inner][INFO] - {"epoch": 77, "update": 76.317, "loss": "70.548", "ntokens": "7347.52", "nsentences": "39.04", "nll_loss": "0.375", "wps": "24824.3", "ups": "3.38", "wpb": "7347.5", "bsz": "39", "num_updates": "54400", "lr": "1.02035e-05", "gnorm": "203.307", "loss_scale": "16", "train_wall": "59", "gb_free": "18", "wall": "16445"}
[2024-09-27 21:08:46,970][train_inner][INFO] - {"epoch": 77, "update": 76.597, "loss": "67.701", "ntokens": "7362.05", "nsentences": "40.36", "nll_loss": "0.371", "wps": "24769.6", "ups": "3.36", "wpb": "7362", "bsz": "40.4", "num_updates": "54600", "lr": "1.00518e-05", "gnorm": "203.221", "loss_scale": "16", "train_wall": "59", "gb_free": "19", "wall": "16504"}
[2024-09-27 21:09:46,352][train_inner][INFO] - {"epoch": 77, "update": 76.878, "loss": "70.29", "ntokens": "7374.85", "nsentences": "39.56", "nll_loss": "0.377", "wps": "24839", "ups": "3.37", "wpb": "7374.9", "bsz": "39.6", "num_updates": "54800", "lr": "9.90239e-06", "gnorm": "205.621", "loss_scale": "16", "train_wall": "59", "gb_free": "19.4", "wall": "16564"}
[2024-09-27 21:10:11,992][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:10:11,993][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:10:12,029][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 78
[2024-09-27 21:10:14,693][valid][INFO] - {"epoch": 77, "valid_loss": "14.294", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.316", "valid_wer": "4.514", "valid_raw_wer": "4.514", "valid_wps": "22082.6", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "54887", "valid_best_wer": "4.411"}
[2024-09-27 21:10:14,693][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 77 @ 54887 updates
[2024-09-27 21:10:14,694][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:10:15,755][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:10:15,775][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 77 @ 54887 updates, score 4.514) (writing took 1.0817553519736975 seconds)
[2024-09-27 21:10:15,775][fairseq_cli.train][INFO] - end of epoch 77 (average epoch stats below)
[2024-09-27 21:10:15,776][train][INFO] - {"epoch": 77, "train_loss": "69.18", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.373", "train_wps": "24332.1", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "54887", "train_lr": "9.83808e-06", "train_gnorm": "203.968", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "18.5", "train_wall": "16593"}
[2024-09-27 21:10:15,776][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:10:15,817][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 78
[2024-09-27 21:10:15,936][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:10:15,938][fairseq.trainer][INFO] - begin training epoch 78
[2024-09-27 21:10:15,938][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:10:49,247][train_inner][INFO] - {"epoch": 78, "update": 77.158, "loss": "68.236", "ntokens": "7283.37", "nsentences": "38.92", "nll_loss": "0.365", "wps": "23160.5", "ups": "3.18", "wpb": "7283.4", "bsz": "38.9", "num_updates": "55000", "lr": "9.75517e-06", "gnorm": "201.819", "loss_scale": "16", "train_wall": "59", "gb_free": "18.5", "wall": "16627"}
[2024-09-27 21:11:48,990][train_inner][INFO] - {"epoch": 78, "update": 77.439, "loss": "66.527", "ntokens": "7389.61", "nsentences": "40.28", "nll_loss": "0.363", "wps": "24737.9", "ups": "3.35", "wpb": "7389.6", "bsz": "40.3", "num_updates": "55200", "lr": "9.61014e-06", "gnorm": "202.622", "loss_scale": "16", "train_wall": "59", "gb_free": "19.2", "wall": "16686"}
[2024-09-27 21:12:48,251][train_inner][INFO] - {"epoch": 78, "update": 77.719, "loss": "66.012", "ntokens": "7338.72", "nsentences": "40.2", "nll_loss": "0.362", "wps": "24767.4", "ups": "3.37", "wpb": "7338.7", "bsz": "40.2", "num_updates": "55400", "lr": "9.46727e-06", "gnorm": "200.77", "loss_scale": "16", "train_wall": "59", "gb_free": "19.4", "wall": "16746"}
[2024-09-27 21:13:47,502][train_inner][INFO] - {"epoch": 78, "update": 78.0, "loss": "70.36", "ntokens": "7351.36", "nsentences": "38.72", "nll_loss": "0.371", "wps": "24814.2", "ups": "3.38", "wpb": "7351.4", "bsz": "38.7", "num_updates": "55600", "lr": "9.32652e-06", "gnorm": "204.54", "loss_scale": "16", "train_wall": "59", "gb_free": "19.2", "wall": "16805"}
[2024-09-27 21:13:47,503][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:13:47,503][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:13:47,537][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 79
[2024-09-27 21:13:50,175][valid][INFO] - {"epoch": 78, "valid_loss": "14.583", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.076", "valid_uer": "1.325", "valid_wer": "4.618", "valid_raw_wer": "4.618", "valid_wps": "22129.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "55600", "valid_best_wer": "4.411"}
[2024-09-27 21:13:50,176][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 78 @ 55600 updates
[2024-09-27 21:13:50,176][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:13:51,289][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:13:51,307][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 78 @ 55600 updates, score 4.618) (writing took 1.130648413905874 seconds)
[2024-09-27 21:13:51,307][fairseq_cli.train][INFO] - end of epoch 78 (average epoch stats below)
[2024-09-27 21:13:51,307][train][INFO] - {"epoch": 78, "train_loss": "67.876", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.366", "train_wps": "24320", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "55600", "train_lr": "9.32652e-06", "train_gnorm": "202.182", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "19.2", "train_wall": "16809"}
[2024-09-27 21:13:51,308][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:13:51,349][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 79
[2024-09-27 21:13:51,469][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:13:51,471][fairseq.trainer][INFO] - begin training epoch 79
[2024-09-27 21:13:51,471][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:14:50,944][train_inner][INFO] - {"epoch": 79, "update": 78.281, "loss": "67.205", "ntokens": "7366.19", "nsentences": "40.24", "nll_loss": "0.367", "wps": "23222.1", "ups": "3.15", "wpb": "7366.2", "bsz": "40.2", "num_updates": "55800", "lr": "9.18786e-06", "gnorm": "201.193", "loss_scale": "16", "train_wall": "59", "gb_free": "18.3", "wall": "16868"}
[2024-09-27 21:15:50,374][train_inner][INFO] - {"epoch": 79, "update": 78.561, "loss": "67.121", "ntokens": "7374.79", "nsentences": "40.28", "nll_loss": "0.367", "wps": "24818.3", "ups": "3.37", "wpb": "7374.8", "bsz": "40.3", "num_updates": "56000", "lr": "9.05126e-06", "gnorm": "204.346", "loss_scale": "16", "train_wall": "59", "gb_free": "19.3", "wall": "16928"}
[2024-09-27 21:16:49,689][train_inner][INFO] - {"epoch": 79, "update": 78.842, "loss": "67.445", "ntokens": "7346.73", "nsentences": "39.04", "nll_loss": "0.358", "wps": "24772.1", "ups": "3.37", "wpb": "7346.7", "bsz": "39", "num_updates": "56200", "lr": "8.9167e-06", "gnorm": "203.438", "loss_scale": "16", "train_wall": "59", "gb_free": "19.2", "wall": "16987"}
[2024-09-27 21:17:22,855][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:17:22,855][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:17:22,889][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 80
[2024-09-27 21:17:25,545][valid][INFO] - {"epoch": 79, "valid_loss": "14.365", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.318", "valid_wer": "4.533", "valid_raw_wer": "4.533", "valid_wps": "21998.9", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "56313", "valid_best_wer": "4.411"}
[2024-09-27 21:17:25,546][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 79 @ 56313 updates
[2024-09-27 21:17:25,546][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:17:26,658][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:17:26,677][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 79 @ 56313 updates, score 4.533) (writing took 1.131268470082432 seconds)
[2024-09-27 21:17:26,678][fairseq_cli.train][INFO] - end of epoch 79 (average epoch stats below)
[2024-09-27 21:17:26,678][train][INFO] - {"epoch": 79, "train_loss": "68.331", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.368", "train_wps": "24338.1", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "56313", "train_lr": "8.84156e-06", "train_gnorm": "204.784", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "19.2", "train_wall": "17024"}
[2024-09-27 21:17:26,678][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:17:26,719][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 80
[2024-09-27 21:17:26,838][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:17:26,840][fairseq.trainer][INFO] - begin training epoch 80
[2024-09-27 21:17:26,840][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:17:52,596][train_inner][INFO] - {"epoch": 80, "update": 79.122, "loss": "70.061", "ntokens": "7262.99", "nsentences": "38.52", "nll_loss": "0.372", "wps": "23091.4", "ups": "3.18", "wpb": "7263", "bsz": "38.5", "num_updates": "56400", "lr": "8.78413e-06", "gnorm": "208.412", "loss_scale": "16", "train_wall": "59", "gb_free": "19.5", "wall": "17050"}
[2024-09-27 21:18:52,489][train_inner][INFO] - {"epoch": 80, "update": 79.403, "loss": "65.119", "ntokens": "7413.35", "nsentences": "41.24", "nll_loss": "0.362", "wps": "24755.1", "ups": "3.34", "wpb": "7413.4", "bsz": "41.2", "num_updates": "56600", "lr": "8.65354e-06", "gnorm": "201.494", "loss_scale": "16", "train_wall": "59", "gb_free": "18.8", "wall": "17110"}
[2024-09-27 21:19:51,890][train_inner][INFO] - {"epoch": 80, "update": 79.683, "loss": "68.687", "ntokens": "7367.4", "nsentences": "39.44", "nll_loss": "0.368", "wps": "24806", "ups": "3.37", "wpb": "7367.4", "bsz": "39.4", "num_updates": "56800", "lr": "8.52489e-06", "gnorm": "213.157", "loss_scale": "16", "train_wall": "59", "gb_free": "19.2", "wall": "17169"}
[2024-09-27 21:20:50,984][train_inner][INFO] - {"epoch": 80, "update": 79.964, "loss": "70.258", "ntokens": "7354.23", "nsentences": "38.68", "nll_loss": "0.37", "wps": "24889.8", "ups": "3.38", "wpb": "7354.2", "bsz": "38.7", "num_updates": "57000", "lr": "8.39815e-06", "gnorm": "200.385", "loss_scale": "16", "train_wall": "59", "gb_free": "19.1", "wall": "17228"}
[2024-09-27 21:20:58,617][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:20:58,618][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:20:58,652][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 81
[2024-09-27 21:21:01,292][valid][INFO] - {"epoch": 80, "valid_loss": "14.237", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.074", "valid_uer": "1.268", "valid_wer": "4.467", "valid_raw_wer": "4.467", "valid_wps": "22110", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "57026", "valid_best_wer": "4.411"}
[2024-09-27 21:21:01,292][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 80 @ 57026 updates
[2024-09-27 21:21:01,292][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:21:02,374][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:21:02,391][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 80 @ 57026 updates, score 4.467) (writing took 1.099335664184764 seconds)
[2024-09-27 21:21:02,392][fairseq_cli.train][INFO] - end of epoch 80 (average epoch stats below)
[2024-09-27 21:21:02,392][train][INFO] - {"epoch": 80, "train_loss": "67.47", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.363", "train_wps": "24299.4", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "57026", "train_lr": "8.38181e-06", "train_gnorm": "204.97", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "19", "train_wall": "17240"}
[2024-09-27 21:21:02,393][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:21:02,433][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 81
[2024-09-27 21:21:02,552][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:21:02,553][fairseq.trainer][INFO] - begin training epoch 81
[2024-09-27 21:21:02,554][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:21:54,072][train_inner][INFO] - {"epoch": 81, "update": 80.244, "loss": "66.406", "ntokens": "7307.04", "nsentences": "39.72", "nll_loss": "0.361", "wps": "23164.8", "ups": "3.17", "wpb": "7307", "bsz": "39.7", "num_updates": "57200", "lr": "8.27329e-06", "gnorm": "208.848", "loss_scale": "16", "train_wall": "59", "gb_free": "18.3", "wall": "17291"}
[2024-09-27 21:22:46,662][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
[2024-09-27 21:22:53,403][train_inner][INFO] - {"epoch": 81, "update": 80.526, "loss": "69.721", "ntokens": "7305.85", "nsentences": "38.8", "nll_loss": "0.37", "wps": "24627.3", "ups": "3.37", "wpb": "7305.8", "bsz": "38.8", "num_updates": "57400", "lr": "8.15029e-06", "gnorm": "203.868", "loss_scale": "16", "train_wall": "59", "gb_free": "19.5", "wall": "17351"}
[2024-09-27 21:23:53,200][train_inner][INFO] - {"epoch": 81, "update": 80.806, "loss": "68.169", "ntokens": "7403.46", "nsentences": "39.88", "nll_loss": "0.367", "wps": "24762.2", "ups": "3.34", "wpb": "7403.5", "bsz": "39.9", "num_updates": "57600", "lr": "8.02912e-06", "gnorm": "202.918", "loss_scale": "16", "train_wall": "59", "gb_free": "17.8", "wall": "17411"}
[2024-09-27 21:24:34,247][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:24:34,248][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:24:34,282][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 82
[2024-09-27 21:24:36,905][valid][INFO] - {"epoch": 81, "valid_loss": "14.119", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.074", "valid_uer": "1.286", "valid_wer": "4.589", "valid_raw_wer": "4.589", "valid_wps": "22146.1", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "57738", "valid_best_wer": "4.411"}
[2024-09-27 21:24:36,905][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 81 @ 57738 updates
[2024-09-27 21:24:36,906][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:24:37,905][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:24:37,923][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 81 @ 57738 updates, score 4.589) (writing took 1.0173570769838989 seconds)
[2024-09-27 21:24:37,923][fairseq_cli.train][INFO] - end of epoch 81 (average epoch stats below)
[2024-09-27 21:24:37,923][train][INFO] - {"epoch": 81, "train_loss": "67.969", "train_ntokens": "7352.5", "train_nsentences": "39.618", "train_nll_loss": "0.366", "train_wps": "24288.8", "train_ups": "3.3", "train_wpb": "7352.5", "train_bsz": "39.6", "train_num_updates": "57738", "train_lr": "7.94657e-06", "train_gnorm": "203.013", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "19.1", "train_wall": "17455"}
[2024-09-27 21:24:37,924][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:24:37,965][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 82
[2024-09-27 21:24:38,083][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:24:38,085][fairseq.trainer][INFO] - begin training epoch 82
[2024-09-27 21:24:38,085][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:24:56,627][train_inner][INFO] - {"epoch": 82, "update": 81.087, "loss": "67.016", "ntokens": "7378.57", "nsentences": "39.92", "nll_loss": "0.363", "wps": "23266.3", "ups": "3.15", "wpb": "7378.6", "bsz": "39.9", "num_updates": "57800", "lr": "7.90975e-06", "gnorm": "198.095", "loss_scale": "16", "train_wall": "59", "gb_free": "19.4", "wall": "17474"}
[2024-09-27 21:25:55,504][train_inner][INFO] - {"epoch": 82, "update": 81.367, "loss": "66.931", "ntokens": "7313.64", "nsentences": "39.56", "nll_loss": "0.362", "wps": "24843.9", "ups": "3.4", "wpb": "7313.6", "bsz": "39.6", "num_updates": "58000", "lr": "7.79216e-06", "gnorm": "200.321", "loss_scale": "16", "train_wall": "59", "gb_free": "18.8", "wall": "17533"}
[2024-09-27 21:26:55,109][train_inner][INFO] - {"epoch": 82, "update": 81.648, "loss": "66.404", "ntokens": "7390.1", "nsentences": "40.64", "nll_loss": "0.365", "wps": "24797", "ups": "3.36", "wpb": "7390.1", "bsz": "40.6", "num_updates": "58200", "lr": "7.67631e-06", "gnorm": "203.088", "loss_scale": "16", "train_wall": "59", "gb_free": "17.9", "wall": "17593"}
[2024-09-27 21:27:54,584][train_inner][INFO] - {"epoch": 82, "update": 81.928, "loss": "67.877", "ntokens": "7373.94", "nsentences": "39.28", "nll_loss": "0.362", "wps": "24796.7", "ups": "3.36", "wpb": "7373.9", "bsz": "39.3", "num_updates": "58400", "lr": "7.56219e-06", "gnorm": "209.167", "loss_scale": "16", "train_wall": "59", "gb_free": "19.4", "wall": "17652"}
[2024-09-27 21:28:09,531][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:28:09,531][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:28:09,565][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 83
[2024-09-27 21:28:12,199][valid][INFO] - {"epoch": 82, "valid_loss": "14.047", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.073", "valid_uer": "1.245", "valid_wer": "4.486", "valid_raw_wer": "4.486", "valid_wps": "22141.5", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "58451", "valid_best_wer": "4.411"}
[2024-09-27 21:28:12,199][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 82 @ 58451 updates
[2024-09-27 21:28:12,200][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:28:13,318][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:28:13,336][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 82 @ 58451 updates, score 4.486) (writing took 1.1368900078814477 seconds)
[2024-09-27 21:28:13,337][fairseq_cli.train][INFO] - end of epoch 82 (average epoch stats below)
[2024-09-27 21:28:13,337][train][INFO] - {"epoch": 82, "train_loss": "67.364", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.363", "train_wps": "24333.3", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "58451", "train_lr": "7.53336e-06", "train_gnorm": "204.258", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "17671"}
[2024-09-27 21:28:13,338][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:28:13,379][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 83
[2024-09-27 21:28:13,498][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:28:13,500][fairseq.trainer][INFO] - begin training epoch 83
[2024-09-27 21:28:13,500][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:28:58,220][train_inner][INFO] - {"epoch": 83, "update": 82.209, "loss": "68.835", "ntokens": "7368.18", "nsentences": "38.88", "nll_loss": "0.363", "wps": "23157.6", "ups": "3.14", "wpb": "7368.2", "bsz": "38.9", "num_updates": "58600", "lr": "7.44976e-06", "gnorm": "200.202", "loss_scale": "16", "train_wall": "59", "gb_free": "19", "wall": "17716"}
[2024-09-27 21:29:05,868][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-09-27 21:29:44,838][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-09-27 21:29:58,193][train_inner][INFO] - {"epoch": 83, "update": 82.492, "loss": "65.853", "ntokens": "7324.92", "nsentences": "39.84", "nll_loss": "0.358", "wps": "24427.4", "ups": "3.33", "wpb": "7324.9", "bsz": "39.8", "num_updates": "58800", "lr": "7.339e-06", "gnorm": "204.827", "loss_scale": "4", "train_wall": "60", "gb_free": "18.9", "wall": "17776"}
[2024-09-27 21:30:57,615][train_inner][INFO] - {"epoch": 83, "update": 82.773, "loss": "66.15", "ntokens": "7364.03", "nsentences": "40.32", "nll_loss": "0.362", "wps": "24785.4", "ups": "3.37", "wpb": "7364", "bsz": "40.3", "num_updates": "59000", "lr": "7.2299e-06", "gnorm": "202.239", "loss_scale": "4", "train_wall": "59", "gb_free": "18.7", "wall": "17835"}
[2024-09-27 21:31:45,541][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:31:45,542][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:31:45,578][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 84
[2024-09-27 21:31:48,214][valid][INFO] - {"epoch": 83, "valid_loss": "14.185", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.074", "valid_uer": "1.251", "valid_wer": "4.505", "valid_raw_wer": "4.505", "valid_wps": "22130.7", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "59162", "valid_best_wer": "4.411"}
[2024-09-27 21:31:48,215][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 83 @ 59162 updates
[2024-09-27 21:31:48,215][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:31:49,289][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:31:49,306][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 83 @ 59162 updates, score 4.505) (writing took 1.09092288906686 seconds)
[2024-09-27 21:31:49,306][fairseq_cli.train][INFO] - end of epoch 83 (average epoch stats below)
[2024-09-27 21:31:49,306][train][INFO] - {"epoch": 83, "train_loss": "67.224", "train_ntokens": "7351.19", "train_nsentences": "39.5387", "train_nll_loss": "0.362", "train_wps": "24201.1", "train_ups": "3.29", "train_wpb": "7351.2", "train_bsz": "39.5", "train_num_updates": "59162", "train_lr": "7.14271e-06", "train_gnorm": "205.364", "train_loss_scale": "4", "train_train_wall": "211", "train_gb_free": "19.1", "train_wall": "17887"}
[2024-09-27 21:31:49,307][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:31:49,348][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 84
[2024-09-27 21:31:49,468][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:31:49,470][fairseq.trainer][INFO] - begin training epoch 84
[2024-09-27 21:31:49,470][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:32:00,988][train_inner][INFO] - {"epoch": 84, "update": 83.053, "loss": "68.175", "ntokens": "7339.56", "nsentences": "39.04", "nll_loss": "0.363", "wps": "23163.2", "ups": "3.16", "wpb": "7339.6", "bsz": "39", "num_updates": "59200", "lr": "7.12241e-06", "gnorm": "213.713", "loss_scale": "4", "train_wall": "59", "gb_free": "19", "wall": "17898"}
[2024-09-27 21:33:00,567][train_inner][INFO] - {"epoch": 84, "update": 83.334, "loss": "67.524", "ntokens": "7392.84", "nsentences": "40.12", "nll_loss": "0.366", "wps": "24817.1", "ups": "3.36", "wpb": "7392.8", "bsz": "40.1", "num_updates": "59400", "lr": "7.01652e-06", "gnorm": "204.808", "loss_scale": "4", "train_wall": "59", "gb_free": "18.4", "wall": "17958"}
[2024-09-27 21:33:59,365][train_inner][INFO] - {"epoch": 84, "update": 83.614, "loss": "68.037", "ntokens": "7284.36", "nsentences": "39.08", "nll_loss": "0.365", "wps": "24777.7", "ups": "3.4", "wpb": "7284.4", "bsz": "39.1", "num_updates": "59600", "lr": "6.9122e-06", "gnorm": "205.933", "loss_scale": "4", "train_wall": "58", "gb_free": "19.4", "wall": "18017"}
[2024-09-27 21:34:58,802][train_inner][INFO] - {"epoch": 84, "update": 83.895, "loss": "65.904", "ntokens": "7331.22", "nsentences": "39.56", "nll_loss": "0.356", "wps": "24668.9", "ups": "3.36", "wpb": "7331.2", "bsz": "39.6", "num_updates": "59800", "lr": "6.80944e-06", "gnorm": "202.712", "loss_scale": "4", "train_wall": "59", "gb_free": "19.2", "wall": "18076"}
[2024-09-27 21:35:21,073][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:35:21,073][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:35:21,117][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 85
[2024-09-27 21:35:23,761][valid][INFO] - {"epoch": 84, "valid_loss": "14.269", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.244", "valid_wer": "4.373", "valid_raw_wer": "4.373", "valid_wps": "22145.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "59875", "valid_best_wer": "4.373"}
[2024-09-27 21:35:23,761][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 84 @ 59875 updates
[2024-09-27 21:35:23,762][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 21:35:24,861][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 21:35:25,434][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 84 @ 59875 updates, score 4.373) (writing took 1.6721665228251368 seconds)
[2024-09-27 21:35:25,434][fairseq_cli.train][INFO] - end of epoch 84 (average epoch stats below)
[2024-09-27 21:35:25,434][train][INFO] - {"epoch": 84, "train_loss": "67.332", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.363", "train_wps": "24252.9", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "59875", "train_lr": "6.7713e-06", "train_gnorm": "204.429", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "19", "train_wall": "18103"}
[2024-09-27 21:35:25,435][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:35:25,476][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 85
[2024-09-27 21:35:25,595][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:35:25,597][fairseq.trainer][INFO] - begin training epoch 85
[2024-09-27 21:35:25,597][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:36:02,876][train_inner][INFO] - {"epoch": 85, "update": 84.175, "loss": "68.95", "ntokens": "7395.86", "nsentences": "39.4", "nll_loss": "0.367", "wps": "23085.3", "ups": "3.12", "wpb": "7395.9", "bsz": "39.4", "num_updates": "60000", "lr": "6.7082e-06", "gnorm": "204.146", "loss_scale": "4", "train_wall": "59", "gb_free": "18.7", "wall": "18140"}
[2024-09-27 21:37:02,115][train_inner][INFO] - {"epoch": 85, "update": 84.456, "loss": "68.805", "ntokens": "7344.71", "nsentences": "39.32", "nll_loss": "0.368", "wps": "24797.1", "ups": "3.38", "wpb": "7344.7", "bsz": "39.3", "num_updates": "60200", "lr": "6.60847e-06", "gnorm": "204.841", "loss_scale": "4", "train_wall": "59", "gb_free": "18.9", "wall": "18200"}
[2024-09-27 21:38:01,640][train_inner][INFO] - {"epoch": 85, "update": 84.736, "loss": "69.688", "ntokens": "7412.31", "nsentences": "39.84", "nll_loss": "0.375", "wps": "24904.9", "ups": "3.36", "wpb": "7412.3", "bsz": "39.8", "num_updates": "60400", "lr": "6.51022e-06", "gnorm": "206.771", "loss_scale": "4", "train_wall": "59", "gb_free": "18.9", "wall": "18259"}
[2024-09-27 21:38:56,885][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:38:56,885][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:38:56,922][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 86
[2024-09-27 21:38:59,565][valid][INFO] - {"epoch": 85, "valid_loss": "14.117", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.074", "valid_uer": "1.242", "valid_wer": "4.392", "valid_raw_wer": "4.392", "valid_wps": "22122.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "60588", "valid_best_wer": "4.373"}
[2024-09-27 21:38:59,565][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 85 @ 60588 updates
[2024-09-27 21:38:59,566][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:39:00,668][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:39:00,678][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 85 @ 60588 updates, score 4.392) (writing took 1.1129449589643627 seconds)
[2024-09-27 21:39:00,679][fairseq_cli.train][INFO] - end of epoch 85 (average epoch stats below)
[2024-09-27 21:39:00,679][train][INFO] - {"epoch": 85, "train_loss": "68.681", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.37", "train_wps": "24352.4", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "60588", "train_lr": "6.4192e-06", "train_gnorm": "204.36", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "19.8", "train_wall": "18318"}
[2024-09-27 21:39:00,680][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:39:00,720][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 86
[2024-09-27 21:39:00,840][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:39:00,842][fairseq.trainer][INFO] - begin training epoch 86
[2024-09-27 21:39:00,842][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:39:04,319][train_inner][INFO] - {"epoch": 86, "update": 85.017, "loss": "68.174", "ntokens": "7257.72", "nsentences": "39.32", "nll_loss": "0.369", "wps": "23158.4", "ups": "3.19", "wpb": "7257.7", "bsz": "39.3", "num_updates": "60600", "lr": "6.41344e-06", "gnorm": "202.212", "loss_scale": "4", "train_wall": "58", "gb_free": "19.3", "wall": "18322"}
[2024-09-27 21:40:03,057][train_inner][INFO] - {"epoch": 86, "update": 85.297, "loss": "70.057", "ntokens": "7285.5", "nsentences": "38.52", "nll_loss": "0.37", "wps": "24806.6", "ups": "3.4", "wpb": "7285.5", "bsz": "38.5", "num_updates": "60800", "lr": "6.31809e-06", "gnorm": "208.274", "loss_scale": "4", "train_wall": "58", "gb_free": "19.3", "wall": "18380"}
[2024-09-27 21:41:02,538][train_inner][INFO] - {"epoch": 86, "update": 85.578, "loss": "64.485", "ntokens": "7378.65", "nsentences": "41.24", "nll_loss": "0.36", "wps": "24810.3", "ups": "3.36", "wpb": "7378.6", "bsz": "41.2", "num_updates": "61000", "lr": "6.22416e-06", "gnorm": "197.712", "loss_scale": "4", "train_wall": "59", "gb_free": "18.5", "wall": "18440"}
[2024-09-27 21:42:02,182][train_inner][INFO] - {"epoch": 86, "update": 85.858, "loss": "68.062", "ntokens": "7396.81", "nsentences": "39.12", "nll_loss": "0.36", "wps": "24803.3", "ups": "3.35", "wpb": "7396.8", "bsz": "39.1", "num_updates": "61200", "lr": "6.13162e-06", "gnorm": "202.804", "loss_scale": "4", "train_wall": "59", "gb_free": "19", "wall": "18500"}
[2024-09-27 21:42:32,222][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:42:32,223][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:42:32,258][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 87
[2024-09-27 21:42:34,886][valid][INFO] - {"epoch": 86, "valid_loss": "14.241", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.074", "valid_uer": "1.244", "valid_wer": "4.373", "valid_raw_wer": "4.373", "valid_wps": "22140.1", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "61301", "valid_best_wer": "4.373"}
[2024-09-27 21:42:34,887][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 86 @ 61301 updates
[2024-09-27 21:42:34,887][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 21:42:35,961][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 21:42:36,539][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 86 @ 61301 updates, score 4.373) (writing took 1.6526778920087963 seconds)
[2024-09-27 21:42:36,540][fairseq_cli.train][INFO] - end of epoch 86 (average epoch stats below)
[2024-09-27 21:42:36,540][train][INFO] - {"epoch": 86, "train_loss": "67.522", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.364", "train_wps": "24282.8", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "61301", "train_lr": "6.08542e-06", "train_gnorm": "202.675", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "18.3", "train_wall": "18534"}
[2024-09-27 21:42:36,541][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:42:36,582][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 87
[2024-09-27 21:42:36,703][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:42:36,704][fairseq.trainer][INFO] - begin training epoch 87
[2024-09-27 21:42:36,705][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:43:06,246][train_inner][INFO] - {"epoch": 87, "update": 86.139, "loss": "66.72", "ntokens": "7412.76", "nsentences": "40.48", "nll_loss": "0.364", "wps": "23141.9", "ups": "3.12", "wpb": "7412.8", "bsz": "40.5", "num_updates": "61400", "lr": "6.04046e-06", "gnorm": "204.272", "loss_scale": "4", "train_wall": "59", "gb_free": "17.8", "wall": "18564"}
[2024-09-27 21:44:04,894][train_inner][INFO] - {"epoch": 87, "update": 86.419, "loss": "67.722", "ntokens": "7257.75", "nsentences": "38.52", "nll_loss": "0.359", "wps": "24750.4", "ups": "3.41", "wpb": "7257.8", "bsz": "38.5", "num_updates": "61600", "lr": "5.95066e-06", "gnorm": "206.175", "loss_scale": "4", "train_wall": "58", "gb_free": "18.3", "wall": "18622"}
[2024-09-27 21:45:04,539][train_inner][INFO] - {"epoch": 87, "update": 86.7, "loss": "63.649", "ntokens": "7396.98", "nsentences": "40.6", "nll_loss": "0.349", "wps": "24803.2", "ups": "3.35", "wpb": "7397", "bsz": "40.6", "num_updates": "61800", "lr": "5.86219e-06", "gnorm": "201.011", "loss_scale": "4", "train_wall": "59", "gb_free": "17.9", "wall": "18682"}
[2024-09-27 21:46:04,002][train_inner][INFO] - {"epoch": 87, "update": 86.98, "loss": "69.448", "ntokens": "7357.36", "nsentences": "39.04", "nll_loss": "0.369", "wps": "24746.2", "ups": "3.36", "wpb": "7357.4", "bsz": "39", "num_updates": "62000", "lr": "5.77504e-06", "gnorm": "206.907", "loss_scale": "4", "train_wall": "59", "gb_free": "19", "wall": "18741"}
[2024-09-27 21:46:08,050][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:46:08,050][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:46:08,086][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 88
[2024-09-27 21:46:10,719][valid][INFO] - {"epoch": 87, "valid_loss": "14.37", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.252", "valid_wer": "4.336", "valid_raw_wer": "4.336", "valid_wps": "22082.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "62014", "valid_best_wer": "4.336"}
[2024-09-27 21:46:10,720][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 87 @ 62014 updates
[2024-09-27 21:46:10,720][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 21:46:11,808][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 21:46:12,392][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 87 @ 62014 updates, score 4.336) (writing took 1.6725361389108002 seconds)
[2024-09-27 21:46:12,392][fairseq_cli.train][INFO] - end of epoch 87 (average epoch stats below)
[2024-09-27 21:46:12,393][train][INFO] - {"epoch": 87, "train_loss": "66.881", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.36", "train_wps": "24283.8", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "62014", "train_lr": "5.76898e-06", "train_gnorm": "205.392", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "18.5", "train_wall": "18750"}
[2024-09-27 21:46:12,393][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:46:12,434][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 88
[2024-09-27 21:46:12,554][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:46:12,556][fairseq.trainer][INFO] - begin training epoch 88
[2024-09-27 21:46:12,556][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:47:08,012][train_inner][INFO] - {"epoch": 88, "update": 87.261, "loss": "68.19", "ntokens": "7374.53", "nsentences": "39.16", "nll_loss": "0.362", "wps": "23041.8", "ups": "3.12", "wpb": "7374.5", "bsz": "39.2", "num_updates": "62200", "lr": "5.68918e-06", "gnorm": "204.56", "loss_scale": "4", "train_wall": "59", "gb_free": "18.8", "wall": "18805"}
[2024-09-27 21:48:07,270][train_inner][INFO] - {"epoch": 88, "update": 87.541, "loss": "66.91", "ntokens": "7372.35", "nsentences": "40.28", "nll_loss": "0.366", "wps": "24882", "ups": "3.38", "wpb": "7372.4", "bsz": "40.3", "num_updates": "62400", "lr": "5.6046e-06", "gnorm": "200.323", "loss_scale": "4", "train_wall": "59", "gb_free": "18.5", "wall": "18865"}
[2024-09-27 21:49:06,606][train_inner][INFO] - {"epoch": 88, "update": 87.822, "loss": "67.106", "ntokens": "7371.81", "nsentences": "39.48", "nll_loss": "0.359", "wps": "24848.1", "ups": "3.37", "wpb": "7371.8", "bsz": "39.5", "num_updates": "62600", "lr": "5.52127e-06", "gnorm": "207.707", "loss_scale": "4", "train_wall": "59", "gb_free": "19.1", "wall": "18924"}
[2024-09-27 21:49:43,843][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:49:43,843][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:49:43,877][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 89
[2024-09-27 21:49:46,505][valid][INFO] - {"epoch": 88, "valid_loss": "14.249", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.267", "valid_wer": "4.373", "valid_raw_wer": "4.373", "valid_wps": "22108.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "62727", "valid_best_wer": "4.336"}
[2024-09-27 21:49:46,505][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 88 @ 62727 updates
[2024-09-27 21:49:46,506][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:49:47,575][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:49:47,586][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 88 @ 62727 updates, score 4.373) (writing took 1.0803986550308764 seconds)
[2024-09-27 21:49:47,586][fairseq_cli.train][INFO] - end of epoch 88 (average epoch stats below)
[2024-09-27 21:49:47,586][train][INFO] - {"epoch": 88, "train_loss": "67.255", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.362", "train_wps": "24358.1", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "62727", "train_lr": "5.46901e-06", "train_gnorm": "204.577", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "19.1", "train_wall": "18965"}
[2024-09-27 21:49:47,587][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:49:47,628][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 89
[2024-09-27 21:49:47,747][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:49:47,749][fairseq.trainer][INFO] - begin training epoch 89
[2024-09-27 21:49:47,749][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:50:09,727][train_inner][INFO] - {"epoch": 89, "update": 88.102, "loss": "66.115", "ntokens": "7300.1", "nsentences": "39.4", "nll_loss": "0.357", "wps": "23130.3", "ups": "3.17", "wpb": "7300.1", "bsz": "39.4", "num_updates": "62800", "lr": "5.43919e-06", "gnorm": "205.516", "loss_scale": "4", "train_wall": "59", "gb_free": "19", "wall": "18987"}
[2024-09-27 21:51:08,776][train_inner][INFO] - {"epoch": 89, "update": 88.383, "loss": "66.701", "ntokens": "7302.97", "nsentences": "39.6", "nll_loss": "0.362", "wps": "24735.7", "ups": "3.39", "wpb": "7303", "bsz": "39.6", "num_updates": "63000", "lr": "5.35832e-06", "gnorm": "207.461", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "19046"}
[2024-09-27 21:52:07,946][train_inner][INFO] - {"epoch": 89, "update": 88.663, "loss": "69.446", "ntokens": "7324.52", "nsentences": "38.76", "nll_loss": "0.367", "wps": "24757.6", "ups": "3.38", "wpb": "7324.5", "bsz": "38.8", "num_updates": "63200", "lr": "5.27866e-06", "gnorm": "200.436", "loss_scale": "8", "train_wall": "59", "gb_free": "19", "wall": "19105"}
[2024-09-27 21:53:07,447][train_inner][INFO] - {"epoch": 89, "update": 88.944, "loss": "65.094", "ntokens": "7398.52", "nsentences": "40.4", "nll_loss": "0.355", "wps": "24868.6", "ups": "3.36", "wpb": "7398.5", "bsz": "40.4", "num_updates": "63400", "lr": "5.20018e-06", "gnorm": "202.318", "loss_scale": "8", "train_wall": "59", "gb_free": "18.1", "wall": "19165"}
[2024-09-27 21:53:19,323][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:53:19,323][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:53:19,357][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 90
[2024-09-27 21:53:21,977][valid][INFO] - {"epoch": 89, "valid_loss": "14.758", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.077", "valid_uer": "1.259", "valid_wer": "4.326", "valid_raw_wer": "4.326", "valid_wps": "22123.5", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "63440", "valid_best_wer": "4.326"}
[2024-09-27 21:53:21,978][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 89 @ 63440 updates
[2024-09-27 21:53:21,978][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 21:53:23,046][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 21:53:23,631][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 89 @ 63440 updates, score 4.326) (writing took 1.6531856479123235 seconds)
[2024-09-27 21:53:23,631][fairseq_cli.train][INFO] - end of epoch 89 (average epoch stats below)
[2024-09-27 21:53:23,631][train][INFO] - {"epoch": 89, "train_loss": "66.64", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.359", "train_wps": "24262.2", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "63440", "train_lr": "5.18463e-06", "train_gnorm": "202.942", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "18.9", "train_wall": "19181"}
[2024-09-27 21:53:23,632][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:53:23,673][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 90
[2024-09-27 21:53:23,794][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:53:23,796][fairseq.trainer][INFO] - begin training epoch 90
[2024-09-27 21:53:23,796][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:54:11,629][train_inner][INFO] - {"epoch": 90, "update": 89.224, "loss": "68.051", "ntokens": "7387.55", "nsentences": "38.8", "nll_loss": "0.357", "wps": "23020.4", "ups": "3.12", "wpb": "7387.6", "bsz": "38.8", "num_updates": "63600", "lr": "5.12287e-06", "gnorm": "200.679", "loss_scale": "8", "train_wall": "59", "gb_free": "19.5", "wall": "19229"}
[2024-09-27 21:55:10,902][train_inner][INFO] - {"epoch": 90, "update": 89.505, "loss": "65.552", "ntokens": "7350.97", "nsentences": "40.68", "nll_loss": "0.363", "wps": "24804.1", "ups": "3.37", "wpb": "7351", "bsz": "40.7", "num_updates": "63800", "lr": "5.04671e-06", "gnorm": "201.605", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "19288"}
[2024-09-27 21:56:09,806][train_inner][INFO] - {"epoch": 90, "update": 89.785, "loss": "67.479", "ntokens": "7322.02", "nsentences": "39.32", "nll_loss": "0.362", "wps": "24860.8", "ups": "3.4", "wpb": "7322", "bsz": "39.3", "num_updates": "64000", "lr": "4.97168e-06", "gnorm": "204.268", "loss_scale": "8", "train_wall": "59", "gb_free": "18.6", "wall": "19347"}
[2024-09-27 21:56:55,274][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 21:56:55,274][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:56:55,309][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 91
[2024-09-27 21:56:57,975][valid][INFO] - {"epoch": 90, "valid_loss": "14.448", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.076", "valid_uer": "1.252", "valid_wer": "4.43", "valid_raw_wer": "4.43", "valid_wps": "22012.7", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "64153", "valid_best_wer": "4.326"}
[2024-09-27 21:56:57,975][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 90 @ 64153 updates
[2024-09-27 21:56:57,976][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:56:59,092][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 21:56:59,103][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 90 @ 64153 updates, score 4.43) (writing took 1.1276843959931284 seconds)
[2024-09-27 21:56:59,103][fairseq_cli.train][INFO] - end of epoch 90 (average epoch stats below)
[2024-09-27 21:56:59,104][train][INFO] - {"epoch": 90, "train_loss": "66.731", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.36", "train_wps": "24326.6", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "64153", "train_lr": "4.91504e-06", "train_gnorm": "202.58", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "19397"}
[2024-09-27 21:56:59,104][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 21:56:59,145][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 91
[2024-09-27 21:56:59,265][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 21:56:59,267][fairseq.trainer][INFO] - begin training epoch 91
[2024-09-27 21:56:59,267][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 21:57:13,326][train_inner][INFO] - {"epoch": 91, "update": 90.066, "loss": "65.83", "ntokens": "7366.01", "nsentences": "39.44", "nll_loss": "0.352", "wps": "23193", "ups": "3.15", "wpb": "7366", "bsz": "39.4", "num_updates": "64200", "lr": "4.89777e-06", "gnorm": "205.548", "loss_scale": "8", "train_wall": "59", "gb_free": "18.7", "wall": "19411"}
[2024-09-27 21:58:12,488][train_inner][INFO] - {"epoch": 91, "update": 90.346, "loss": "67.418", "ntokens": "7351.93", "nsentences": "39.64", "nll_loss": "0.364", "wps": "24853.4", "ups": "3.38", "wpb": "7351.9", "bsz": "39.6", "num_updates": "64400", "lr": "4.82495e-06", "gnorm": "195.589", "loss_scale": "8", "train_wall": "59", "gb_free": "19.5", "wall": "19470"}
[2024-09-27 21:59:11,386][train_inner][INFO] - {"epoch": 91, "update": 90.627, "loss": "66.052", "ntokens": "7301.76", "nsentences": "39.28", "nll_loss": "0.355", "wps": "24794.7", "ups": "3.4", "wpb": "7301.8", "bsz": "39.3", "num_updates": "64600", "lr": "4.75322e-06", "gnorm": "203.939", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "19529"}
[2024-09-27 22:00:11,175][train_inner][INFO] - {"epoch": 91, "update": 90.907, "loss": "65.289", "ntokens": "7411.14", "nsentences": "40.52", "nll_loss": "0.357", "wps": "24791", "ups": "3.35", "wpb": "7411.1", "bsz": "40.5", "num_updates": "64800", "lr": "4.68255e-06", "gnorm": "201.544", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "19589"}
[2024-09-27 22:00:30,647][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:00:30,647][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:00:30,682][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 92
[2024-09-27 22:00:33,316][valid][INFO] - {"epoch": 91, "valid_loss": "14.433", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.242", "valid_wer": "4.392", "valid_raw_wer": "4.392", "valid_wps": "22117", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "64866", "valid_best_wer": "4.326"}
[2024-09-27 22:00:33,316][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 91 @ 64866 updates
[2024-09-27 22:00:33,317][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:00:34,431][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:00:34,451][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 91 @ 64866 updates, score 4.392) (writing took 1.1342767160385847 seconds)
[2024-09-27 22:00:34,451][fairseq_cli.train][INFO] - end of epoch 91 (average epoch stats below)
[2024-09-27 22:00:34,451][train][INFO] - {"epoch": 91, "train_loss": "66.307", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.357", "train_wps": "24340.7", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "64866", "train_lr": "4.65946e-06", "train_gnorm": "201.8", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.1", "train_wall": "19612"}
[2024-09-27 22:00:34,452][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:00:34,493][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 92
[2024-09-27 22:00:34,612][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:00:34,614][fairseq.trainer][INFO] - begin training epoch 92
[2024-09-27 22:00:34,614][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:01:14,591][train_inner][INFO] - {"epoch": 92, "update": 91.188, "loss": "66.724", "ntokens": "7357.77", "nsentences": "38.68", "nll_loss": "0.351", "wps": "23205", "ups": "3.15", "wpb": "7357.8", "bsz": "38.7", "num_updates": "65000", "lr": "4.61294e-06", "gnorm": "207.078", "loss_scale": "8", "train_wall": "59", "gb_free": "18.6", "wall": "19652"}
[2024-09-27 22:02:13,665][train_inner][INFO] - {"epoch": 92, "update": 91.468, "loss": "67.49", "ntokens": "7323.35", "nsentences": "39.32", "nll_loss": "0.362", "wps": "24793.6", "ups": "3.39", "wpb": "7323.4", "bsz": "39.3", "num_updates": "65200", "lr": "4.54436e-06", "gnorm": "201.419", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "19711"}
[2024-09-27 22:03:12,831][train_inner][INFO] - {"epoch": 92, "update": 91.749, "loss": "67.852", "ntokens": "7350.48", "nsentences": "39.08", "nll_loss": "0.361", "wps": "24847.1", "ups": "3.38", "wpb": "7350.5", "bsz": "39.1", "num_updates": "65400", "lr": "4.47679e-06", "gnorm": "207.769", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "19770"}
[2024-09-27 22:04:05,864][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:04:05,864][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:04:05,899][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 93
[2024-09-27 22:04:08,557][valid][INFO] - {"epoch": 92, "valid_loss": "14.317", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.251", "valid_wer": "4.326", "valid_raw_wer": "4.326", "valid_wps": "22135.8", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "65579", "valid_best_wer": "4.326"}
[2024-09-27 22:04:08,557][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 92 @ 65579 updates
[2024-09-27 22:04:08,558][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 22:04:09,635][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 22:04:10,223][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 92 @ 65579 updates, score 4.326) (writing took 1.665669966954738 seconds)
[2024-09-27 22:04:10,223][fairseq_cli.train][INFO] - end of epoch 92 (average epoch stats below)
[2024-09-27 22:04:10,224][train][INFO] - {"epoch": 92, "train_loss": "66.615", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.359", "train_wps": "24292.8", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "65579", "train_lr": "4.41718e-06", "train_gnorm": "204.758", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "17.8", "train_wall": "19828"}
[2024-09-27 22:04:10,225][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:04:10,266][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 93
[2024-09-27 22:04:10,385][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:04:10,387][fairseq.trainer][INFO] - begin training epoch 93
[2024-09-27 22:04:10,387][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:04:16,671][train_inner][INFO] - {"epoch": 93, "update": 92.029, "loss": "64.059", "ntokens": "7355.44", "nsentences": "40.92", "nll_loss": "0.356", "wps": "23043.5", "ups": "3.13", "wpb": "7355.4", "bsz": "40.9", "num_updates": "65600", "lr": "4.41024e-06", "gnorm": "203.357", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "19834"}
[2024-09-27 22:05:15,814][train_inner][INFO] - {"epoch": 93, "update": 92.31, "loss": "69.175", "ntokens": "7316.76", "nsentences": "38.76", "nll_loss": "0.366", "wps": "24742.6", "ups": "3.38", "wpb": "7316.8", "bsz": "38.8", "num_updates": "65800", "lr": "4.34467e-06", "gnorm": "206.959", "loss_scale": "8", "train_wall": "59", "gb_free": "17.9", "wall": "19893"}
[2024-09-27 22:06:15,088][train_inner][INFO] - {"epoch": 93, "update": 92.59, "loss": "66.46", "ntokens": "7342.23", "nsentences": "39.32", "nll_loss": "0.356", "wps": "24774.1", "ups": "3.37", "wpb": "7342.2", "bsz": "39.3", "num_updates": "66000", "lr": "4.28008e-06", "gnorm": "201.424", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "19952"}
[2024-09-27 22:07:14,775][train_inner][INFO] - {"epoch": 93, "update": 92.871, "loss": "66.216", "ntokens": "7369.14", "nsentences": "40.04", "nll_loss": "0.36", "wps": "24692.7", "ups": "3.35", "wpb": "7369.1", "bsz": "40", "num_updates": "66200", "lr": "4.21645e-06", "gnorm": "206.666", "loss_scale": "8", "train_wall": "59", "gb_free": "19.3", "wall": "20012"}
[2024-09-27 22:07:42,290][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:07:42,291][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:07:42,327][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 94
[2024-09-27 22:07:44,979][valid][INFO] - {"epoch": 93, "valid_loss": "14.213", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.074", "valid_uer": "1.252", "valid_wer": "4.364", "valid_raw_wer": "4.364", "valid_wps": "22084.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "66292", "valid_best_wer": "4.326"}
[2024-09-27 22:07:44,979][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 93 @ 66292 updates
[2024-09-27 22:07:44,980][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:07:46,061][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:07:46,072][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 93 @ 66292 updates, score 4.364) (writing took 1.092568821972236 seconds)
[2024-09-27 22:07:46,072][fairseq_cli.train][INFO] - end of epoch 93 (average epoch stats below)
[2024-09-27 22:07:46,072][train][INFO] - {"epoch": 93, "train_loss": "66.418", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.358", "train_wps": "24284.2", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "66292", "train_lr": "4.18749e-06", "train_gnorm": "204.12", "train_loss_scale": "8", "train_train_wall": "211", "train_gb_free": "18.3", "train_wall": "20043"}
[2024-09-27 22:07:46,073][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:07:46,114][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 94
[2024-09-27 22:07:46,233][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:07:46,235][fairseq.trainer][INFO] - begin training epoch 94
[2024-09-27 22:07:46,235][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:08:18,482][train_inner][INFO] - {"epoch": 94, "update": 93.151, "loss": "63.111", "ntokens": "7393.02", "nsentences": "41.2", "nll_loss": "0.352", "wps": "23209.5", "ups": "3.14", "wpb": "7393", "bsz": "41.2", "num_updates": "66400", "lr": "4.15376e-06", "gnorm": "196.799", "loss_scale": "8", "train_wall": "59", "gb_free": "17.9", "wall": "20076"}
[2024-09-27 22:09:17,537][train_inner][INFO] - {"epoch": 94, "update": 93.432, "loss": "69.323", "ntokens": "7320.5", "nsentences": "38.68", "nll_loss": "0.366", "wps": "24792", "ups": "3.39", "wpb": "7320.5", "bsz": "38.7", "num_updates": "66600", "lr": "4.09201e-06", "gnorm": "203.419", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "20135"}
[2024-09-27 22:10:17,166][train_inner][INFO] - {"epoch": 94, "update": 93.712, "loss": "67.239", "ntokens": "7372.35", "nsentences": "39.64", "nll_loss": "0.362", "wps": "24727.6", "ups": "3.35", "wpb": "7372.3", "bsz": "39.6", "num_updates": "66800", "lr": "4.03117e-06", "gnorm": "210.128", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "20195"}
[2024-09-27 22:11:16,523][train_inner][INFO] - {"epoch": 94, "update": 93.993, "loss": "68.025", "ntokens": "7358.91", "nsentences": "39.28", "nll_loss": "0.363", "wps": "24795.2", "ups": "3.37", "wpb": "7358.9", "bsz": "39.3", "num_updates": "67000", "lr": "3.97124e-06", "gnorm": "207.96", "loss_scale": "16", "train_wall": "59", "gb_free": "19.1", "wall": "20254"}
[2024-09-27 22:11:17,926][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:11:17,927][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:11:17,961][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 95
[2024-09-27 22:11:20,591][valid][INFO] - {"epoch": 94, "valid_loss": "14.23", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.074", "valid_uer": "1.261", "valid_wer": "4.373", "valid_raw_wer": "4.373", "valid_wps": "22134.6", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "67005", "valid_best_wer": "4.326"}
[2024-09-27 22:11:20,591][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 94 @ 67005 updates
[2024-09-27 22:11:20,592][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:11:21,704][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:11:21,722][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 94 @ 67005 updates, score 4.373) (writing took 1.1303895572200418 seconds)
[2024-09-27 22:11:21,722][fairseq_cli.train][INFO] - end of epoch 94 (average epoch stats below)
[2024-09-27 22:11:21,722][train][INFO] - {"epoch": 94, "train_loss": "67.316", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.363", "train_wps": "24306.6", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "67005", "train_lr": "3.96975e-06", "train_gnorm": "204.924", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "18.6", "train_wall": "20259"}
[2024-09-27 22:11:21,723][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:11:21,764][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 95
[2024-09-27 22:11:21,884][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:11:21,886][fairseq.trainer][INFO] - begin training epoch 95
[2024-09-27 22:11:21,886][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:12:19,710][train_inner][INFO] - {"epoch": 95, "update": 94.273, "loss": "64.941", "ntokens": "7340.37", "nsentences": "40.44", "nll_loss": "0.358", "wps": "23233.9", "ups": "3.17", "wpb": "7340.4", "bsz": "40.4", "num_updates": "67200", "lr": "3.9122e-06", "gnorm": "197.147", "loss_scale": "16", "train_wall": "59", "gb_free": "19.1", "wall": "20317"}
[2024-09-27 22:13:19,413][train_inner][INFO] - {"epoch": 95, "update": 94.554, "loss": "63.771", "ntokens": "7368.21", "nsentences": "40.44", "nll_loss": "0.35", "wps": "24683.1", "ups": "3.35", "wpb": "7368.2", "bsz": "40.4", "num_updates": "67400", "lr": "3.85404e-06", "gnorm": "196.308", "loss_scale": "16", "train_wall": "59", "gb_free": "19.3", "wall": "20377"}
[2024-09-27 22:14:18,714][train_inner][INFO] - {"epoch": 95, "update": 94.835, "loss": "67.092", "ntokens": "7369.16", "nsentences": "39.48", "nll_loss": "0.359", "wps": "24853.5", "ups": "3.37", "wpb": "7369.2", "bsz": "39.5", "num_updates": "67600", "lr": "3.79674e-06", "gnorm": "203.188", "loss_scale": "16", "train_wall": "59", "gb_free": "19.3", "wall": "20436"}
[2024-09-27 22:14:53,478][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:14:53,478][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:14:53,512][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 96
[2024-09-27 22:14:56,146][valid][INFO] - {"epoch": 95, "valid_loss": "14.24", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.074", "valid_uer": "1.284", "valid_wer": "4.458", "valid_raw_wer": "4.458", "valid_wps": "22150.1", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "67718", "valid_best_wer": "4.326"}
[2024-09-27 22:14:56,146][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 95 @ 67718 updates
[2024-09-27 22:14:56,147][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:14:57,215][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:14:57,232][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 95 @ 67718 updates, score 4.458) (writing took 1.0858754150103778 seconds)
[2024-09-27 22:14:57,233][fairseq_cli.train][INFO] - end of epoch 95 (average epoch stats below)
[2024-09-27 22:14:57,233][train][INFO] - {"epoch": 95, "train_loss": "66.322", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.357", "train_wps": "24322.3", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "67718", "train_lr": "3.76333e-06", "train_gnorm": "202.322", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "19.4", "train_wall": "20475"}
[2024-09-27 22:14:57,234][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:14:57,275][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 96
[2024-09-27 22:14:57,396][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:14:57,398][fairseq.trainer][INFO] - begin training epoch 96
[2024-09-27 22:14:57,398][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:15:00,121][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-09-27 22:15:22,152][train_inner][INFO] - {"epoch": 96, "update": 95.116, "loss": "68.023", "ntokens": "7329.4", "nsentences": "38.64", "nll_loss": "0.359", "wps": "23107.4", "ups": "3.15", "wpb": "7329.4", "bsz": "38.6", "num_updates": "67800", "lr": "3.74029e-06", "gnorm": "209.596", "loss_scale": "8", "train_wall": "59", "gb_free": "18.1", "wall": "20500"}
[2024-09-27 22:16:21,922][train_inner][INFO] - {"epoch": 96, "update": 95.397, "loss": "66.775", "ntokens": "7409.81", "nsentences": "40.4", "nll_loss": "0.364", "wps": "24794.5", "ups": "3.35", "wpb": "7409.8", "bsz": "40.4", "num_updates": "68000", "lr": "3.68468e-06", "gnorm": "206.317", "loss_scale": "8", "train_wall": "59", "gb_free": "18", "wall": "20559"}
[2024-09-27 22:17:20,685][train_inner][INFO] - {"epoch": 96, "update": 95.677, "loss": "68.327", "ntokens": "7275.26", "nsentences": "38.28", "nll_loss": "0.36", "wps": "24761.1", "ups": "3.4", "wpb": "7275.3", "bsz": "38.3", "num_updates": "68200", "lr": "3.6299e-06", "gnorm": "204.27", "loss_scale": "8", "train_wall": "58", "gb_free": "18.9", "wall": "20618"}
[2024-09-27 22:18:19,887][train_inner][INFO] - {"epoch": 96, "update": 95.958, "loss": "67.305", "ntokens": "7354.44", "nsentences": "39.96", "nll_loss": "0.366", "wps": "24845.4", "ups": "3.38", "wpb": "7354.4", "bsz": "40", "num_updates": "68400", "lr": "3.57594e-06", "gnorm": "206.844", "loss_scale": "8", "train_wall": "59", "gb_free": "18.1", "wall": "20677"}
[2024-09-27 22:18:28,767][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:18:28,767][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:18:28,801][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 97
[2024-09-27 22:18:31,454][valid][INFO] - {"epoch": 96, "valid_loss": "14.328", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.265", "valid_wer": "4.401", "valid_raw_wer": "4.401", "valid_wps": "22123.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "68430", "valid_best_wer": "4.326"}
[2024-09-27 22:18:31,454][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 96 @ 68430 updates
[2024-09-27 22:18:31,455][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:18:32,496][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:18:32,514][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 96 @ 68430 updates, score 4.401) (writing took 1.0592734750825912 seconds)
[2024-09-27 22:18:32,514][fairseq_cli.train][INFO] - end of epoch 96 (average epoch stats below)
[2024-09-27 22:18:32,514][train][INFO] - {"epoch": 96, "train_loss": "67.277", "train_ntokens": "7351.02", "train_nsentences": "39.6067", "train_nll_loss": "0.362", "train_wps": "24312.1", "train_ups": "3.31", "train_wpb": "7351", "train_bsz": "39.6", "train_num_updates": "68430", "train_lr": "3.56791e-06", "train_gnorm": "205.3", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "17.4", "train_wall": "20690"}
[2024-09-27 22:18:32,515][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:18:32,556][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 97
[2024-09-27 22:18:32,677][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:18:32,679][fairseq.trainer][INFO] - begin training epoch 97
[2024-09-27 22:18:32,679][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:19:23,446][train_inner][INFO] - {"epoch": 97, "update": 96.238, "loss": "65.749", "ntokens": "7379.06", "nsentences": "39.8", "nll_loss": "0.355", "wps": "23219.5", "ups": "3.15", "wpb": "7379.1", "bsz": "39.8", "num_updates": "68600", "lr": "3.52277e-06", "gnorm": "208.21", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "20741"}
[2024-09-27 22:20:22,556][train_inner][INFO] - {"epoch": 97, "update": 96.519, "loss": "66.426", "ntokens": "7311.81", "nsentences": "39.48", "nll_loss": "0.359", "wps": "24740", "ups": "3.38", "wpb": "7311.8", "bsz": "39.5", "num_updates": "68800", "lr": "3.4704e-06", "gnorm": "202.024", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "20800"}
[2024-09-27 22:21:21,878][train_inner][INFO] - {"epoch": 97, "update": 96.799, "loss": "67.152", "ntokens": "7332.23", "nsentences": "38.84", "nll_loss": "0.356", "wps": "24720", "ups": "3.37", "wpb": "7332.2", "bsz": "38.8", "num_updates": "69000", "lr": "3.41881e-06", "gnorm": "207.356", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "20859"}
[2024-09-27 22:21:42,052][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
[2024-09-27 22:22:04,481][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:22:04,481][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:22:04,518][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 98
[2024-09-27 22:22:07,162][valid][INFO] - {"epoch": 97, "valid_loss": "14.424", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.259", "valid_wer": "4.383", "valid_raw_wer": "4.383", "valid_wps": "22110", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "69142", "valid_best_wer": "4.326"}
[2024-09-27 22:22:07,163][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 97 @ 69142 updates
[2024-09-27 22:22:07,163][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:22:08,318][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:22:08,334][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 97 @ 69142 updates, score 4.383) (writing took 1.1719125960953534 seconds)
[2024-09-27 22:22:08,335][fairseq_cli.train][INFO] - end of epoch 97 (average epoch stats below)
[2024-09-27 22:22:08,335][train][INFO] - {"epoch": 97, "train_loss": "65.827", "train_ntokens": "7350.63", "train_nsentences": "39.6067", "train_nll_loss": "0.355", "train_wps": "24250", "train_ups": "3.3", "train_wpb": "7350.6", "train_bsz": "39.6", "train_num_updates": "69142", "train_lr": "3.38264e-06", "train_gnorm": "204.42", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "19.1", "train_wall": "20906"}
[2024-09-27 22:22:08,336][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:22:08,377][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 98
[2024-09-27 22:22:08,497][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:22:08,499][fairseq.trainer][INFO] - begin training epoch 98
[2024-09-27 22:22:08,499][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:22:25,822][train_inner][INFO] - {"epoch": 98, "update": 97.081, "loss": "66.955", "ntokens": "7389.19", "nsentences": "40.04", "nll_loss": "0.363", "wps": "23111.5", "ups": "3.13", "wpb": "7389.2", "bsz": "40", "num_updates": "69200", "lr": "3.36798e-06", "gnorm": "203.262", "loss_scale": "4", "train_wall": "60", "gb_free": "18.4", "wall": "20923"}
[2024-09-27 22:23:24,749][train_inner][INFO] - {"epoch": 98, "update": 97.362, "loss": "69.929", "ntokens": "7339.62", "nsentences": "38.72", "nll_loss": "0.369", "wps": "24911.1", "ups": "3.39", "wpb": "7339.6", "bsz": "38.7", "num_updates": "69400", "lr": "3.31791e-06", "gnorm": "203.706", "loss_scale": "4", "train_wall": "59", "gb_free": "19", "wall": "20982"}
[2024-09-27 22:24:24,098][train_inner][INFO] - {"epoch": 98, "update": 97.642, "loss": "65.756", "ntokens": "7392.82", "nsentences": "40.52", "nll_loss": "0.36", "wps": "24913.1", "ups": "3.37", "wpb": "7392.8", "bsz": "40.5", "num_updates": "69600", "lr": "3.26858e-06", "gnorm": "198.444", "loss_scale": "4", "train_wall": "59", "gb_free": "18", "wall": "21041"}
[2024-09-27 22:25:23,435][train_inner][INFO] - {"epoch": 98, "update": 97.923, "loss": "67.024", "ntokens": "7328.31", "nsentences": "39.56", "nll_loss": "0.362", "wps": "24700.7", "ups": "3.37", "wpb": "7328.3", "bsz": "39.6", "num_updates": "69800", "lr": "3.21999e-06", "gnorm": "207.753", "loss_scale": "4", "train_wall": "59", "gb_free": "18.8", "wall": "21101"}
[2024-09-27 22:25:39,679][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:25:39,679][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:25:39,714][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 99
[2024-09-27 22:25:42,338][valid][INFO] - {"epoch": 98, "valid_loss": "14.466", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.076", "valid_uer": "1.265", "valid_wer": "4.401", "valid_raw_wer": "4.401", "valid_wps": "22167.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "69855", "valid_best_wer": "4.326"}
[2024-09-27 22:25:42,338][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 98 @ 69855 updates
[2024-09-27 22:25:42,346][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:25:43,416][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:25:43,433][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 98 @ 69855 updates, score 4.401) (writing took 1.0946860031690449 seconds)
[2024-09-27 22:25:43,433][fairseq_cli.train][INFO] - end of epoch 98 (average epoch stats below)
[2024-09-27 22:25:43,433][train][INFO] - {"epoch": 98, "train_loss": "67.536", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.364", "train_wps": "24368.9", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "69855", "train_lr": "3.20675e-06", "train_gnorm": "204.41", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "19.2", "train_wall": "21121"}
[2024-09-27 22:25:43,434][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:25:43,475][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 99
[2024-09-27 22:25:43,596][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:25:43,598][fairseq.trainer][INFO] - begin training epoch 99
[2024-09-27 22:25:43,598][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:26:26,759][train_inner][INFO] - {"epoch": 99, "update": 98.203, "loss": "65.476", "ntokens": "7305.12", "nsentences": "39.2", "nll_loss": "0.351", "wps": "23072.5", "ups": "3.16", "wpb": "7305.1", "bsz": "39.2", "num_updates": "70000", "lr": "3.17211e-06", "gnorm": "208.846", "loss_scale": "4", "train_wall": "59", "gb_free": "19", "wall": "21164"}
[2024-09-27 22:27:26,256][train_inner][INFO] - {"epoch": 99, "update": 98.484, "loss": "67.625", "ntokens": "7382.15", "nsentences": "39.36", "nll_loss": "0.361", "wps": "24815", "ups": "3.36", "wpb": "7382.1", "bsz": "39.4", "num_updates": "70200", "lr": "3.12495e-06", "gnorm": "205.785", "loss_scale": "4", "train_wall": "59", "gb_free": "19", "wall": "21224"}
[2024-09-27 22:28:25,291][train_inner][INFO] - {"epoch": 99, "update": 98.764, "loss": "66.175", "ntokens": "7368.73", "nsentences": "40.44", "nll_loss": "0.363", "wps": "24964.2", "ups": "3.39", "wpb": "7368.7", "bsz": "40.4", "num_updates": "70400", "lr": "3.0785e-06", "gnorm": "205.303", "loss_scale": "4", "train_wall": "59", "gb_free": "19.1", "wall": "21283"}
[2024-09-27 22:29:14,866][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:29:14,867][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:29:14,901][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 100
[2024-09-27 22:29:17,526][valid][INFO] - {"epoch": 99, "valid_loss": "14.443", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.076", "valid_uer": "1.274", "valid_wer": "4.477", "valid_raw_wer": "4.477", "valid_wps": "22119", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "70568", "valid_best_wer": "4.326"}
[2024-09-27 22:29:17,526][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 99 @ 70568 updates
[2024-09-27 22:29:17,527][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:29:18,628][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:29:18,645][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 99 @ 70568 updates, score 4.477) (writing took 1.1185581560712308 seconds)
[2024-09-27 22:29:18,645][fairseq_cli.train][INFO] - end of epoch 99 (average epoch stats below)
[2024-09-27 22:29:18,645][train][INFO] - {"epoch": 99, "train_loss": "66.54", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.358", "train_wps": "24356.1", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "70568", "train_lr": "3.04e-06", "train_gnorm": "204.661", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "17.8", "train_wall": "21336"}
[2024-09-27 22:29:18,646][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:29:18,687][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 100
[2024-09-27 22:29:18,810][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:29:18,812][fairseq.trainer][INFO] - begin training epoch 100
[2024-09-27 22:29:18,812][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:29:28,282][train_inner][INFO] - {"epoch": 100, "update": 99.045, "loss": "65.958", "ntokens": "7341.6", "nsentences": "40", "nll_loss": "0.359", "wps": "23310.1", "ups": "3.18", "wpb": "7341.6", "bsz": "40", "num_updates": "70600", "lr": "3.03273e-06", "gnorm": "199.601", "loss_scale": "4", "train_wall": "59", "gb_free": "18.9", "wall": "21346"}
[2024-09-27 22:30:27,398][train_inner][INFO] - {"epoch": 100, "update": 99.325, "loss": "68.169", "ntokens": "7349.66", "nsentences": "39", "nll_loss": "0.362", "wps": "24865.1", "ups": "3.38", "wpb": "7349.7", "bsz": "39", "num_updates": "70800", "lr": "2.98764e-06", "gnorm": "201.265", "loss_scale": "4", "train_wall": "59", "gb_free": "19.2", "wall": "21405"}
[2024-09-27 22:31:26,595][train_inner][INFO] - {"epoch": 100, "update": 99.606, "loss": "67.504", "ntokens": "7334.39", "nsentences": "38.8", "nll_loss": "0.357", "wps": "24779.8", "ups": "3.38", "wpb": "7334.4", "bsz": "38.8", "num_updates": "71000", "lr": "2.94322e-06", "gnorm": "205.915", "loss_scale": "4", "train_wall": "59", "gb_free": "19", "wall": "21464"}
[2024-09-27 22:32:26,243][train_inner][INFO] - {"epoch": 100, "update": 99.886, "loss": "64.019", "ntokens": "7398.56", "nsentences": "40.76", "nll_loss": "0.353", "wps": "24807.4", "ups": "3.35", "wpb": "7398.6", "bsz": "40.8", "num_updates": "71200", "lr": "2.89946e-06", "gnorm": "201.994", "loss_scale": "4", "train_wall": "59", "gb_free": "18", "wall": "21524"}
[2024-09-27 22:32:50,078][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:32:50,079][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:32:50,115][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 101
[2024-09-27 22:32:52,743][valid][INFO] - {"epoch": 100, "valid_loss": "14.455", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.076", "valid_uer": "1.284", "valid_wer": "4.524", "valid_raw_wer": "4.524", "valid_wps": "22064.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "71281", "valid_best_wer": "4.326"}
[2024-09-27 22:32:52,743][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 100 @ 71281 updates
[2024-09-27 22:32:52,751][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:32:53,859][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:32:53,876][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 100 @ 71281 updates, score 4.524) (writing took 1.1323222229257226 seconds)
[2024-09-27 22:32:53,876][fairseq_cli.train][INFO] - end of epoch 100 (average epoch stats below)
[2024-09-27 22:32:53,876][train][INFO] - {"epoch": 100, "train_loss": "66.26", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.357", "train_wps": "24353.9", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "71281", "train_lr": "2.88193e-06", "train_gnorm": "203.426", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "19.1", "train_wall": "21551"}
[2024-09-27 22:32:53,877][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:32:53,918][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 101
[2024-09-27 22:32:54,037][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:32:54,039][fairseq.trainer][INFO] - begin training epoch 101
[2024-09-27 22:32:54,039][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:33:29,556][train_inner][INFO] - {"epoch": 101, "update": 100.167, "loss": "63.125", "ntokens": "7325.16", "nsentences": "40.04", "nll_loss": "0.345", "wps": "23139.7", "ups": "3.16", "wpb": "7325.2", "bsz": "40", "num_updates": "71400", "lr": "2.85636e-06", "gnorm": "209.481", "loss_scale": "4", "train_wall": "59", "gb_free": "17.7", "wall": "21587"}
[2024-09-27 22:34:29,440][train_inner][INFO] - {"epoch": 101, "update": 100.447, "loss": "64.736", "ntokens": "7409.27", "nsentences": "40.12", "nll_loss": "0.351", "wps": "24745.6", "ups": "3.34", "wpb": "7409.3", "bsz": "40.1", "num_updates": "71600", "lr": "2.81389e-06", "gnorm": "205.12", "loss_scale": "4", "train_wall": "60", "gb_free": "19.5", "wall": "21647"}
[2024-09-27 22:35:28,623][train_inner][INFO] - {"epoch": 101, "update": 100.728, "loss": "65.406", "ntokens": "7303.33", "nsentences": "38.6", "nll_loss": "0.346", "wps": "24680.5", "ups": "3.38", "wpb": "7303.3", "bsz": "38.6", "num_updates": "71800", "lr": "2.77206e-06", "gnorm": "211.758", "loss_scale": "4", "train_wall": "59", "gb_free": "18.5", "wall": "21706"}
[2024-09-27 22:36:25,842][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:36:25,843][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:36:25,879][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 102
[2024-09-27 22:36:28,523][valid][INFO] - {"epoch": 101, "valid_loss": "14.32", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.268", "valid_wer": "4.439", "valid_raw_wer": "4.439", "valid_wps": "22181.2", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "71994", "valid_best_wer": "4.326"}
[2024-09-27 22:36:28,524][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 101 @ 71994 updates
[2024-09-27 22:36:28,524][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:36:29,547][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:36:29,563][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 101 @ 71994 updates, score 4.439) (writing took 1.0393956080079079 seconds)
[2024-09-27 22:36:29,564][fairseq_cli.train][INFO] - end of epoch 101 (average epoch stats below)
[2024-09-27 22:36:29,564][train][INFO] - {"epoch": 101, "train_loss": "65.315", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.352", "train_wps": "24302.3", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "71994", "train_lr": "2.73207e-06", "train_gnorm": "208.827", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "18.9", "train_wall": "21767"}
[2024-09-27 22:36:29,565][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:36:29,605][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 102
[2024-09-27 22:36:29,725][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:36:29,727][fairseq.trainer][INFO] - begin training epoch 102
[2024-09-27 22:36:29,728][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:36:31,468][train_inner][INFO] - {"epoch": 102, "update": 101.008, "loss": "67.525", "ntokens": "7321.97", "nsentences": "39.32", "nll_loss": "0.363", "wps": "23301.7", "ups": "3.18", "wpb": "7322", "bsz": "39.3", "num_updates": "72000", "lr": "2.73085e-06", "gnorm": "209.71", "loss_scale": "4", "train_wall": "59", "gb_free": "19.2", "wall": "21769"}
[2024-09-27 22:37:31,167][train_inner][INFO] - {"epoch": 102, "update": 101.289, "loss": "66.944", "ntokens": "7424.19", "nsentences": "40", "nll_loss": "0.361", "wps": "24872.1", "ups": "3.35", "wpb": "7424.2", "bsz": "40", "num_updates": "72200", "lr": "2.69025e-06", "gnorm": "208.1", "loss_scale": "4", "train_wall": "59", "gb_free": "19", "wall": "21829"}
[2024-09-27 22:38:30,892][train_inner][INFO] - {"epoch": 102, "update": 101.569, "loss": "65.745", "ntokens": "7392.44", "nsentences": "40.28", "nll_loss": "0.358", "wps": "24754.9", "ups": "3.35", "wpb": "7392.4", "bsz": "40.3", "num_updates": "72400", "lr": "2.65025e-06", "gnorm": "198.727", "loss_scale": "4", "train_wall": "59", "gb_free": "18.1", "wall": "21888"}
[2024-09-27 22:39:29,825][train_inner][INFO] - {"epoch": 102, "update": 101.85, "loss": "64.207", "ntokens": "7294.46", "nsentences": "39.6", "nll_loss": "0.349", "wps": "24755.2", "ups": "3.39", "wpb": "7294.5", "bsz": "39.6", "num_updates": "72600", "lr": "2.61085e-06", "gnorm": "206.543", "loss_scale": "4", "train_wall": "59", "gb_free": "18.1", "wall": "21947"}
[2024-09-27 22:40:01,392][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:40:01,393][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:40:01,427][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 103
[2024-09-27 22:40:04,066][valid][INFO] - {"epoch": 102, "valid_loss": "14.374", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.252", "valid_wer": "4.345", "valid_raw_wer": "4.345", "valid_wps": "22158.7", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "72707", "valid_best_wer": "4.326"}
[2024-09-27 22:40:04,067][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 102 @ 72707 updates
[2024-09-27 22:40:04,067][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:40:05,129][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:40:05,146][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 102 @ 72707 updates, score 4.345) (writing took 1.0792437149211764 seconds)
[2024-09-27 22:40:05,146][fairseq_cli.train][INFO] - end of epoch 102 (average epoch stats below)
[2024-09-27 22:40:05,147][train][INFO] - {"epoch": 102, "train_loss": "65.943", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.355", "train_wps": "24314.2", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "72707", "train_lr": "2.59001e-06", "train_gnorm": "205.604", "train_loss_scale": "4", "train_train_wall": "210", "train_gb_free": "19.3", "train_wall": "21983"}
[2024-09-27 22:40:05,147][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:40:05,188][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 103
[2024-09-27 22:40:05,308][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:40:05,310][fairseq.trainer][INFO] - begin training epoch 103
[2024-09-27 22:40:05,310][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:40:32,678][train_inner][INFO] - {"epoch": 103, "update": 102.13, "loss": "66.853", "ntokens": "7271.81", "nsentences": "38.52", "nll_loss": "0.354", "wps": "23139.3", "ups": "3.18", "wpb": "7271.8", "bsz": "38.5", "num_updates": "72800", "lr": "2.57203e-06", "gnorm": "206.96", "loss_scale": "4", "train_wall": "59", "gb_free": "18.2", "wall": "22010"}
[2024-09-27 22:41:32,264][train_inner][INFO] - {"epoch": 103, "update": 102.411, "loss": "70.179", "ntokens": "7391.15", "nsentences": "38.76", "nll_loss": "0.368", "wps": "24808.1", "ups": "3.36", "wpb": "7391.1", "bsz": "38.8", "num_updates": "73000", "lr": "2.5338e-06", "gnorm": "216.236", "loss_scale": "4", "train_wall": "59", "gb_free": "18.1", "wall": "22070"}
[2024-09-27 22:42:31,415][train_inner][INFO] - {"epoch": 103, "update": 102.691, "loss": "62.583", "ntokens": "7352.98", "nsentences": "41", "nll_loss": "0.349", "wps": "24862.1", "ups": "3.38", "wpb": "7353", "bsz": "41", "num_updates": "73200", "lr": "2.49613e-06", "gnorm": "197.098", "loss_scale": "8", "train_wall": "59", "gb_free": "18.2", "wall": "22129"}
[2024-09-27 22:43:30,697][train_inner][INFO] - {"epoch": 103, "update": 102.972, "loss": "68.066", "ntokens": "7358.1", "nsentences": "39.28", "nll_loss": "0.363", "wps": "24823.8", "ups": "3.37", "wpb": "7358.1", "bsz": "39.3", "num_updates": "73400", "lr": "2.45902e-06", "gnorm": "208.752", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "22188"}
[2024-09-27 22:43:36,567][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:43:36,567][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:43:36,604][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 104
[2024-09-27 22:43:39,244][valid][INFO] - {"epoch": 103, "valid_loss": "14.406", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.258", "valid_wer": "4.392", "valid_raw_wer": "4.392", "valid_wps": "22170.6", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "73420", "valid_best_wer": "4.326"}
[2024-09-27 22:43:39,244][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 103 @ 73420 updates
[2024-09-27 22:43:39,251][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:43:40,354][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:43:40,373][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 103 @ 73420 updates, score 4.392) (writing took 1.1282294851262122 seconds)
[2024-09-27 22:43:40,373][fairseq_cli.train][INFO] - end of epoch 103 (average epoch stats below)
[2024-09-27 22:43:40,373][train][INFO] - {"epoch": 103, "train_loss": "66.665", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.359", "train_wps": "24354.4", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "73420", "train_lr": "2.45533e-06", "train_gnorm": "206.811", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.4", "train_wall": "22198"}
[2024-09-27 22:43:40,374][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:43:40,415][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 104
[2024-09-27 22:43:40,536][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:43:40,537][fairseq.trainer][INFO] - begin training epoch 104
[2024-09-27 22:43:40,538][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:44:34,482][train_inner][INFO] - {"epoch": 104, "update": 103.252, "loss": "64.889", "ntokens": "7393.82", "nsentences": "40.2", "nll_loss": "0.353", "wps": "23183.8", "ups": "3.14", "wpb": "7393.8", "bsz": "40.2", "num_updates": "73600", "lr": "2.42246e-06", "gnorm": "199.729", "loss_scale": "8", "train_wall": "59", "gb_free": "18.9", "wall": "22252"}
[2024-09-27 22:45:33,268][train_inner][INFO] - {"epoch": 104, "update": 103.533, "loss": "68.714", "ntokens": "7332.95", "nsentences": "39.08", "nll_loss": "0.366", "wps": "24948", "ups": "3.4", "wpb": "7332.9", "bsz": "39.1", "num_updates": "73800", "lr": "2.38644e-06", "gnorm": "208.481", "loss_scale": "8", "train_wall": "58", "gb_free": "19.2", "wall": "22311"}
[2024-09-27 22:46:32,572][train_inner][INFO] - {"epoch": 104, "update": 103.813, "loss": "65.287", "ntokens": "7348", "nsentences": "39.32", "nll_loss": "0.349", "wps": "24781", "ups": "3.37", "wpb": "7348", "bsz": "39.3", "num_updates": "74000", "lr": "2.35096e-06", "gnorm": "207.243", "loss_scale": "8", "train_wall": "59", "gb_free": "18.8", "wall": "22370"}
[2024-09-27 22:47:11,712][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:47:11,713][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:47:11,747][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 105
[2024-09-27 22:47:14,390][valid][INFO] - {"epoch": 104, "valid_loss": "14.292", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.259", "valid_wer": "4.401", "valid_raw_wer": "4.401", "valid_wps": "22131", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "74133", "valid_best_wer": "4.326"}
[2024-09-27 22:47:14,391][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 104 @ 74133 updates
[2024-09-27 22:47:14,391][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:47:15,520][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:47:15,539][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 104 @ 74133 updates, score 4.401) (writing took 1.1476912759244442 seconds)
[2024-09-27 22:47:15,539][fairseq_cli.train][INFO] - end of epoch 104 (average epoch stats below)
[2024-09-27 22:47:15,539][train][INFO] - {"epoch": 104, "train_loss": "66.125", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.356", "train_wps": "24361.3", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "74133", "train_lr": "2.32766e-06", "train_gnorm": "205.583", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "17.7", "train_wall": "22413"}
[2024-09-27 22:47:15,540][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:47:15,581][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 105
[2024-09-27 22:47:15,702][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:47:15,704][fairseq.trainer][INFO] - begin training epoch 105
[2024-09-27 22:47:15,704][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:47:35,579][train_inner][INFO] - {"epoch": 105, "update": 104.094, "loss": "67.146", "ntokens": "7307.82", "nsentences": "39.48", "nll_loss": "0.363", "wps": "23196.8", "ups": "3.17", "wpb": "7307.8", "bsz": "39.5", "num_updates": "74200", "lr": "2.31601e-06", "gnorm": "207.813", "loss_scale": "8", "train_wall": "59", "gb_free": "17.5", "wall": "22433"}
[2024-09-27 22:48:34,902][train_inner][INFO] - {"epoch": 105, "update": 104.374, "loss": "67.384", "ntokens": "7359.33", "nsentences": "39.56", "nll_loss": "0.362", "wps": "24811", "ups": "3.37", "wpb": "7359.3", "bsz": "39.6", "num_updates": "74400", "lr": "2.28158e-06", "gnorm": "205.751", "loss_scale": "8", "train_wall": "59", "gb_free": "18.9", "wall": "22492"}
[2024-09-27 22:49:34,486][train_inner][INFO] - {"epoch": 105, "update": 104.655, "loss": "64.317", "ntokens": "7398.7", "nsentences": "40.4", "nll_loss": "0.351", "wps": "24834.6", "ups": "3.36", "wpb": "7398.7", "bsz": "40.4", "num_updates": "74600", "lr": "2.24766e-06", "gnorm": "200.837", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "22552"}
[2024-09-27 22:50:33,595][train_inner][INFO] - {"epoch": 105, "update": 104.935, "loss": "65.459", "ntokens": "7305.62", "nsentences": "39.12", "nll_loss": "0.351", "wps": "24719.4", "ups": "3.38", "wpb": "7305.6", "bsz": "39.1", "num_updates": "74800", "lr": "2.21424e-06", "gnorm": "201.884", "loss_scale": "8", "train_wall": "59", "gb_free": "19.6", "wall": "22611"}
[2024-09-27 22:50:47,205][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:50:47,205][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:50:47,239][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 106
[2024-09-27 22:50:49,883][valid][INFO] - {"epoch": 105, "valid_loss": "14.404", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.245", "valid_wer": "4.317", "valid_raw_wer": "4.317", "valid_wps": "22127", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "74846", "valid_best_wer": "4.317"}
[2024-09-27 22:50:49,884][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 105 @ 74846 updates
[2024-09-27 22:50:49,884][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 22:50:50,976][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 22:50:51,565][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 105 @ 74846 updates, score 4.317) (writing took 1.6811465788632631 seconds)
[2024-09-27 22:50:51,565][fairseq_cli.train][INFO] - end of epoch 105 (average epoch stats below)
[2024-09-27 22:50:51,565][train][INFO] - {"epoch": 105, "train_loss": "66.237", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.357", "train_wps": "24264.3", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "74846", "train_lr": "2.20663e-06", "train_gnorm": "202.952", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.2", "train_wall": "22629"}
[2024-09-27 22:50:51,566][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:50:51,607][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 106
[2024-09-27 22:50:51,727][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:50:51,729][fairseq.trainer][INFO] - begin training epoch 106
[2024-09-27 22:50:51,729][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:51:37,657][train_inner][INFO] - {"epoch": 106, "update": 105.216, "loss": "64.049", "ntokens": "7381.71", "nsentences": "40.52", "nll_loss": "0.352", "wps": "23045.5", "ups": "3.12", "wpb": "7381.7", "bsz": "40.5", "num_updates": "75000", "lr": "2.18132e-06", "gnorm": "197.966", "loss_scale": "8", "train_wall": "59", "gb_free": "19.2", "wall": "22675"}
[2024-09-27 22:52:36,963][train_inner][INFO] - {"epoch": 106, "update": 105.496, "loss": "63.527", "ntokens": "7347.77", "nsentences": "40.12", "nll_loss": "0.347", "wps": "24779.3", "ups": "3.37", "wpb": "7347.8", "bsz": "40.1", "num_updates": "75200", "lr": "2.14889e-06", "gnorm": "201.615", "loss_scale": "8", "train_wall": "59", "gb_free": "19.5", "wall": "22734"}
[2024-09-27 22:53:36,240][train_inner][INFO] - {"epoch": 106, "update": 105.777, "loss": "67.028", "ntokens": "7331.13", "nsentences": "38.36", "nll_loss": "0.351", "wps": "24735.3", "ups": "3.37", "wpb": "7331.1", "bsz": "38.4", "num_updates": "75400", "lr": "2.11695e-06", "gnorm": "214.51", "loss_scale": "8", "train_wall": "59", "gb_free": "17.7", "wall": "22794"}
[2024-09-27 22:54:23,352][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:54:23,353][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:54:23,387][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 107
[2024-09-27 22:54:26,026][valid][INFO] - {"epoch": 106, "valid_loss": "14.282", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.252", "valid_wer": "4.336", "valid_raw_wer": "4.336", "valid_wps": "22088.8", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "75559", "valid_best_wer": "4.317"}
[2024-09-27 22:54:26,027][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 106 @ 75559 updates
[2024-09-27 22:54:26,027][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:54:27,075][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:54:27,086][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 106 @ 75559 updates, score 4.336) (writing took 1.0589540170039982 seconds)
[2024-09-27 22:54:27,086][fairseq_cli.train][INFO] - end of epoch 106 (average epoch stats below)
[2024-09-27 22:54:27,086][train][INFO] - {"epoch": 106, "train_loss": "65.383", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.352", "train_wps": "24321.2", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "75559", "train_lr": "2.09189e-06", "train_gnorm": "205.253", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.4", "train_wall": "22844"}
[2024-09-27 22:54:27,087][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:54:27,128][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 107
[2024-09-27 22:54:27,248][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:54:27,250][fairseq.trainer][INFO] - begin training epoch 107
[2024-09-27 22:54:27,251][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:54:39,676][train_inner][INFO] - {"epoch": 107, "update": 106.058, "loss": "66.165", "ntokens": "7378.76", "nsentences": "40.32", "nll_loss": "0.362", "wps": "23263.6", "ups": "3.15", "wpb": "7378.8", "bsz": "40.3", "num_updates": "75600", "lr": "2.08547e-06", "gnorm": "200.365", "loss_scale": "8", "train_wall": "59", "gb_free": "18.4", "wall": "22857"}
[2024-09-27 22:55:39,312][train_inner][INFO] - {"epoch": 107, "update": 106.338, "loss": "67.171", "ntokens": "7437.55", "nsentences": "40.32", "nll_loss": "0.364", "wps": "24943.2", "ups": "3.35", "wpb": "7437.6", "bsz": "40.3", "num_updates": "75800", "lr": "2.05447e-06", "gnorm": "205.424", "loss_scale": "8", "train_wall": "59", "gb_free": "18.9", "wall": "22917"}
[2024-09-27 22:56:38,052][train_inner][INFO] - {"epoch": 107, "update": 106.619, "loss": "68.374", "ntokens": "7292.1", "nsentences": "38.44", "nll_loss": "0.36", "wps": "24828.5", "ups": "3.4", "wpb": "7292.1", "bsz": "38.4", "num_updates": "76000", "lr": "2.02392e-06", "gnorm": "213.095", "loss_scale": "8", "train_wall": "58", "gb_free": "19.3", "wall": "22975"}
[2024-09-27 22:57:37,479][train_inner][INFO] - {"epoch": 107, "update": 106.899, "loss": "65.97", "ntokens": "7342.44", "nsentences": "38.88", "nll_loss": "0.349", "wps": "24710.9", "ups": "3.37", "wpb": "7342.4", "bsz": "38.9", "num_updates": "76200", "lr": "1.99383e-06", "gnorm": "204.396", "loss_scale": "8", "train_wall": "59", "gb_free": "18.8", "wall": "23035"}
[2024-09-27 22:57:58,476][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 22:57:58,476][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:57:58,512][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 108
[2024-09-27 22:58:01,169][valid][INFO] - {"epoch": 107, "valid_loss": "14.338", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.251", "valid_wer": "4.373", "valid_raw_wer": "4.373", "valid_wps": "22074.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "76272", "valid_best_wer": "4.317"}
[2024-09-27 22:58:01,169][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 107 @ 76272 updates
[2024-09-27 22:58:01,170][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:58:02,229][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 22:58:02,246][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 107 @ 76272 updates, score 4.373) (writing took 1.076703862985596 seconds)
[2024-09-27 22:58:02,246][fairseq_cli.train][INFO] - end of epoch 107 (average epoch stats below)
[2024-09-27 22:58:02,246][train][INFO] - {"epoch": 107, "train_loss": "66.459", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.358", "train_wps": "24361.9", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "76272", "train_lr": "1.98311e-06", "train_gnorm": "205.581", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "18.3", "train_wall": "23060"}
[2024-09-27 22:58:02,247][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 22:58:02,288][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 108
[2024-09-27 22:58:02,407][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 22:58:02,409][fairseq.trainer][INFO] - begin training epoch 108
[2024-09-27 22:58:02,409][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 22:58:40,675][train_inner][INFO] - {"epoch": 108, "update": 107.18, "loss": "65.008", "ntokens": "7295.98", "nsentences": "39.72", "nll_loss": "0.354", "wps": "23089.9", "ups": "3.16", "wpb": "7296", "bsz": "39.7", "num_updates": "76400", "lr": "1.96419e-06", "gnorm": "208.221", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "23098"}
[2024-09-27 22:59:39,915][train_inner][INFO] - {"epoch": 108, "update": 107.46, "loss": "65.153", "ntokens": "7368.47", "nsentences": "40", "nll_loss": "0.354", "wps": "24877.1", "ups": "3.38", "wpb": "7368.5", "bsz": "40", "num_updates": "76600", "lr": "1.93499e-06", "gnorm": "206.156", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "23157"}
[2024-09-27 23:00:39,033][train_inner][INFO] - {"epoch": 108, "update": 107.741, "loss": "64.281", "ntokens": "7368.23", "nsentences": "40.56", "nll_loss": "0.354", "wps": "24927.2", "ups": "3.38", "wpb": "7368.2", "bsz": "40.6", "num_updates": "76800", "lr": "1.90622e-06", "gnorm": "200.276", "loss_scale": "8", "train_wall": "59", "gb_free": "18.6", "wall": "23216"}
[2024-09-27 23:01:33,812][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 23:01:33,812][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 23:01:33,848][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 109
[2024-09-27 23:01:36,489][valid][INFO] - {"epoch": 108, "valid_loss": "14.268", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.247", "valid_wer": "4.27", "valid_raw_wer": "4.27", "valid_wps": "22159.8", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "76985", "valid_best_wer": "4.27"}
[2024-09-27 23:01:36,490][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 108 @ 76985 updates
[2024-09-27 23:01:36,490][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 23:01:37,582][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_best.pt
[2024-09-27 23:01:38,165][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 108 @ 76985 updates, score 4.27) (writing took 1.675449580186978 seconds)
[2024-09-27 23:01:38,166][fairseq_cli.train][INFO] - end of epoch 108 (average epoch stats below)
[2024-09-27 23:01:38,166][train][INFO] - {"epoch": 108, "train_loss": "66.093", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.356", "train_wps": "24276.3", "train_ups": "3.3", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "76985", "train_lr": "1.87999e-06", "train_gnorm": "206.633", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "17.7", "train_wall": "23276"}
[2024-09-27 23:01:38,167][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 23:01:38,208][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 109
[2024-09-27 23:01:38,326][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 23:01:38,328][fairseq.trainer][INFO] - begin training epoch 109
[2024-09-27 23:01:38,328][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 23:01:42,742][train_inner][INFO] - {"epoch": 109, "update": 108.021, "loss": "69.413", "ntokens": "7316.98", "nsentences": "38.28", "nll_loss": "0.363", "wps": "22969.9", "ups": "3.14", "wpb": "7317", "bsz": "38.3", "num_updates": "77000", "lr": "1.87788e-06", "gnorm": "210.686", "loss_scale": "8", "train_wall": "59", "gb_free": "19.4", "wall": "23280"}
[2024-09-27 23:02:42,120][train_inner][INFO] - {"epoch": 109, "update": 108.302, "loss": "66.219", "ntokens": "7372.34", "nsentences": "39.4", "nll_loss": "0.354", "wps": "24832", "ups": "3.37", "wpb": "7372.3", "bsz": "39.4", "num_updates": "77200", "lr": "1.84996e-06", "gnorm": "206.533", "loss_scale": "8", "train_wall": "59", "gb_free": "18.5", "wall": "23340"}
[2024-09-27 23:03:41,629][train_inner][INFO] - {"epoch": 109, "update": 108.582, "loss": "64.621", "ntokens": "7411.31", "nsentences": "40.68", "nll_loss": "0.355", "wps": "24908.1", "ups": "3.36", "wpb": "7411.3", "bsz": "40.7", "num_updates": "77400", "lr": "1.82246e-06", "gnorm": "208.405", "loss_scale": "16", "train_wall": "59", "gb_free": "19.3", "wall": "23399"}
[2024-09-27 23:04:40,682][train_inner][INFO] - {"epoch": 109, "update": 108.863, "loss": "64.308", "ntokens": "7335.69", "nsentences": "40", "nll_loss": "0.351", "wps": "24844.4", "ups": "3.39", "wpb": "7335.7", "bsz": "40", "num_updates": "77600", "lr": "1.79537e-06", "gnorm": "206.348", "loss_scale": "16", "train_wall": "59", "gb_free": "19.4", "wall": "23458"}
[2024-09-27 23:05:09,565][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 23:05:09,565][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 23:05:09,602][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 110
[2024-09-27 23:05:12,240][valid][INFO] - {"epoch": 109, "valid_loss": "14.264", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.249", "valid_wer": "4.336", "valid_raw_wer": "4.336", "valid_wps": "22148.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "77698", "valid_best_wer": "4.27"}
[2024-09-27 23:05:12,241][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 109 @ 77698 updates
[2024-09-27 23:05:12,241][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 23:05:13,337][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 23:05:13,349][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 109 @ 77698 updates, score 4.336) (writing took 1.1078369899187237 seconds)
[2024-09-27 23:05:13,349][fairseq_cli.train][INFO] - end of epoch 109 (average epoch stats below)
[2024-09-27 23:05:13,349][train][INFO] - {"epoch": 109, "train_loss": "65.243", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.352", "train_wps": "24359.3", "train_ups": "3.31", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "77698", "train_lr": "1.78224e-06", "train_gnorm": "209.322", "train_loss_scale": "16", "train_train_wall": "210", "train_gb_free": "19.6", "train_wall": "23491"}
[2024-09-27 23:05:13,350][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 23:05:13,391][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 110
[2024-09-27 23:05:13,510][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 23:05:13,512][fairseq.trainer][INFO] - begin training epoch 110
[2024-09-27 23:05:13,512][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 23:05:43,803][train_inner][INFO] - {"epoch": 110, "update": 109.143, "loss": "66.713", "ntokens": "7312.44", "nsentences": "38.44", "nll_loss": "0.351", "wps": "23169.8", "ups": "3.17", "wpb": "7312.4", "bsz": "38.4", "num_updates": "77800", "lr": "1.76867e-06", "gnorm": "217.984", "loss_scale": "16", "train_wall": "59", "gb_free": "18.9", "wall": "23521"}
[2024-09-27 23:05:57,357][fairseq.trainer][INFO] - NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
[2024-09-27 23:06:43,575][train_inner][INFO] - {"epoch": 110, "update": 109.425, "loss": "66.616", "ntokens": "7386.81", "nsentences": "39.76", "nll_loss": "0.359", "wps": "24716.5", "ups": "3.35", "wpb": "7386.8", "bsz": "39.8", "num_updates": "78000", "lr": "1.74238e-06", "gnorm": "203.839", "loss_scale": "8", "train_wall": "59", "gb_free": "19.1", "wall": "23581"}
[2024-09-27 23:07:43,451][train_inner][INFO] - {"epoch": 110, "update": 109.705, "loss": "65.07", "ntokens": "7429.96", "nsentences": "40.24", "nll_loss": "0.352", "wps": "24818.2", "ups": "3.34", "wpb": "7430", "bsz": "40.2", "num_updates": "78200", "lr": "1.71648e-06", "gnorm": "208.791", "loss_scale": "8", "train_wall": "59", "gb_free": "17.8", "wall": "23641"}
[2024-09-27 23:08:42,112][train_inner][INFO] - {"epoch": 110, "update": 109.986, "loss": "63.805", "ntokens": "7240.2", "nsentences": "39.08", "nll_loss": "0.344", "wps": "24684.8", "ups": "3.41", "wpb": "7240.2", "bsz": "39.1", "num_updates": "78400", "lr": "1.69096e-06", "gnorm": "204.356", "loss_scale": "8", "train_wall": "58", "gb_free": "19.4", "wall": "23700"}
[2024-09-27 23:08:44,971][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 23:08:44,971][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 23:08:45,005][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 111
[2024-09-27 23:08:47,640][valid][INFO] - {"epoch": 110, "valid_loss": "14.365", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.233", "valid_wer": "4.279", "valid_raw_wer": "4.279", "valid_wps": "22145.4", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "78410", "valid_best_wer": "4.27"}
[2024-09-27 23:08:47,640][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 110 @ 78410 updates
[2024-09-27 23:08:47,641][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 23:08:48,645][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 23:08:48,662][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 110 @ 78410 updates, score 4.279) (writing took 1.0212500849738717 seconds)
[2024-09-27 23:08:48,662][fairseq_cli.train][INFO] - end of epoch 110 (average epoch stats below)
[2024-09-27 23:08:48,662][train][INFO] - {"epoch": 110, "train_loss": "65.641", "train_ntokens": "7352.93", "train_nsentences": "39.618", "train_nll_loss": "0.354", "train_wps": "24314.8", "train_ups": "3.31", "train_wpb": "7352.9", "train_bsz": "39.6", "train_num_updates": "78410", "train_lr": "1.68969e-06", "train_gnorm": "207.036", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.1", "train_wall": "23706"}
[2024-09-27 23:08:48,663][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 23:08:48,704][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 111
[2024-09-27 23:08:48,822][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 23:08:48,824][fairseq.trainer][INFO] - begin training epoch 111
[2024-09-27 23:08:48,824][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 23:09:44,582][train_inner][INFO] - {"epoch": 111, "update": 110.266, "loss": "67.367", "ntokens": "7236.44", "nsentences": "38.96", "nll_loss": "0.363", "wps": "23167.7", "ups": "3.2", "wpb": "7236.4", "bsz": "39", "num_updates": "78600", "lr": "1.66582e-06", "gnorm": "203.224", "loss_scale": "8", "train_wall": "58", "gb_free": "19.4", "wall": "23762"}
[2024-09-27 23:10:44,076][train_inner][INFO] - {"epoch": 111, "update": 110.547, "loss": "64.954", "ntokens": "7432.73", "nsentences": "40.44", "nll_loss": "0.353", "wps": "24986.7", "ups": "3.36", "wpb": "7432.7", "bsz": "40.4", "num_updates": "78800", "lr": "1.64105e-06", "gnorm": "199.111", "loss_scale": "8", "train_wall": "59", "gb_free": "17.5", "wall": "23821"}
[2024-09-27 23:11:43,903][train_inner][INFO] - {"epoch": 111, "update": 110.827, "loss": "64.63", "ntokens": "7415.94", "nsentences": "39.88", "nll_loss": "0.348", "wps": "24791.4", "ups": "3.34", "wpb": "7415.9", "bsz": "39.9", "num_updates": "79000", "lr": "1.61665e-06", "gnorm": "205.451", "loss_scale": "8", "train_wall": "59", "gb_free": "19.5", "wall": "23881"}
[2024-09-27 23:12:19,947][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 23:12:19,947][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 23:12:19,981][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 112
[2024-09-27 23:12:22,608][valid][INFO] - {"epoch": 111, "valid_loss": "14.326", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.244", "valid_wer": "4.307", "valid_raw_wer": "4.307", "valid_wps": "22154.3", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "79123", "valid_best_wer": "4.27"}
[2024-09-27 23:12:22,609][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 111 @ 79123 updates
[2024-09-27 23:12:22,609][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 23:12:23,691][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 23:12:23,708][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 111 @ 79123 updates, score 4.307) (writing took 1.0991400338243693 seconds)
[2024-09-27 23:12:23,708][fairseq_cli.train][INFO] - end of epoch 111 (average epoch stats below)
[2024-09-27 23:12:23,709][train][INFO] - {"epoch": 111, "train_loss": "65.754", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.354", "train_wps": "24374.8", "train_ups": "3.32", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "79123", "train_lr": "1.60183e-06", "train_gnorm": "202.541", "train_loss_scale": "8", "train_train_wall": "210", "train_gb_free": "19.4", "train_wall": "23921"}
[2024-09-27 23:12:23,709][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 23:12:23,750][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 112
[2024-09-27 23:12:23,870][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 23:12:23,872][fairseq.trainer][INFO] - begin training epoch 112
[2024-09-27 23:12:23,872][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 23:12:46,669][train_inner][INFO] - {"epoch": 112, "update": 111.108, "loss": "67.909", "ntokens": "7284.53", "nsentences": "38.92", "nll_loss": "0.363", "wps": "23211.4", "ups": "3.19", "wpb": "7284.5", "bsz": "38.9", "num_updates": "79200", "lr": "1.59262e-06", "gnorm": "205.895", "loss_scale": "8", "train_wall": "58", "gb_free": "17.6", "wall": "23944"}
[2024-09-27 23:13:45,533][train_inner][INFO] - {"epoch": 112, "update": 111.388, "loss": "64.134", "ntokens": "7394.81", "nsentences": "40.4", "nll_loss": "0.35", "wps": "25125.5", "ups": "3.4", "wpb": "7394.8", "bsz": "40.4", "num_updates": "79400", "lr": "1.56894e-06", "gnorm": "203.102", "loss_scale": "8", "train_wall": "58", "gb_free": "19.2", "wall": "24003"}
[2024-09-27 23:14:43,272][train_inner][INFO] - {"epoch": 112, "update": 111.669, "loss": "65.466", "ntokens": "7412.48", "nsentences": "39.64", "nll_loss": "0.35", "wps": "25675.8", "ups": "3.46", "wpb": "7412.5", "bsz": "39.6", "num_updates": "79600", "lr": "1.54562e-06", "gnorm": "204.74", "loss_scale": "8", "train_wall": "57", "gb_free": "18.9", "wall": "24061"}
[2024-09-27 23:15:41,598][train_inner][INFO] - {"epoch": 112, "update": 111.95, "loss": "68.264", "ntokens": "7264.39", "nsentences": "37.84", "nll_loss": "0.356", "wps": "24909.6", "ups": "3.43", "wpb": "7264.4", "bsz": "37.8", "num_updates": "79800", "lr": "1.52264e-06", "gnorm": "202.035", "loss_scale": "8", "train_wall": "58", "gb_free": "19.2", "wall": "24119"}
[2024-09-27 23:15:52,233][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 23:15:52,233][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 23:15:52,269][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 113
[2024-09-27 23:15:54,894][valid][INFO] - {"epoch": 112, "valid_loss": "14.281", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.247", "valid_wer": "4.345", "valid_raw_wer": "4.345", "valid_wps": "22097.2", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "79836", "valid_best_wer": "4.27"}
[2024-09-27 23:15:54,895][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 112 @ 79836 updates
[2024-09-27 23:15:54,895][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 23:15:55,987][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 23:15:56,001][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 112 @ 79836 updates, score 4.345) (writing took 1.1068243621848524 seconds)
[2024-09-27 23:15:56,002][fairseq_cli.train][INFO] - end of epoch 112 (average epoch stats below)
[2024-09-27 23:15:56,002][train][INFO] - {"epoch": 112, "train_loss": "65.63", "train_ntokens": "7351.63", "train_nsentences": "39.6073", "train_nll_loss": "0.354", "train_wps": "24690.9", "train_ups": "3.36", "train_wpb": "7351.6", "train_bsz": "39.6", "train_num_updates": "79836", "train_lr": "1.51854e-06", "train_gnorm": "204.012", "train_loss_scale": "8", "train_train_wall": "207", "train_gb_free": "19.6", "train_wall": "24133"}
[2024-09-27 23:15:56,003][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 23:15:56,044][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 113
[2024-09-27 23:15:56,163][fairseq.data.iterators][INFO] - grouped total_num_itrs = 713
[2024-09-27 23:15:56,165][fairseq.trainer][INFO] - begin training epoch 113
[2024-09-27 23:15:56,165][fairseq_cli.train][INFO] - Start iterating over samples
[2024-09-27 23:16:44,688][train_inner][INFO] - {"epoch": 113, "update": 112.23, "loss": "63.042", "ntokens": "7373.88", "nsentences": "41.04", "nll_loss": "0.351", "wps": "23375.7", "ups": "3.17", "wpb": "7373.9", "bsz": "41", "num_updates": "80000", "lr": "1.5e-06", "gnorm": "201.619", "loss_scale": "8", "train_wall": "59", "gb_free": "19.7", "wall": "24182"}
[2024-09-27 23:16:44,688][fairseq_cli.train][INFO] - Stopping training due to num_updates: 80000 >= max_update: 80000
[2024-09-27 23:16:44,689][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2024-09-27 23:16:44,689][fairseq.tasks.fairseq_task][INFO] - can_reuse_epoch_itr = True
[2024-09-27 23:16:44,723][fairseq.tasks.fairseq_task][INFO] - creating new batches for epoch 114
[2024-09-27 23:16:47,627][valid][INFO] - {"epoch": 113, "valid_loss": "14.316", "valid_ntokens": "1769.12", "valid_nsentences": "9.25", "valid_nll_loss": "0.075", "valid_uer": "1.249", "valid_wer": "4.326", "valid_raw_wer": "4.326", "valid_wps": "20819.2", "valid_wpb": "1769.1", "valid_bsz": "9.2", "valid_num_updates": "80000", "valid_best_wer": "4.27"}
[2024-09-27 23:16:47,628][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 113 @ 80000 updates
[2024-09-27 23:16:47,628][fairseq.trainer][INFO] - Saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 23:16:48,696][fairseq.trainer][INFO] - Finished saving checkpoint to /home/kobie/workspace/fairseq/outputs/2024-09-27/16-33-40/checkpoints/checkpoint_last.pt
[2024-09-27 23:16:48,710][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 113 @ 80000 updates, score 4.326) (writing took 1.081644134130329 seconds)
[2024-09-27 23:16:48,710][fairseq_cli.train][INFO] - end of epoch 113 (average epoch stats below)
[2024-09-27 23:16:48,710][train][INFO] - {"epoch": 113, "train_loss": "65.071", "train_ntokens": "7367.03", "train_nsentences": "40", "train_nll_loss": "0.353", "train_wps": "22922.4", "train_ups": "3.11", "train_wpb": "7367", "train_bsz": "40", "train_num_updates": "80000", "train_lr": "1.5e-06", "train_gnorm": "201.586", "train_loss_scale": "8", "train_train_wall": "48", "train_gb_free": "19.7", "train_wall": "24186"}
[2024-09-27 23:16:48,710][fairseq_cli.train][INFO] - done training in 24185.3 seconds
